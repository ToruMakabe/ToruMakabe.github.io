<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Posts &middot; re-imagine</title>

    <meta name="description" content="my life is the sum of my imagination">

    <meta name="generator" content="Hugo 0.17" />
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="tmak_tw" />
    <meta name="twitter:title" content="Posts &middot; re-imagine">
    <meta name="twitter:description" content="my life is the sum of my imagination">

    <meta property="og:type" content="article">
    <meta property="og:title" content="Posts &middot; re-imagine">
    <meta property="og:description" content="my life is the sum of my imagination">

    <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700|Oxygen:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/pure-min.css">
    <!--[if lte IE 8]>
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/grids-responsive-old-ie-min.css">
    <![endif]-->
    <!--[if gt IE 8]><!-->
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/grids-responsive-min.css">
    <!--<![endif]-->

    <link rel="stylesheet" href="http://torumakabe.github.io//css/all.min.css">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

    <link rel="alternate" type="application/rss+xml" title="re-imagine" href="http://torumakabe.github.io//index.xml" />
</head>
<body>


<div id="layout" class="pure-g">
    <div class="sidebar pure-u-1 pure-u-md-1-4">
    <div class="header">
        <hgroup>
            <h1 class="brand-title"><a href="http://torumakabe.github.io/">re-imagine</a></h1>
            <h2 class="brand-tagline"> my life is the sum of my imagination </h2>
        </hgroup>

        <nav class="nav">
            <ul class="nav-list">
                
                <li class="nav-item">
                    <a class="pure-button" href="https://twitter.com/tmak_tw"><i class="fa fa-twitter"></i> Twitter</a>
                </li>
                
                
                <li class="nav-item">
                    <a class="pure-button" href="https://github.com/ToruMakabe "><i class="fa fa-github-alt"></i> github</a>
                </li>
                
                <li class="nav-item">
                    <a class="pure-button" href="http://torumakabe.github.io//index.xml"><i class="fa fa-rss"></i> rss</a>
                </li>
            </ul>
        </nav>
    </div>
</div>


    <div class="content pure-u-1 pure-u-md-3-4">
        <div>
            
            <div class="posts">
                
                <h1 class="content-subhead">27 Mar 2018, 00:10</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="http://torumakabe.github.io/post/azure_private_dns_preview/" class="post-title">Azure DNS Private Zonesの動きを確認する</a>

                        <p class="post-meta">
                            
                            
                                under 
                                
                                <a class="post-category post-category-Azure" href="http://torumakabe.github.io//categories/azure">Azure</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        

<h2 id="プライベートゾーンのパブリックプレビュー開始">プライベートゾーンのパブリックプレビュー開始</h2>

<p>Azure DNSのプライベートゾーン対応が、全リージョンでパブリックプレビューとなりました。ゾーンとプレビューのプライベートとパブリックが入り混じって、なにやら紛らわしいですが。</p>

<p>さて、このプライベートゾーン対応ですが、名前のとおりAzure DNSをプライベートな仮想ネットワーク(VNet)で使えるようになります。加えて、しみじみと嬉しい便利機能がついています。</p>

<ul>
<li>Split-Horizonに対応します。VNet内からの問い合わせにはプライベートゾーン、それ以外からはパブリックゾーンのレコードを返します。</li>
<li>仮想マシンの作成時、プライベートゾーンへ自動でホスト名を追加します。</li>
<li>プライベートゾーンとVNetをリンクして利用します。複数のVNetをリンクすることが可能です。</li>
<li>リンクの種類として、仮想マシンホスト名の自動登録が行われるVNetをRegistration VNet、名前解決(正引き)のみ可能なResolution VNetがあります。</li>
<li>プライベートゾーンあたり、Registration VNetの現時点の上限数は1、Resolution VNetは10です。</li>
</ul>

<p>公式ドキュメントは<a href="https://docs.microsoft.com/en-us/azure/dns/private-dns-overview">こちら</a>。現時点の<a href="https://docs.microsoft.com/en-us/azure/dns/private-dns-overview#limitations">制約もまとまっている</a>ので、目を通しておきましょう。</p>

<h2 id="動きを見てみよう">動きを見てみよう</h2>

<p>公式ドキュメントには<a href="https://docs.microsoft.com/en-us/azure/dns/private-dns-scenarios">想定シナリオ</a>があり、これを読めばできることがだいたい分かります。ですが、名前解決は呼吸のようなもの、体に叩き込みたいお気持ちです。手を動かして確認します。</p>

<h3 id="事前に準備する環境">事前に準備する環境</h3>

<p>下記リソースを先に作っておきます。手順は割愛。ドメイン名はexample.comとしましたが、適宜読み替えてください。</p>

<ul>
<li>VNet *2

<ul>
<li>vnet01</li>
<li>subnet01

<ul>
<li>subnet01-nsg (allow ssh)</li>
</ul></li>
<li>vnet02</li>
<li>subnet01

<ul>
<li>subnet01-nsg (allow ssh)</li>
</ul></li>
</ul></li>
<li>Azure DNS Public Zone

<ul>
<li>example.com</li>
</ul></li>
</ul>

<h3 id="azure-cliへdns拡張を導入">Azure CLIへDNS拡張を導入</h3>

<p>プレビュー機能をCLIに導入します。いずれ要らなくなるかもしれませんので、要否は<a href="https://docs.microsoft.com/en-us/azure/dns/private-dns-getstarted-cli#to-installuse-azure-dns-private-zones-feature-public-preview">公式ドキュメント</a>で確認してください。</p>

<pre><code>$ az extension add --name dns
</code></pre>

<h3 id="プライベートゾーンの作成">プライベートゾーンの作成</h3>

<p>既存のゾーンを確認します。パブリックゾーンがあります。</p>

<pre><code>$ az network dns zone list -o table
ZoneName      ResourceGroup             RecordSets    MaxRecordSets
------------  ----------------------  ------------  ---------------
example.com   common-global-rg                   2             5000
</code></pre>

<p>プライベートゾーンを作成します。Registration VNetとしてvnet01をリンクします。<a href="https://docs.microsoft.com/en-us/azure/dns/private-dns-overview#limitations">現時点の制約</a>で、リンク時にはVNet上にVMが無い状態にする必要があります。</p>

<pre><code>$ az network dns zone create -g private-dns-poc-ejp-rg -n example.com --zone-type Private --registration-vnets vnet01
</code></pre>

<p>同じ名前のゾーンが2つになりました。</p>

<pre><code>$ az network dns zone list -o table
ZoneName      ResourceGroup             RecordSets    MaxRecordSets
------------  ----------------------  ------------  ---------------
example.com   common-global-rg                   2             5000
example.com   private-dns-poc-ejp-rg             1             5000
</code></pre>

<h3 id="registration-vnetへvmを作成">Registration VNetへVMを作成</h3>

<p>VMを2つ作ります。1つにはインターネット経由でsshするので、パブリックIPを割り当てます。</p>

<pre><code>$ BASE_NAME=&quot;private-dns-poc-ejp&quot;
$ az network public-ip create -n vm01-pip -g ${BASE_NAME}-rg
$ az network nic create -g ${BASE_NAME}-rg -n vm01-nic --public-ip-address vm01-pip --vnet vnet01 --subnet subnet01
$ az vm create -g ${BASE_NAME}-rg -n vm01 --image Canonical:UbuntuServer:16.04.0-LTS:latest --size Standard_B1s --nics vm01-nic
$ az network nic create -g ${BASE_NAME}-rg -n vm02-nic --vnet vnet01 --subnet subnet01
$ az vm create -g ${BASE_NAME}-rg -n vm02 --image Canonical:UbuntuServer:16.04.0-LTS:latest --size Standard_B1s --nics vm02-nic
</code></pre>

<h3 id="パブリックipをパブリックゾーンへ登録">パブリックIPをパブリックゾーンへ登録</h3>

<p>Split-Horizonの動きを確認したいので、パブリックIPをパブリックゾーンへ登録します。</p>

<pre><code>$ az network public-ip show -g private-dns-poc-ejp-rg -n vm01-pip --query ipAddress
&quot;13.78.84.84&quot;
$ az network dns record-set a add-record -g common-global-rg -z example.com -n vm01 -a 13.78.84.84
</code></pre>

<p>パブリックゾーンで名前解決できることを確認します。</p>

<pre><code>$ nslookup vm01.example.com
Server:         103.5.140.1
Address:        103.5.140.1#53

Non-authoritative answer:
Name:   vm01.example.com
Address: 13.78.84.84
</code></pre>

<h3 id="registration-vnetの動きを確認">Registration VNetの動きを確認</h3>

<p>vnet01のvm01へ、パブリックIP経由でsshします。</p>

<pre><code>$ ssh vm01.example.com
</code></pre>

<p>同じRegistration VNet上のvm02を正引きします。ドメイン名無し、ホスト名だけでnslookupすると、VNetの内部ドメイン名がSuffixになります。</p>

<pre><code>vm01:~$ nslookup vm02
Server:         168.63.129.16
Address:        168.63.129.16#53

Non-authoritative answer:
Name:   vm02.aioh0amlfdze5drhlpb1ktqwxd.lx.internal.cloudapp.net
Address: 10.0.0.5
</code></pre>

<p>ドメイン名をつけてみましょう。Nameはvnet01にリンクしたプライベートゾーンのドメイン名になりました。</p>

<pre><code>vm01:~$ nslookup vm02.example.com
Server:         168.63.129.16
Address:        168.63.129.16#53

Non-authoritative answer:
Name:   vm02.example.com
Address: 10.0.0.5
</code></pre>

<p>逆引きもできます。</p>

<pre><code>vm01:~$ nslookup 10.0.0.5
Server:         168.63.129.16
Address:        168.63.129.16#53

Non-authoritative answer:
5.0.0.10.in-addr.arpa   name = vm02.example.com.

Authoritative answers can be found from:
</code></pre>

<h3 id="split-horizonの動きを確認">Split-Horizonの動きを確認</h3>

<p>さて、いま作業をしているvm01には、インターネット経由でパブリックゾーンで名前解決してsshしたわけですが、プライベートなVNet内でnslookupするとどうなるでしょう。</p>

<pre><code>vm01:~$ nslookup vm01.example.com
Server:         168.63.129.16
Address:        168.63.129.16#53

Non-authoritative answer:
Name:   vm01.example.com
Address: 10.0.0.4
</code></pre>

<p>プライベートゾーンで解決されました。Split-Horizonが機能していることが分かります。</p>

<p>あ、どうでもいいことですが、Split-Horizonって戦隊モノの必殺技みたいなネーミングですね。叫びながら地面に拳を叩きつけたい感じ。</p>

<h3 id="resolution-vnetの動きを確認">Resolution VNetの動きを確認</h3>

<p>vnet02を作成し、Resolution VNetとしてプライベートゾーンとリンクします。そして、vnet02にvm03を作ります。vm03へのsshまで一気に進めます。</p>

<pre><code>$ BASE_NAME=&quot;private-dns-poc-ejp&quot;
$ az network vnet create -g ${BASE_NAME}-rg -n vnet02 --address-prefix 10.1.0.0/16 --subnet-name subnet01
$ az network vnet subnet update -g ${BASE_NAME}-rg -n subnet01 --vnet-name vnet02 --network-security-group subnet01-nsg
$ az network public-ip create -n vm03-pip -g ${BASE_NAME}-rg
$ az network dns zone update -g private-dns-poc-ejp-rg -n example.com --resolution-vnets vnet02
$ az network nic create -g ${BASE_NAME}-rg -n vm03-nic --public-ip-address vm03-pip --vnet vnet02 --subnet subnet01
$ az vm create -g ${BASE_NAME}-rg -n vm03 --image Canonical:UbuntuServer:16.04.0-LTS:latest --size Standard_B1s --nics vm03-nic
$ az network public-ip show -g private-dns-poc-ejp-rg -n vm03-pip --query ipAddress
&quot;13.78.54.133&quot;
$ ssh 13.78.54.133
</code></pre>

<p>名前解決の確認が目的なので、vnet01/02間はPeeringしません。</p>

<p>では、vnet01上のvm01を正引きします。ドメイン名を指定しないと、解決できません。vnet02上にvm01がある、と指定されたと判断するからです。</p>

<pre><code>vm03:~$ nslookup vm01
Server:         168.63.129.16
Address:        168.63.129.16#53

** server can't find vm01: SERVFAIL
</code></pre>

<p>ではプライベートゾーンのドメイン名をつけてみます。解決できました。</p>

<pre><code>vm03:~$ nslookup vm01.example.com
Server:         168.63.129.16
Address:        168.63.129.16#53

Non-authoritative answer:
Name:   vm01.example.com
Address: 10.0.0.4
</code></pre>

<p>Resolution VNetからは、逆引きできません。</p>

<pre><code>vm03:~$ nslookup 10.0.0.4
Server:         168.63.129.16
Address:        168.63.129.16#53

** server can't find 4.0.0.10.in-addr.arpa: NXDOMAIN
</code></pre>

<p>ところでRegistration VNetからResolution VNetのホスト名をnslookupするとどうなるでしょう。</p>

<pre><code>$ ssh vm01.example.com
vm01:~$ nslookup vm03
Server:         168.63.129.16
Address:        168.63.129.16#53

** server can't find vm03: SERVFAIL

vm01:~$ nslookup vm03.example.com
Server:         168.63.129.16
Address:        168.63.129.16#53

** server can't find vm03.example.com: NXDOMAIN
</code></pre>

<p>ドメイン名あり、なしに関わらず、名前解決できません。VNetが別なのでVNetの内部DNSで解決できない、また、Resolution VNetのVMはレコードがプライベートゾーンに自動登録されないことが分かります。</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">26 Mar 2018, 00:08</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="http://torumakabe.github.io/post/az_vmss_terraform/" class="post-title">AzureのAvailability Zonesへ分散するVMSSをTerraformで作る</a>

                        <p class="post-meta">
                            
                            
                                under 
                                
                                <a class="post-category post-category-Azure" href="http://torumakabe.github.io//categories/azure">Azure</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        

<h2 id="動機">動機</h2>

<p>Terraform Azure Provider 1.3.0で、VMSSを作る際にAvailability Zonesを指定できるように<a href="https://github.com/terraform-providers/terraform-provider-azurerm/pull/811">なりました</a>。Availability Zonesはインフラの根っこの仕組みなので、現在(<sup>2018</sup>&frasl;<sub>3</sub>)限定されたリージョンで長めのプレビュー期間がとられています。ですが、GAやグローバル展開を見据え、素振りしておきましょう。</p>

<h2 id="前提条件">前提条件</h2>

<ul>
<li>Availability Zones対応リージョンを選びます。現在は<a href="https://docs.microsoft.com/en-us/azure/availability-zones/az-overview#regions-that-support-availability-zones">5リージョン</a>です。この記事ではEast US 2とします。</li>
<li>Availability Zonesのプレビューに<a href="https://docs.microsoft.com/ja-jp/azure/availability-zones/az-overview">サインアップ</a>済みとします。</li>
<li>bashでsshの公開鍵が~/.ssh/id_rsa.pubにあると想定します。</li>
<li>動作確認した環境は以下です。

<ul>
<li>Terraform 0.11.2</li>
<li>Terraform Azure Provider 1.3.0</li>
<li>WSL (ubuntu 16.04)</li>
<li>macos (High Sierra 10.13.3)</li>
</ul></li>
</ul>

<h2 id="コード">コード</h2>

<p>以下のファイルを同じディレクトリに作成します。</p>

<h3 id="terraform-メインコード">Terraform メインコード</h3>

<p>VMSSと周辺リソースを作ります。</p>

<ul>
<li>最終行近くの &ldquo;zones = [1, 2, 3]&rdquo; がポイントです。これだけで、インスタンスを散らす先のゾーンを指定できます。</li>
<li>クロスゾーン負荷分散、冗長化するため、Load BalancerとパブリックIPのSKUをStandardにします。</li>
</ul>

<p>[main.tf]</p>

<pre><code>resource &quot;azurerm_resource_group&quot; &quot;poc&quot; {
  name     = &quot;${var.resource_group_name}&quot;
  location = &quot;East US 2&quot;
}

resource &quot;azurerm_virtual_network&quot; &quot;poc&quot; {
  name                = &quot;vnet01&quot;
  resource_group_name = &quot;${azurerm_resource_group.poc.name}&quot;
  location            = &quot;${azurerm_resource_group.poc.location}&quot;
  address_space       = [&quot;10.0.0.0/16&quot;]
}

resource &quot;azurerm_subnet&quot; &quot;poc&quot; {
  name                      = &quot;subnet01&quot;
  resource_group_name       = &quot;${azurerm_resource_group.poc.name}&quot;
  virtual_network_name      = &quot;${azurerm_virtual_network.poc.name}&quot;
  address_prefix            = &quot;10.0.2.0/24&quot;
  network_security_group_id = &quot;${azurerm_network_security_group.poc.id}&quot;
}

resource &quot;azurerm_network_security_group&quot; &quot;poc&quot; {
  name                = &quot;nsg01&quot;
  resource_group_name = &quot;${azurerm_resource_group.poc.name}&quot;
  location            = &quot;${azurerm_resource_group.poc.location}&quot;

  security_rule = [
    {
      name                       = &quot;allow_http&quot;
      priority                   = 100
      direction                  = &quot;Inbound&quot;
      access                     = &quot;Allow&quot;
      protocol                   = &quot;Tcp&quot;
      source_port_range          = &quot;*&quot;
      destination_port_range     = &quot;80&quot;
      source_address_prefix      = &quot;*&quot;
      destination_address_prefix = &quot;*&quot;
    },
    {
      name                       = &quot;allow_ssh&quot;
      priority                   = 101
      direction                  = &quot;Inbound&quot;
      access                     = &quot;Allow&quot;
      protocol                   = &quot;Tcp&quot;
      source_port_range          = &quot;*&quot;
      destination_port_range     = &quot;22&quot;
      source_address_prefix      = &quot;*&quot;
      destination_address_prefix = &quot;*&quot;
    },
  ]
}

resource &quot;azurerm_public_ip&quot; &quot;poc&quot; {
  name                         = &quot;pip01&quot;
  resource_group_name          = &quot;${azurerm_resource_group.poc.name}&quot;
  location                     = &quot;${azurerm_resource_group.poc.location}&quot;
  public_ip_address_allocation = &quot;static&quot;
  domain_name_label            = &quot;${var.scaleset_name}&quot;

  sku = &quot;Standard&quot;
}

resource &quot;azurerm_lb&quot; &quot;poc&quot; {
  name                = &quot;lb01&quot;
  resource_group_name = &quot;${azurerm_resource_group.poc.name}&quot;
  location            = &quot;${azurerm_resource_group.poc.location}&quot;

  frontend_ip_configuration {
    name                 = &quot;fipConf01&quot;
    public_ip_address_id = &quot;${azurerm_public_ip.poc.id}&quot;
  }

  sku = &quot;Standard&quot;
}

resource &quot;azurerm_lb_backend_address_pool&quot; &quot;poc&quot; {
  name                = &quot;bePool01&quot;
  resource_group_name = &quot;${azurerm_resource_group.poc.name}&quot;
  loadbalancer_id     = &quot;${azurerm_lb.poc.id}&quot;
}

resource &quot;azurerm_lb_rule&quot; &quot;poc&quot; {
  name                           = &quot;lbRule&quot;
  resource_group_name            = &quot;${azurerm_resource_group.poc.name}&quot;
  loadbalancer_id                = &quot;${azurerm_lb.poc.id}&quot;
  protocol                       = &quot;Tcp&quot;
  frontend_port                  = 80
  backend_port                   = 80
  frontend_ip_configuration_name = &quot;fipConf01&quot;
  backend_address_pool_id        = &quot;${azurerm_lb_backend_address_pool.poc.id}&quot;
  probe_id                       = &quot;${azurerm_lb_probe.poc.id}&quot;
}

resource &quot;azurerm_lb_probe&quot; &quot;poc&quot; {
  name                = &quot;http-probe&quot;
  resource_group_name = &quot;${azurerm_resource_group.poc.name}&quot;
  loadbalancer_id     = &quot;${azurerm_lb.poc.id}&quot;
  port                = 80
}

resource &quot;azurerm_lb_nat_pool&quot; &quot;poc&quot; {
  count                          = 3
  name                           = &quot;ssh&quot;
  resource_group_name            = &quot;${azurerm_resource_group.poc.name}&quot;
  loadbalancer_id                = &quot;${azurerm_lb.poc.id}&quot;
  protocol                       = &quot;Tcp&quot;
  frontend_port_start            = 50000
  frontend_port_end              = 50119
  backend_port                   = 22
  frontend_ip_configuration_name = &quot;fipConf01&quot;
}

data &quot;template_cloudinit_config&quot; &quot;poc&quot; {
  gzip          = true
  base64_encode = true

  part {
    content_type = &quot;text/cloud-config&quot;
    content      = &quot;${file(&quot;${path.module}/cloud-config.yaml&quot;)}&quot;
  }
}

resource &quot;azurerm_virtual_machine_scale_set&quot; &quot;poc&quot; {
  name                = &quot;${var.scaleset_name}&quot;
  resource_group_name = &quot;${azurerm_resource_group.poc.name}&quot;
  location            = &quot;${azurerm_resource_group.poc.location}&quot;
  upgrade_policy_mode = &quot;Manual&quot;

  sku {
    name     = &quot;Standard_B1s&quot;
    tier     = &quot;Standard&quot;
    capacity = 3
  }

  storage_profile_image_reference {
    publisher = &quot;Canonical&quot;
    offer     = &quot;UbuntuServer&quot;
    sku       = &quot;16.04-LTS&quot;
    version   = &quot;latest&quot;
  }

  storage_profile_os_disk {
    name              = &quot;&quot;
    caching           = &quot;ReadWrite&quot;
    create_option     = &quot;FromImage&quot;
    managed_disk_type = &quot;Standard_LRS&quot;
  }

  os_profile {
    computer_name_prefix = &quot;pocvmss&quot;
    admin_username       = &quot;${var.admin_username}&quot;
    admin_password       = &quot;&quot;
    custom_data          = &quot;${data.template_cloudinit_config.poc.rendered}&quot;
  }

  os_profile_linux_config {
    disable_password_authentication = true

    ssh_keys {
      path     = &quot;/home/${var.admin_username}/.ssh/authorized_keys&quot;
      key_data = &quot;${file(&quot;~/.ssh/id_rsa.pub&quot;)}&quot;
    }
  }

  network_profile {
    name    = &quot;terraformnetworkprofile&quot;
    primary = true

    ip_configuration {
      name                                   = &quot;PoCIPConfiguration&quot;
      subnet_id                              = &quot;${azurerm_subnet.poc.id}&quot;
      load_balancer_backend_address_pool_ids = [&quot;${azurerm_lb_backend_address_pool.poc.id}&quot;]
      load_balancer_inbound_nat_rules_ids    = [&quot;${element(azurerm_lb_nat_pool.poc.*.id, count.index)}&quot;]
    }
  }

  zones = [1, 2, 3]
}
</code></pre>

<h3 id="cloud-init-configファイル">cloud-init configファイル</h3>

<p>各インスタンスがどのゾーンで動いているか確認したいので、インスタンス作成時にcloud-initでWebサーバーを仕込みます。メタデータからインスタンス名と実行ゾーンを引っ張り、nginxのドキュメントルートに書きます。</p>

<p>[cloud-config.yaml]</p>

<pre><code>#cloud-config
package_upgrade: true
packages:
  - nginx
runcmd:
  - 'echo &quot;[Instance Name]: `curl -H Metadata:true &quot;http://169.254.169.254/metadata/instance/compute/name?api-version=2017-12-01&amp;format=text&quot;`    [Zone]: `curl -H Metadata:true &quot;http://169.254.169.254/metadata/instance/compute/zone?api-version=2017-12-01&amp;format=text&quot;`&quot; &gt; /var/www/html/index.nginx-debian.html'
</code></pre>

<p>インスタンス作成時、パッケージの導入やアップデートに時間をかけたくない場合は、Packerなどで前もってカスタムイメージを作っておくのも手です。</p>

<ul>
<li><a href="https://docs.microsoft.com/ja-jp/azure/virtual-machines/linux/build-image-with-packer">Packer を使用して Azure に Linux 仮想マシンのイメージを作成する方法</a></li>
<li><a href="https://docs.microsoft.com/ja-jp/azure/terraform/terraform-create-vm-scaleset-network-disks-using-packer-hcl">Terraform を使用して Packer カスタム イメージから Azure 仮想マシン スケール セットを作成する</a></li>
</ul>

<h3 id="terraform-変数ファイル">Terraform 変数ファイル</h3>

<p>変数は別ファイルへ。</p>

<p>[variables.tf]</p>

<pre><code>variable &quot;resource_group_name&quot; {
  default = &quot;your-rg&quot;
}

variable &quot;scaleset_name&quot; {
  default = &quot;yourvmss01&quot;
}

variable &quot;admin_username&quot; {
  default = &quot;yourname&quot;
}
</code></pre>

<h2 id="実行">実行</h2>

<p>では実行。</p>

<pre><code>$ terraform init
$ terraform plan
$ terraform apply
</code></pre>

<p>5分くらいで完了しました。このサンプルでは、この後のcloud-initのパッケージ処理に時間がかかります。待てない場合は前述の通り、カスタムイメージを使いましょう。</p>

<p>インスタンスへのsshを通すよう、Load BalancerにNATを設定していますので、cloud-initの進捗は確認できます。</p>

<pre><code>$ ssh -p 50000 yourname@yourvmss01.eastus2.cloudapp.azure.com
$ tail -f /var/log/cloud-init-output.log
Cloud-init v. 17.1 finished at Sun, 25 Mar 2018 10:41:40 +0000. Datasource DataSourceAzure [seed=/dev/sr0].  Up 611.51 seconds
</code></pre>

<p>ではWebサーバーにアクセスしてみましょう。</p>

<pre><code>$ while true; do curl yourvmss01.eastus2.cloudapp.azure.com; sleep 1; done;
[Instance Name]: yourvmss01_2    [Zone]: 3
[Instance Name]: yourvmss01_0    [Zone]: 1
[Instance Name]: yourvmss01_2    [Zone]: 3
[Instance Name]: yourvmss01_1    [Zone]: 2
</code></pre>

<p>VMSSのインスタンスがゾーンに分散されたことが分かります。</p>

<p>では、このままスケールアウトしてみましょう。main.tfのazurerm_virtual_machine_scale_set.poc.sku.capacityを3から4にし、再度applyします。</p>

<pre><code>[Instance Name]: yourvmss01_1    [Zone]: 2
[Instance Name]: yourvmss01_3    [Zone]: 1
[Instance Name]: yourvmss01_3    [Zone]: 1
[Instance Name]: yourvmss01_1    [Zone]: 2
[Instance Name]: yourvmss01_3    [Zone]: 1
</code></pre>

<p>ダウンタイムなしに、yourvmss01_3が追加されました。すこぶる簡単。</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">12 Mar 2018, 00:21</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="http://torumakabe.github.io/post/aks_dns/" class="post-title">AKSのService作成時にホスト名を付ける</a>

                        <p class="post-meta">
                            
                            
                                under 
                                
                                <a class="post-category post-category-Azure" href="http://torumakabe.github.io//categories/azure">Azure</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        

<h2 id="2つのやり口">2つのやり口</h2>

<p>Azure Container Service(AKS)はServiceを公開する際、パブリックIPを割り当てられます。でもIPだけじゃなく、ホスト名も同時に差し出して欲しいケースがありますよね。</p>

<p>わたしの知る限り、2つの方法があります。</p>

<ul>
<li>AKS(k8s) 1.9で対応した<a href="https://github.com/kubernetes/kubernetes/pull/47849">DNSラベル名付与機能</a>を使う</li>
<li><a href="https://github.com/kubernetes-incubator/external-dns">Kubenetes ExternalDNS</a>を使ってAzure DNSへAレコードを追加する</li>
</ul>

<p>以下、AKS 1.9.2での実現手順です。</p>

<h2 id="dnsラベル名付与機能">DNSラベル名付与機能</h2>

<p>簡単です。Serviceのannotationsに定義するだけ。試しにnginxをServiceとして公開し、確認してみましょう。</p>

<p>[nginx-label.yaml]</p>

<pre><code>apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx
spec:
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: hogeginx
  annotations:
    service.beta.kubernetes.io/azure-dns-label-name: hogeginx
spec:
  selector:
    app: nginx
  type: LoadBalancer
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
</code></pre>

<p>デプロイ。</p>

<pre><code>$ kubectl create -f nginx-label.yaml
</code></pre>

<p>パブリックIP(EXTERNAL-IP)が割り当てられた後、ラベル名が使えます。ルールは [ラベル名].[リージョン].cloudapp.azure.com です。</p>

<pre><code>$ curl hogeginx.eastus.cloudapp.azure.com
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
[snip]
</code></pre>

<p>ドメイン名は指定しなくていいから、Service毎にホスト名を固定したいんじゃ、という場合にはこれでOK。</p>

<h2 id="kubenetes-externaldns">Kubenetes ExternalDNS</h2>

<p>任意のドメイン名を使いたい場合は、Incubatorプロジェクトのひとつ、Kubenetes ExternalDNSを使ってAzure DNSへAレコードを追加する手があります。本家の説明は<a href="https://github.com/kubernetes-incubator/external-dns/blob/master/docs/tutorials/azure.md">こちら</a>。</p>

<p>Kubenetes ExternalDNSは、Azure DNSなどAPIを持つDNSサービスを操作するアプリです。k8sのDeploymentとして動かせます。Route 53などにも対応。</p>

<p>さて動かしてみましょう。前提として、すでにAzure DNSにゾーンがあるものとします。</p>

<p>ExternalDNSがDNSゾーンを操作できるよう、サービスプリンシパルを作成しましょう。スコープはDNSゾーンが置かれているリソースグループ、ロールはContributorとします。</p>

<pre><code>$ az ad sp create-for-rbac --role=&quot;Contributor&quot; --scopes=&quot;/subscriptions/your-subscription-id/resourceGroups/hoge-dns-rg&quot; -n hogeExtDnsSp
</code></pre>

<p>appId、password、tenantを控えておいてください。次でsecretに使います。</p>

<p>ではExteralDNSに渡すsecretを作ります。まずJSONファイルに書きます。</p>

<p>[azure.json]</p>

<pre><code>{
    &quot;tenantId&quot;: &quot;your-tenant&quot;,
    &quot;subscriptionId&quot;: &quot;your-subscription-id&quot;,
    &quot;aadClientId&quot;: &quot;your-appId&quot;,
    &quot;aadClientSecret&quot;: &quot;your-password&quot;,
    &quot;resourceGroup&quot;: &quot;hoge-dns-rg&quot;
}
</code></pre>

<p>JSONファイルを元に、secretを作ります。</p>

<pre><code>$ kubectl create secret generic azure-config-file --from-file=azure.json
</code></pre>

<p>ExteralDNSのマニフェストを作ります。ドメイン名はexmaple.comとしていますが、使うDNSゾーンに合わせてください。以下はRBACを使っていない環境での書き方です。</p>

<p>[extdns.yaml]</p>

<pre><code>apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: external-dns
spec:
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: external-dns
    spec:
      containers:
      - name: external-dns
        image: registry.opensource.zalan.do/teapot/external-dns:v0.4.8
        args:
        - --source=service
        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.
        - --provider=azure
        - --azure-resource-group=hoge-dns-rg # (optional) use the DNS zones from the tutorial's resource group
        volumeMounts:
        - name: azure-config-file
          mountPath: /etc/kubernetes
          readOnly: true
      volumes:
      - name: azure-config-file
        secret:
          secretName: azure-config-file
</code></pre>

<p>ExternalDNSをデプロイします。</p>

<pre><code>$ kubectl create -f extdns.yaml
</code></pre>

<p>ではホスト名を付与するServiceのマニフェストを作りましょう。先ほどのDNSラベル名付与機能と同様、annotationsへ定義します。</p>

<p>[nginx-extdns.yaml]</p>

<pre><code>apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx-extdns
spec:
  template:
    metadata:
      labels:
        app: nginx-extdns
    spec:
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: hogeginx-extdns
  annotations:
    external-dns.alpha.kubernetes.io/hostname: hogeginx.example.com
spec:
  selector:
    app: nginx-extdns
  type: LoadBalancer
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
</code></pre>

<p>デプローイ。</p>

<pre><code>$ kubectl create -f nginx-extdns.yaml
</code></pre>

<p>パブリックIP(EXTERNAL-IP)が割り当てられた後、Aレコードが登録されます。確認してみましょう。</p>

<pre><code>$ az network dns record-set a list -g hoge-dns-rg -z example.com -o table
Name      ResourceGroup       Ttl  Type    Metadata
--------  ----------------  -----  ------  ----------
hogeginx  hoge-dns-rg         300  A
</code></pre>

<p>ゲッツ。</p>

<pre><code>$ curl hogeginx.example.com
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
[snip]
</code></pre>

<p>Incubatorプロジェクトなので今後大きく変化する可能性がありますが、ご参考になれば。</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">11 Feb 2018, 00:20</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="http://torumakabe.github.io/post/aks_tls_autorenewal/" class="post-title">AKSのIngress TLS証明書を自動更新する</a>

                        <p class="post-meta">
                            
                            
                                under 
                                
                                <a class="post-category post-category-Azure" href="http://torumakabe.github.io//categories/azure">Azure</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        

<h2 id="カジュアルな証明書管理方式が欲しい">カジュアルな証明書管理方式が欲しい</h2>

<p>ChromeがHTTPサイトに対する警告を<a href="https://japan.cnet.com/article/35100589/">強化するそうです</a>。非HTTPSサイトには、生きづらい世の中になりました。</p>

<p>さてそうなると、TLS証明書の入手と更新、めんどくさいですね。ガチなサイトでは証明書の維持管理を計画的に行うべきですが、検証とかちょっとした用途で立てるサイトでは、とにかくめんどくさい。カジュアルな方式が望まれます。</p>

<p>そこで、Azure Container Service(AKS)で使える気軽な方法をご紹介します。</p>

<ul>
<li>TLSはIngress(NGINX Ingress Controller)でまとめて終端</li>
<li><a href="https://letsencrypt.org/">Let&rsquo;s Encypt</a>から証明書を入手</li>
<li>Kubenetesのアドオンである<a href="https://github.com/jetstack/cert-manager/">cert-manager</a>で証明書の入手、更新とIngressへの適用を自動化

<ul>
<li>ACME(Automatic Certificate Management Environment)対応</li>
<li>cert-managerはまだ歴史の浅いプロジェクトだが、<a href="https://github.com/jetstack/cert-manager/">kube-lego</a>の後継として期待</li>
</ul></li>
</ul>

<p>なおKubernetes/AKSは開発ペースやエコシステムの変化が速いので要注意。この記事は2018/2/10に書いています。</p>

<h2 id="使い方">使い方</h2>

<p>AKSクラスターと、Azure DNS上に利用可能なゾーンがあることを前提にします。ない場合、それぞれ公式ドキュメントを参考にしてください。</p>

<ul>
<li><a href="https://docs.microsoft.com/ja-jp/azure/aks/kubernetes-walkthrough">Azure Container Service (AKS) クラスターのデプロイ</a></li>
<li><a href="https://docs.microsoft.com/ja-jp/azure/dns/dns-getstarted-cli">Azure CLI 2.0 で Azure DNS の使用を開始する</a></li>
</ul>

<p>まずAKSにNGINX Ingress Controllerを導入します。helmで入れるのが楽でしょう。<a href="http://torumakabe.github.io/post/aks_ingress_quickdeploy/">この記事</a>も参考に。</p>

<pre><code>$ helm install stable/nginx-ingress --name my-nginx
</code></pre>

<p>サービスの状況を確認します。NGINX Ingress ControllerにEXTERNAL-IPが割り当てられるまで、待ちます。</p>

<pre><code>$ kubectl get svc
NAME                                     TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                     AGE
kubernetes                               ClusterIP      10.0.0.1       &lt;none&gt;           443/TCP                     79d
my-nginx-nginx-ingress-controller        LoadBalancer   10.0.2.105     52.234.148.138   80:30613/TCP,443:30186/TCP   6m
my-nginx-nginx-ingress-default-backend   ClusterIP      10.0.102.246   &lt;none&gt;           80/TCP                     6m
</code></pre>

<p>EXTERNAL-IPが割り当てられたら、Azure DNSで名前解決できるようにします。Azure CLIを使います。Ingressのホスト名をwww.example.comとする例です。このホスト名で、後ほどLet&rsquo;s Encryptから証明書を取得します。</p>

<pre><code>$ az network dns record-set a add-record -z example.com -g your-dnszone-rg -n www -a 52.234.148.138
</code></pre>

<p>cert-managerのソースをGitHubから取得し、contribからhelm installします。いずれstableを使えるようになるでしょう。なお、このAKSクラスターはまだRBACを使っていないので、&rdquo;&ndash;set rbac.create=false&rdquo;オプションを指定しています。</p>

<pre><code>$ git clone https://github.com/jetstack/cert-manager
$ cd cert-manager/
$ helm install --name cert-manager --namespace kube-system contrib/charts/cert-manager --set rbac.create=false
</code></pre>

<p>では任意の作業ディレクトリに移動し、以下の内容でマニフェストを作ります。cm-issuer-le-staging-sample.yamlとします。</p>

<pre><code>apiVersion: certmanager.k8s.io/v1alpha1
kind: Issuer
metadata:
  name: letsencrypt-staging
  namespace: default
spec:
  acme:
    # The ACME server URL
    server: https://acme-staging.api.letsencrypt.org/directory
    # Email address used for ACME registration
    email: hoge@example.com
    # Name of a secret used to store the ACME account private key
    privateKeySecretRef:
      name: letsencrypt-staging
    # Enable the HTTP-01 challenge provider
    http01: {}
</code></pre>

<p>証明書を発行してもらうLet&rsquo;s EncryptをIssuerとして登録するわけですが、まずはステージングのAPIエンドポイントを指定しています。Let&rsquo;s Encryptには<a href="https://letsencrypt.org/docs/rate-limits/">Rate Limit</a>があり、失敗した時に痛いからです。Let&rsquo;s EncryptのステージングAPIを使うとフェイクな証明書(Fake LE Intermediate X1)が発行されますが、流れの確認やマニフェストの検証は、できます。</p>

<p>なお、Let&rsquo;s Encryptとのチャレンジには今回、HTTPを使います。DNSチャレンジも<a href="https://github.com/jetstack/cert-manager/pull/246">いずれ対応する見込み</a>です。</p>

<p>では、Issuerを登録します。</p>

<pre><code>$ kubectl apply -f cm-issuer-le-staging-sample.yaml
</code></pre>

<p>次は証明書の設定です。マニフェストはcm-cert-le-staging-sample.yamlとします。acme節にACME構成を書きます。チャレンジはHTTP、ingressClassはnginxです。</p>

<pre><code>apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: example-com
  namespace: default
spec:
  secretName: example-com-tls
  issuerRef:
    name: letsencrypt-staging
  commonName: www.example.com
  dnsNames:
  - www.example.com
  acme:
    config:
    - http01:
        ingressClass: nginx
      domains:
      - www.example.com
</code></pre>

<p>証明書設定をデプロイします。</p>

<pre><code>$ kubectl apply -f cm-cert-le-staging-sample.yaml
</code></pre>

<p>証明書の発行状況を確認します。</p>

<pre><code>$ kubectl describe certificate example-com
Name:         example-com
Namespace:    default
[snip]
Events:
  Type     Reason                 Age              From                     Message
  ----     ------                 ----             ----                     -------
  Warning  ErrorCheckCertificate  8m               cert-manager-controller  Error checking existing TLS certificate: secret &quot;example-com-tls&quot; not found
  Normal   PrepareCertificate     8m               cert-manager-controller  Preparing certificate with issuer
  Normal   PresentChallenge       8m               cert-manager-controller  Presenting http-01 challenge for domain www.example.com
  Normal   SelfCheck              8m               cert-manager-controller  Performing self-check for domain www.example.com
  Normal   ObtainAuthorization    7m               cert-manager-controller  Obtained authorization for domain www.example.com
  Normal   IssueCertificate       7m               cert-manager-controller  Issuing certificate...
  Normal   CeritifcateIssued      7m               cert-manager-controller  Certificated issuedsuccessfully
  Normal   RenewalScheduled       7m (x2 over 7m)  cert-manager-controller  Certificate scheduled for renewal in 1438 hours
</code></pre>

<p>無事に証明書が発行され、更新もスケジュールされました。手順やマニフェストの書きっぷりは問題なさそうです。これをもってステージング完了としましょう。</p>

<p>ではLet&rsquo;s EncryptのAPIエンドポイントをProduction向けに変更し、新たにIssuer登録します。cm-issuer-le-prod-sample.yamlとします。</p>

<pre><code>apiVersion: certmanager.k8s.io/v1alpha1
kind: Issuer
metadata:
  name: letsencrypt-prod
  namespace: default
spec:
  acme:
    # The ACME server URL
    server: https://acme-v01.api.letsencrypt.org/directory
    # Email address used for ACME registration
    email: hoge@example.com
    # Name of a secret used to store the ACME account private key
    privateKeySecretRef:
      name: letsencrypt-prod
    # Enable the HTTP-01 challenge provider
    http01: {}
</code></pre>

<p>デプロイします。</p>

<pre><code>$ kubectl apply -f cm-issuer-le-prod-sample.yaml
</code></pre>

<p>同様に、Production向けの証明書設定をします。cm-cert-le-prod-sample.yamlとします。</p>

<pre><code>apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: prod-example-com
  namespace: default
spec:
  secretName: prod-example-com-tls
  issuerRef:
    name: letsencrypt-prod
  commonName: www.example.com
  dnsNames:
  - www.example.com
  acme:
    config:
    - http01:
        ingressClass: nginx
      domains:
      - www.example.com
</code></pre>

<p>デプロイします。</p>

<pre><code>$ kubectl apply -f cm-cert-le-prod-sample.yaml
</code></pre>

<p>発行状況を確認します。</p>

<pre><code>$ kubectl describe certificate prod-example-com
Name:         prod-example-com
Namespace:    default
[snip]
Events:
  Type     Reason                 Age              From                     Message
  ----     ------                 ----             ----                     -------
  Warning  ErrorCheckCertificate  27s              cert-manager-controller  Error checking existing TLS certificate: secret &quot;prod-example-com-tls&quot; not found
  Normal   PrepareCertificate     27s              cert-manager-controller  Preparing certificate with issuer
  Normal   PresentChallenge       26s              cert-manager-controller  Presenting http-01 challenge for domain www.example.com
  Normal   SelfCheck              26s              cert-manager-controller  Performing self-check for domain www.example.com
  Normal   IssueCertificate       7s               cert-manager-controller  Issuing certificate...
  Normal   ObtainAuthorization    7s               cert-manager-controller  Obtained authorization for domain www.example.com
  Normal   RenewalScheduled       6s (x3 over 5m)  cert-manager-controller  Certificate scheduled for renewal in 1438 hours
  Normal   CeritifcateIssued      6s               cert-manager-controller  Certificated issuedsuccessfully
</code></pre>

<p>証明書が発行され、1438時間(約60日)内の更新がスケジュールされました。</p>

<p>ではバックエンドを設定して確認してみましょう。バックエンドにNGINXを立て、exposeします。</p>

<pre><code>$ kubectl run nginx --image nginx --port 80
$ kubectl expose deployment nginx --type NodePort
</code></pre>

<p>Ingressを設定します。ファイル名はingress-nginx-sample.yamlとします。</p>

<pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
  name: ingress-nginx-sample
spec:
  rules:
    - host: www.example.com
      http:
        paths:
          - path: /
            backend:
              serviceName: nginx
              servicePort: 80
  tls:
    - hosts:
      - www.example.com
      secretName: prod-example-com-tls
</code></pre>

<p>デプロイします。</p>

<pre><code>$ kubectl apply -f ingress-nginx-sample.yaml
</code></pre>

<p>いざ確認。</p>

<pre><code>$ curl https://www.example.com/
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
[snip]
</code></pre>

<p>便利ですね。Let&rsquo;s Encryptをはじめ、関連プロジェクトに感謝です。</p>

                    </div>
                </section>
                
                <h1 class="content-subhead">10 Feb 2018, 11:00</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="http://torumakabe.github.io/post/aks_ingress_quickdeploy/" class="post-title">AKSのNGINX Ingress Controllerのデプロイで悩んだら</a>

                        <p class="post-meta">
                            
                            
                                under 
                                
                                <a class="post-category post-category-Azure" href="http://torumakabe.github.io//categories/azure">Azure</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        

<h2 id="楽したいならhelmで入れましょう">楽したいならhelmで入れましょう</h2>

<p>AKSに限った話ではありませんが、Kubernetesにぶら下げるアプリの数が多くなってくると、URLマッピングやTLS終端がしたくなります。方法は色々あるのですが、シンプルな選択肢はNGINX Ingress Controllerでしょう。</p>

<p>さて、そのNGINX Ingress Controllerのデプロイは<a href="https://github.com/kubernetes/ingress-nginx/blob/master/deploy/README.md">GitHubのドキュメント</a>通りに淡々とやればいいのですが、<a href="https://github.com/kubernetes/helm">helm</a>を使えばコマンド一発です。そのようにドキュメントにも書いてあるのですが、最後の方で出てくるので「それ早く言ってよ」な感じです。</p>

<p>せっかくなので、Azure(AKS)での使い方をまとめておきます。開発ペースやエコシステムの変化が速いので要注意。この記事は2018/2/10に書いています。</p>

<h2 id="使い方">使い方</h2>

<p>AKSクラスターと、Azure DNS上に利用可能なゾーンがあることを前提にします。ない場合、それぞれ公式ドキュメントを参考にしてください。</p>

<ul>
<li><a href="https://docs.microsoft.com/ja-jp/azure/aks/kubernetes-walkthrough">Azure Container Service (AKS) クラスターのデプロイ</a></li>
<li><a href="https://docs.microsoft.com/ja-jp/azure/dns/dns-getstarted-cli">Azure CLI 2.0 で Azure DNS の使用を開始する</a></li>
</ul>

<p>ではhelmでNGINX Ingress Controllerを導入します。helmを使っていなければ、<a href="https://github.com/kubernetes/helm#install">入れておいてください</a>。デプロイはこれだけ。Chartは<a href="https://github.com/kubernetes/charts/tree/master/stable/nginx-ingress">ここ</a>。</p>

<pre><code>$ helm install stable/nginx-ingress --name my-nginx
</code></pre>

<p>バックエンドへのつなぎが機能するか、Webアプリを作ってテストします。NGINXとApacheを選びました。</p>

<pre><code>$ kubectl run nginx --image nginx --port 80
$ kubectl run apache --image httpd --port 80
</code></pre>

<p>サービスとしてexposeします。</p>

<pre><code>$ kubectl expose deployment nginx --type NodePort
$ kubectl expose deployment apache --type NodePort
</code></pre>

<p>現時点のサービスたちを確認します。</p>

<pre><code>$ kubectl get svc
NAME                                     TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)                  AGE
apache                                   NodePort       10.0.244.167   &lt;none&gt;          80:30928/TCP                 14h
kubernetes                               ClusterIP      10.0.0.1       &lt;none&gt;          443/TCP                  79d
my-nginx-nginx-ingress-controller        LoadBalancer   10.0.91.78     13.72.108.187   80:32448/TCP,443:31991/TCP   14h
my-nginx-nginx-ingress-default-backend   ClusterIP      10.0.74.104    &lt;none&gt;          80/TCP                  14h
nginx                                    NodePort       10.0.191.16    &lt;none&gt;          80:30752/TCP                 14h
</code></pre>

<p>AKSの場合はパブリックIPがNGINX Ingress Controllerに割り当てられます。EXTERNAL-IPがpendingの場合は割り当て中なので、しばし待ちます。</p>

<p>割り当てられたら、EXTERNAL-IPをAzure DNSで名前解決できるようにしましょう。Azure CLIを使います。dev.example.comの例です。</p>

<pre><code>$ az network dns record-set a add-record -z example.com -g your-dnszone-rg -n dev -a 13.72.108.187
</code></pre>

<p>TLSが終端できるかも検証したいので、Secretを作ります。証明書とキーはLet&rsquo;s Encryptで作っておきました。</p>

<pre><code>$ kubectl create secret tls example-tls --key privkey.pem --cert fullchain.pem
</code></pre>

<p>ではIngressを構成しましょう。以下をファイル名ingress-nginx-sample.yamlとして保存します。IngressでTLSを終端し、/へのアクセスは先ほどexposeしたNGINXのサービスへ、/apacheへのアクセスはApacheへ流します。rewrite-targetをannotaionsで指定するのを、忘れずに。</p>

<pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
  name: ingress-nginx-sample
spec:
  rules:
    - host: dev.example.com
      http:
        paths:
          - path: /
            backend:
              serviceName: nginx
              servicePort: 80
          - path: /apache
            backend:
              serviceName: apache
              servicePort: 80
  tls:
    - hosts:
      - dev.example.com
      secretName: example-tls
</code></pre>

<p>あとは反映するだけ。</p>

<pre><code>$ kubectl apply -f ingress-nginx-sample.yaml
</code></pre>

<p>curlで確認します。</p>

<pre><code>$ curl https://dev.example.com
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
[snip]
</code></pre>

<p>/apacheへのパスも確認します。</p>

<pre><code>$ curl https://dev.example.com/apache
&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;
</code></pre>

<p>簡単ですね。</p>

                    </div>
                </section>
                
            </div>
            
<div class="pagination">
  <nav role="pagination" class="post-list-pagination">
      
    <span class="post-list-pagination-item post-list-pagination-item-current">Page 1 of 12</span>
    
      <a href="/post/page/2/" class="post-list-pagination-item pure-button post-list-pagination-item-next">
        Older&nbsp;<i class="fa fa-angle-double-right"></i>
      </a>
    
  </nav>
</div>


            <div class="footer">
    <div class="pure-menu pure-menu-horizontal pure-menu-open">
        <ul>
            <li>Powered by <a class="hugo" href="http://hugo.spf13.com/" target="_blank">hugo</a></li>
        </ul>
    </div>
</div>
<script src="http://torumakabe.github.io//js/all.min.js"></script>
        </div>
    </div>
</div>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', '', 'auto');
ga('send', 'pageview');

</script>

</body>
</html>
