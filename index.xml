<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>re-imagine</title>
    <link>https://ToruMakabe.github.io/</link>
    <description>Recent content on re-imagine</description>
    <generator>Hugo - gohugo.io</generator>
    <language>en</language>
    <contact></contact>
    <copyright>&copy; Copyright 2019 Toru Makabe</copyright>
    
        <atom:link href="https://ToruMakabe.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>作りかけのAKSクラスターにTerraformで追いプロビジョニングする</title>
      <link>https://ToruMakabe.github.io/post/additional_aks_provisioning_with_tf/</link>
      <pubDate>Fri, 26 Apr 2019 18:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/additional_aks_provisioning_with_tf/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;何の話か&#34;&gt;何の話か&lt;/h2&gt;

&lt;p&gt;CLIやポータルで作ったAKSクラスターに、後からIstioなどの基盤ソフトや運用関連のツールを後から入れるのが面倒なので、Terraformを使って楽に入れよう、という話です。アプリのデプロイメントとは分けられる話なので、触れません。&lt;/p&gt;

&lt;h2 id=&#34;動機&#34;&gt;動機&lt;/h2&gt;

&lt;p&gt;Azure CLIやポータルを使えば、AKSクラスターを楽に作れます。加えてAzure Monitorとの連携など、多くのユーザーが必要とする機能は、作成時にオプションで導入できます。&lt;/p&gt;

&lt;p&gt;ですが、実際にAKSクラスターを運用するなら、他にも導入したいインフラ関連の基盤ソフトやツールが出てきます。たとえばわたしは最近、クラスターを作る度に後追いでこんなものを入れています。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Istio&lt;/li&gt;
&lt;li&gt;Kured (ノードOSに再起動が必要なパッチが当たった時、ローリング再起動してくれる)&lt;/li&gt;
&lt;li&gt;HelmのTiller (helm initで作ると守りが緩いので、localhostに限定したdeploymentで入れたい)&lt;/li&gt;
&lt;li&gt;AKSマスターコンポーネントのログ転送設定 (Azure Diagnostics)&lt;/li&gt;
&lt;li&gt;リアルタイムコンテナーログ表示設定&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;kubectlやAzure CLIでコツコツ設定すると、まあ、めんどくさいわけです。クラスター作成時にAzure CLIやポータルで入れてくれたらなぁ、と思わなくもないですが、これらがみなに必要かという疑問ですし、多くを飲み込もうと欲張ると肥大化します。Kubernetesエコシステムは新陳代謝が激しいので、現状の提供機能は妥当かな、と感じます。&lt;/p&gt;

&lt;p&gt;とはいえクラスターを作るたびの追加作業量が無視できないので、わたしはTerraformをよく使います。Azure、Kubernetesリソースを同じツールで扱えるからです。環境をまるっと作成、廃棄できて、とても便利。&lt;a href=&#34;https://www.amazon.co.jp/dp/B07L94XGPY&#34;&gt;今年のはじめに書いた本&lt;/a&gt;でも、Terraformの活用例を紹介しています。サンプルコードは&lt;a href=&#34;https://github.com/ToruMakabe/Understanding-K8s&#34;&gt;こちら&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;で、ここまでは、Terraform Azure Providerが、使いたいAKSの機能をサポートしていれば、の話。ここからがこのエントリーの本題です。&lt;/p&gt;

&lt;p&gt;AKSはインパクトの大きな機能を、プレビューというかたちで順次提供しています。プレビュー期間にユーザーとともに実績を積み、GAに持っていきます。たとえば2019/4時点で、下記のプレビューが提供されています。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/aks/virtual-nodes-cli&#34;&gt;Virtual Node&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/aks/cluster-autoscaler&#34;&gt;Cluster Autoscaler (on Virtual Machine Scale Sets)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/aks/use-network-policies&#34;&gt;Network Policy (with Calico)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/aks/use-pod-security-policies&#34;&gt;Pod Security Policy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/cli/azure/ext/aks-preview/aks/nodepool?view=azure-cli-latest&#34;&gt;Multi Nodepool&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/azure-monitor/insights/container-insights-live-logs&#34;&gt;リアルタイムコンテナーログ表示&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;これらの機能に、すぐにTerraformが対応するとは限りません。たいてい、遅れてやってきます。ということは、使うなら二択です。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Terraformの対応を待つ、貢献する&lt;/li&gt;
&lt;li&gt;Azure CLIやポータルでプレビュー機能を有効化したクラスターを作り、Terraformで追いプロビジョニングする&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;インパクトの大きい機能は、その価値やリスクを見極めるため、早めに検証に着手したいものです。早めに着手すれば、要否を判断したり運用に組み込む時間を確保しやすいでしょう。そしてその時、本番に近い環境を楽に作れるようにしておけば、幸せになれます。&lt;/p&gt;

&lt;p&gt;ということで前置きが長くなりましたが、2が今回やりたいことです。本番のクラスター運用、というよりは、検証環境のセットアップを楽に、という話です。&lt;/p&gt;

&lt;h2 id=&#34;意外に知られていない-terraform-data-source&#34;&gt;意外に知られていない Terraform Data Source&lt;/h2&gt;

&lt;p&gt;Terraformを使い始めるとすぐにその存在に気付くのですが、使う前には気付きにくいものがいくつかあります。その代表例がData Sourceです。ざっくりいうと、参照用のリソースです。&lt;/p&gt;

&lt;p&gt;Terraformはリソースを&amp;rdquo;API Management Resource(resource)&amp;ldquo;として定義すると、作成から廃棄まで、ライフサイクル全体の面倒をみます。つまりresourceとして定義したものをapplyすれば作成し、destroyすれば廃棄します。いっぽうでData Source(data)は参照用ですので、定義したリソースに、変更を加えません。&lt;/p&gt;

&lt;p&gt;たとえば、AKSマスターコンポーネントのログをLog Analyticsへ転送するために、Azure Diagnoticsリソースを作成するとしましょう。作成には、対象となる既存AKSクラスターのIDとLog AnalyticsのWorkspace IDが要ります。IDとは、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/subscriptions/hogehoge/resourcegroups/fugafuga/providers/Microsoft.ContainerService/managedClusters/hogefuga
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;とかいうやつです。いちいち調べるの、めんどくさい。&lt;/p&gt;

&lt;p&gt;そこで、AKSクラスターとLog AnalyticsのWorkspaceをdataとして定義します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data &amp;quot;azurerm_kubernetes_cluster&amp;quot; &amp;quot;aks&amp;quot; {
  name                = &amp;quot;${var.aks_cluster_name}&amp;quot;
  resource_group_name = &amp;quot;${var.aks_cluster_rg}&amp;quot;
}

data &amp;quot;azurerm_log_analytics_workspace&amp;quot; &amp;quot;aks&amp;quot; {
  name                = &amp;quot;${var.la_workspace_name_for_aks}&amp;quot;
  resource_group_name = &amp;quot;${var.la_workspace_rg_for_aks}&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;それを次のように、resource作成時に参照できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;resource &amp;quot;azurerm_monitor_diagnostic_setting&amp;quot; &amp;quot;aks&amp;quot; {
  name                       = &amp;quot;diag_aks&amp;quot;
  target_resource_id         = &amp;quot;${data.azurerm_kubernetes_cluster.aks.id}&amp;quot;
  log_analytics_workspace_id = &amp;quot;${data.azurerm_log_analytics_workspace.aks.id}&amp;quot;

  log {
    category = &amp;quot;kube-apiserver&amp;quot;
    enabled  = true

    retention_policy {
      enabled = false
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;また、Kubernetesを操作するProvider登録時に、認証情報を渡すこともできます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;provider &amp;quot;kubernetes&amp;quot; {
  load_config_file       = false
  host                   = &amp;quot;${data.azurerm_kubernetes_cluster.aks.kube_config.0.host}&amp;quot;
  client_certificate     = &amp;quot;${base64decode(data.azurerm_kubernetes_cluster.aks.kube_config.0.client_certificate)}&amp;quot;
  client_key             = &amp;quot;${base64decode(data.azurerm_kubernetes_cluster.aks.kube_config.0.client_key)}&amp;quot;
  cluster_ca_certificate = &amp;quot;${base64decode(data.azurerm_kubernetes_cluster.aks.kube_config.0.cluster_ca_certificate)}&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;こうしておけば、TerraformがKubernetesを操作できます。以下はkured用のサービスアカウントを定義する例です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;resource &amp;quot;kubernetes_service_account&amp;quot; &amp;quot;kured&amp;quot; {
  metadata {
    name      = &amp;quot;kured&amp;quot;
    namespace = &amp;quot;kube-system&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;なお、Terraform実行ホスト上のKubernetes configファイルを使って&lt;a href=&#34;https://www.terraform.io/docs/providers/kubernetes/guides/getting-started.html&#34;&gt;認証する&lt;/a&gt;こともできます。要件に合わせて選択しましょう。&lt;/p&gt;

&lt;p&gt;ここまでを、まとめます。Azure CLIやポータルを使って作ったAKSクラスターに対し、TerraformのData SourceやKubenetesのconfigファイルを使って属性、認証情報を取得し、追加プロビジョニングを一括で、楽にできる、という話でした。&lt;/p&gt;

&lt;h2 id=&#34;サンプル&#34;&gt;サンプル&lt;/h2&gt;

&lt;p&gt;ではどんなことができるのか、サンプルをGistに置いておきました。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://gist.github.com/ToruMakabe/46ac0b31f7f8a07fa9a1254f862bc15c&#34;&gt;サンプルコード&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;冒頭で挙げた、以下リソースの導入や設定ができます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Istio&lt;/li&gt;
&lt;li&gt;Kured&lt;/li&gt;
&lt;li&gt;HelmのTiller&lt;/li&gt;
&lt;li&gt;AKSマスターコンポーネントのログ転送設定&lt;/li&gt;
&lt;li&gt;リアルタイムコンテナーログ表示設定&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;HelmとIstioは開発の流れが速いので、ワークアラウンド多めです。詳細はソース上のコメントを参考にしてください。&lt;/p&gt;

&lt;h3 id=&#34;使い方&#34;&gt;使い方&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Gist上の3ファイルを同じディレクトリに置く&lt;/li&gt;
&lt;li&gt;variables.tf.sampleの変数を設定し、ファイルをリネーム(.sampleを消し、.tfにする)&lt;/li&gt;
&lt;li&gt;Terraform導入済みのホストで実行

&lt;ul&gt;
&lt;li&gt;WSL(Ubuntu 18.04)とmasOS Mojaveで動作検証しています&lt;/li&gt;
&lt;li&gt;Terraformの設定から確認したい場合は以下を参考に&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/virtual-machines/linux/terraform-install-configure&#34;&gt;VM などのインフラストラクチャを Azure にプロビジョニングするための Terraform のインストールと構成&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learn.hashicorp.com/terraform/?track=azure#azure&#34;&gt;Getting started with Terraform using the Azure provider&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;誰かのお役に、立ちますように。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>GitHub ActionsでAzure CLIとkubectlを動かす</title>
      <link>https://ToruMakabe.github.io/post/github_actions_aks/</link>
      <pubDate>Sat, 22 Dec 2018 20:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/github_actions_aks/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;github-actionsのプレビュー招待がきた&#34;&gt;GitHub Actionsのプレビュー招待がきた&lt;/h2&gt;

&lt;p&gt;ので試します。プレビュー中なので細かいことは抜きに、ざっくりどんなことができるか。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://developer.github.com/actions/&#34;&gt;GitHub Actions&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;数時間、触った印象。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GitHubへのPushなどイベントをトリガーにWorkflowを流せる&lt;/li&gt;
&lt;li&gt;シンプルなWorkflow記法 (TerraformのHCLに似ている)&lt;/li&gt;
&lt;li&gt;Workflowから呼び出すActionはDockerコンテナー&lt;/li&gt;
&lt;li&gt;Dockerコンテナーをビルドしておかなくてもいい (Dockerfileをリポジトリに置けば実行時にビルドされる)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Dockerに慣れていて、ちょっとしたタスクの自動化を、GitHubで完結したい人に良さそうです。&lt;/p&gt;

&lt;h2 id=&#34;azure-cli-kubernetes-aks-kubectlサンプル&#34;&gt;Azure CLI/Kubernetes(AKS) kubectlサンプル&lt;/h2&gt;

&lt;p&gt;こんなことを試してみました。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;KubernetesのマニフェストをGitHubリポジトリへPush&lt;/li&gt;
&lt;li&gt;PushイベントをトリガーにWorkflowを起動&lt;/li&gt;
&lt;li&gt;Azure CLIを使ってAKSクラスターのCredentialを取得&lt;/li&gt;
&lt;li&gt;イベント発生元がmasterブランチであれば継続&lt;/li&gt;
&lt;li&gt;kubectl applyでマニフェストを適用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;kubectlを制限したい、証明書を配るのめんどくさい、なのでGitHubにPushされたらActionsでデプロイ、ってシナリオです。がっつり使うにはまだ検証足らずですが、ひとまずできることは確認しました。&lt;/p&gt;

&lt;p&gt;コードは &lt;a href=&#34;https://github.com/ToruMakabe/actions-playground&#34;&gt;ここ&lt;/a&gt; に。&lt;/p&gt;

&lt;p&gt;ディレクトリ構造は、こうです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.
├── .git
│   └── (省略)
├── .github
│   └── main.workflow
├── LICENSE
├── README.md
├── azure-cli
│   ├── Dockerfile
│   └── entrypoint.sh
└── sampleapp.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;.github の下にWorkflowを書きます&lt;/li&gt;
&lt;li&gt;azure-cli の下に自作Actionを置きました&lt;/li&gt;
&lt;li&gt;sampleapp.yaml がkubernetesのマニフェストです&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;workflow&#34;&gt;Workflow&lt;/h3&gt;

&lt;p&gt;まず、 .github/main.workflow を見てみましょう&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;workflow &amp;quot;Deploy app to AKS&amp;quot; {
  on = &amp;quot;push&amp;quot;
  resolves = [&amp;quot;Deploy to AKS&amp;quot;]
}

action &amp;quot;Load AKS credential&amp;quot; {
  uses = &amp;quot;./azure-cli/&amp;quot;
  secrets = [&amp;quot;AZURE_SERVICE_APP_ID&amp;quot;, &amp;quot;AZURE_SERVICE_PASSWORD&amp;quot;, &amp;quot;AZURE_SERVICE_TENANT&amp;quot;]
  args = &amp;quot;aks get-credentials -g $AKS_RG_NAME -n $AKS_CLUSTER_NAME -a&amp;quot;
  env = {
    AZ_OUTPUT_FORMAT = &amp;quot;table&amp;quot;
    AKS_RG_NAME = &amp;quot;your-aks-rg&amp;quot;
    AKS_CLUSTER_NAME = &amp;quot;youraks&amp;quot;
  }
}

action &amp;quot;Deploy branch filter&amp;quot; {
  uses = &amp;quot;actions/bin/filter@master&amp;quot;
  args = &amp;quot;branch master&amp;quot;
}

action &amp;quot;Deploy to AKS&amp;quot; {
  needs = [&amp;quot;Load AKS credential&amp;quot;, &amp;quot;Deploy branch filter&amp;quot;]
  uses = &amp;quot;docker://gcr.io/cloud-builders/kubectl&amp;quot;
  runs = &amp;quot;sh -l -c&amp;quot;
  args = [&amp;quot;cat $GITHUB_WORKSPACE/sampleapp.yaml |  sed -e &#39;s/YOUR_VALUE/&#39;\&amp;quot;$YOUR_VALUE\&amp;quot;&#39;/&#39; -e &#39;s/YOUR_DNS_LABEL_NAME/&#39;$YOUR_DNS_LABEL_NAME&#39;/&#39; | kubectl apply -f - &amp;quot;]
  env = {
    YOUR_VALUE = &amp;quot;Ale&amp;quot;
    YOUR_DNS_LABEL_NAME = &amp;quot;yournamedispvar&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;シンプルですね。全体を定義するworkflowブロックと、それぞれのActionを書くactionブロックがあります。記法やオプションは&lt;a href=&#34;https://developer.github.com/actions/creating-workflows/&#34;&gt;ドキュメント&lt;/a&gt;を読めばだいたい分かります。依存関係はneedsで書ける。&lt;/p&gt;

&lt;p&gt;それぞれのactionブロックでDockerコンテナーを呼び出します。usesで指定したディレクトリにDockerファイルをおいておけばビルドされ、そのactionで使えます。&amp;rdquo;Load AKS credential&amp;rdquo;ブロックがその例です。&amp;rdquo;./azure-cli/&amp;ldquo;にDockerfileとエントリーポイントとなるbashスクリプトを置きます。&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Deploy branch filter&amp;rdquo;ブロックは&lt;a href=&#34;https://github.com/actions&#34;&gt;GitHub Actionsが提供している&lt;/a&gt;コンテナーの、&amp;rdquo;Deploy to AKS&amp;rdquo;は外部Dockerレジストリーを利用した例です。詳しくは後ほど。&lt;/p&gt;

&lt;p&gt;リポジトリでPushイベントが発生すると、Workflowが実行されます。リポジトリの&amp;rdquo;Actions&amp;rdquo;タブで実行結果を確認できます。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ToruMakabe/Images/master/ghaction_sc.png&#34; alt=&#34;Workflow&#34; /&gt;&lt;/p&gt;

&lt;p&gt;できた。Azure CLIとkubectlが使えるなら、他にも応用できそう。&lt;/p&gt;

&lt;p&gt;以下、actionブロックを補足します。&lt;/p&gt;

&lt;h3 id=&#34;load-aks-credential&#34;&gt;Load AKS credential&lt;/h3&gt;

&lt;p&gt;Azure CLIを使ってAKSクラスターのCredentialを取得し、kubectlのコンテキスト設定します。envでクラスターのリソースグループ名とクラスター名を渡しています。&lt;/p&gt;

&lt;p&gt;GitHub ActionsがAzure CLI Actionを&lt;a href=&#34;https://github.com/actions/azure&#34;&gt;提供している&lt;/a&gt;のですが、Azure CLIのバージョンを最新にしたかったのと、動的にコンテナーを作ってみたかったので ./azure-cli にDockerfileを&lt;a href=&#34;https://github.com/ToruMakabe/actions-playground/tree/master/azure-cli&#34;&gt;書きました&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;試す時は、みなさんの環境向けにenvを書き換えてください。なお、AKSクラスターからCredentialを取得する権限が必要です。権限を持ったサービスプリンシパルのアプリケーションID/パスワード、テナントIDをGitHubリポジトリのSecretに設定してください。actionブロックのsecretsで指定している通りです。&lt;/p&gt;

&lt;h3 id=&#34;deploy-branch-filter&#34;&gt;Deploy branch filter&lt;/h3&gt;

&lt;p&gt;bin/filterはGitHub Actionsが&lt;a href=&#34;https://github.com/actions/bin&#34;&gt;提供している&lt;/a&gt;ユーティリティです。この例では、イベントがmasterブランチで発生した場合のみWorkflowを継続します。&lt;/p&gt;

&lt;h3 id=&#34;deploy-to-aks&#34;&gt;Deploy to AKS&lt;/h3&gt;

&lt;p&gt;gcr.ioからkubectlコンテナーを取得し、実行しています。マニフェストの一部を動的に変えたいことは多いので、sedでマニフェストの一部を置換する例にしました。&lt;/p&gt;

&lt;p&gt;このactionが実行されると、envの&amp;rdquo;YOUR_VALUE&amp;rdquo;にセットした文字列を表示する&lt;a href=&#34;https://github.com/ToruMakabe/container-simpledemo/blob/master/displayEnvVar/main.go&#34;&gt;Golang Webアプリ&lt;/a&gt;のDeploymentと公開用のServiceができます。&amp;rdquo;YOUR_DNS_LABEL_NAME&amp;rdquo;にはDNSラベル名を指定でき、FQDNはAKSクラスターの配置リージョンに応じて決定されます。東日本リージョンの場合、YOUR_DNS_LABEL_NAME.japaneast.cloudapp.azure.com となります。&lt;/p&gt;

&lt;p&gt;アプリのパスは /dispvar です。curlしてみると。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl http://yournamedispvar.japaneast.cloudapp.azure.com/dispvar
Hello. You set &amp;quot;Ale&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;

&lt;p&gt;Dockerに慣れていれば便利に使えるかな、という印象です。Terraformのfmt/validate/plan用Actionなども&lt;a href=&#34;https://www.terraform.io/docs/github-actions/index.html&#34;&gt;公開されています&lt;/a&gt;。がっつりパイプラインを作らないとしても、コードのフォーマットや構文チェックに良さそうです。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>WSLENVでWSLとWindowsの環境変数を共有する(Go開発環境編)</title>
      <link>https://ToruMakabe.github.io/post/wslenv_golang/</link>
      <pubDate>Wed, 02 May 2018 17:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/wslenv_golang/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;見た目は地味だが役に立つ&#34;&gt;見た目は地味だが役に立つ&lt;/h2&gt;

&lt;p&gt;Windows 10 April 2018 Update (別名: バージョン1803)がリリースされました。タイムラインなど目立つ機能が注目されていますが、開発者支援系の機能、ツールも&lt;a href=&#34;https://blogs.msdn.microsoft.com/commandline/2018/03/07/windows10v1803/&#34;&gt;拡充&lt;/a&gt;されています。特に、WSL/Windowsの連携、相互運用まわりは着実に進化しています。そのうちのひとつが、このエントリーで紹介するWSLENVです。&lt;/p&gt;

&lt;p&gt;WSLENVは、WSL/Windows間で環境変数を共有する仕組みです。ただ単純に共有するだけでなく、ルールに従って変換も行います。これが地味に便利。でも地味だから、あまり話題になっていない。なので具体例で紹介しよう、というのがこのエントリーの目的です。&lt;/p&gt;

&lt;h2 id=&#34;tl-dr&#34;&gt;TL;DR&lt;/h2&gt;

&lt;p&gt;英語が読めて、「あ、それ便利ね」とピンとくる人は以下を。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blogs.msdn.microsoft.com/commandline/2017/12/22/share-environment-vars-between-wsl-and-windows/&#34;&gt;Share Environment Vars between WSL and Windows&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;go開発環境を例に&#34;&gt;Go開発環境を例に&lt;/h2&gt;

&lt;p&gt;前述のリンクでも紹介されていますが、Goの開発環境はWSLENVの代表的なユースケースです。GOPATHをいい感じにWSL/Windowsで共有できます。掘り下げていきましょう。&lt;/p&gt;

&lt;h3 id=&#34;想定開発者像-ペルソナ&#34;&gt;想定開発者像、ペルソナ&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Windows端末を使っている&lt;/li&gt;
&lt;li&gt;Go言語を使っている&lt;/li&gt;
&lt;li&gt;CLIはbash/WSL中心

&lt;ul&gt;
&lt;li&gt;スクリプト書くならPowerShellもいいけど、インタラクティブな操作はbashが楽&lt;/li&gt;
&lt;li&gt;アプリをDockerコンテナーとしてビルドするなど、OSSエコシステム、ツールとの連携を考慮&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;とはいえエディタ/IDEはWindows側で動かしたい、最近はVS Code中心&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;前提条件&#34;&gt;前提条件&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;WSL、WindowsそれぞれにGoを導入

&lt;ul&gt;
&lt;li&gt;バージョン管理のためにも、パッケージマネージャーがおすすめ&lt;/li&gt;
&lt;li&gt;わたしはWSL(Ubuntu)でapt、WindowsではChocolateyを使ってGoを導入しています&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;GOPATHは %USERPROFILE%go とする

&lt;ul&gt;
&lt;li&gt;ユーザー名を tomakabeとすると C:\Users\tomakabe\go&lt;/li&gt;
&lt;li&gt;setx GOPATH &amp;ldquo;$env:USERPROFILE\go&amp;rdquo; で設定&lt;/li&gt;
&lt;li&gt;WSLでもこのディレクトリをGOPATHとする&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;VS Code + &lt;a href=&#34;https://github.com/Microsoft/vscode-go&#34;&gt;Go拡張&lt;/a&gt;をWindowsに導入&lt;/li&gt;
&lt;li&gt;WindowsのCLIはPowerShellを利用&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;そぞろ歩き-その1-windowsでのgo開発&#34;&gt;そぞろ歩き その1(WindowsでのGo開発)&lt;/h3&gt;

&lt;p&gt;では、何が課題で、WSLがどのようにそれを解決するか、見ていきましょう。&lt;/p&gt;

&lt;p&gt;まず、Windowsで環境変数GOPATHを確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\WINDOWS\system32&amp;gt; Get-ChildItem env:GOPATH

Name                           Value
----                           -----
GOPATH                         C:\Users\tomakabe\go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;GOPATHに移動し、ディレクトリ構造を確認します。この環境にはすでにディレクトリbinとsrcがあり、binにはいくつかexeが入っています。VS CodeのGo拡張を入れると導入を促されるツール群は、ここに格納され、構文チェックや補完でVS Codeと連動します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\WINDOWS\system32&amp;gt; cd C:\Users\tomakabe\go
PS C:\Users\tomakabe\go&amp;gt; ls


    ディレクトリ: C:\Users\tomakabe\go


Mode                LastWriteTime         Length Name
----                -------------         ------ ----
d-----       2018/05/02     11:10                bin
d-----       2018/05/02     11:06                src

PS C:\Users\tomakabe\go&amp;gt; ls .\bin\


    ディレクトリ: C:\Users\tomakabe\go\bin


Mode                LastWriteTime         Length Name
----                -------------         ------ ----
-a----       2018/05/02     11:10       14835200 dlv.exe
-a----       2018/05/02     11:09        4239360 go-outline.exe
-a----       2018/05/02     11:09        4045824 go-symbols.exe
-a----       2018/05/02     11:08       11094528 gocode.exe
-a----       2018/05/02     11:09        5708288 godef.exe
[snip]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サンプルコードのディレクトリへ移動し、中身を確認します。シンプルな挨拶アプリです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Users\tomakabe\go&amp;gt; cd .\src\github.com\ToruMakabe\work\
PS C:\Users\tomakabe\go\src\github.com\ToruMakabe\work&amp;gt; cat .\hello.go
package main

import &amp;quot;fmt&amp;quot;

func main() {
        fmt.Println(&amp;quot;Hello Go on the new WSL&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ビルドして動かしてみましょう。Windows環境ではデフォルトで実行ファイルとしてexeが作られます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Users\tomakabe\go\src\github.com\ToruMakabe\work&amp;gt; go build .\hello.go
PS C:\Users\tomakabe\go\src\github.com\ToruMakabe\work&amp;gt; ls


    ディレクトリ: C:\Users\tomakabe\go\src\github.com\ToruMakabe\work


Mode                LastWriteTime         Length Name
----                -------------         ------ ----
-a----       2018/05/02     11:54        2049536 hello.exe
-a----       2018/05/02     11:10             91 hello.go


PS C:\Users\tomakabe\go\src\github.com\ToruMakabe\work&amp;gt; .\hello.exe
Hello Go on the new WSL
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここまでは従来のWindowsにおけるGo開発環境です。ではWSLに話を移しましょう。&lt;/p&gt;

&lt;h3 id=&#34;そぞろ歩き-その2-wslでのgo開発&#34;&gt;そぞろ歩き その2(WSLでのGo開発)&lt;/h3&gt;

&lt;p&gt;WSLにつなぎます。ターミナルは任意ですが、わたしはVS Codeの統合ターミナルが好きです。コードを書きながら操作できるので。&lt;/p&gt;

&lt;p&gt;GOPATHを確認します。空っぽです。WSLは既定でWindowsから環境変数PATHを受け取ります。PATHは特別扱いです。ですが、他の環境変数は、指定しないと渡されません。よってWindowsで設定していても、WSLから見るとGOPATHは空っぽです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~ $ echo $GOPATH

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;$HOMEもきれいな状態です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~ $ ls
~ $
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ではGOPATHに指定したい、先ほどWindowsで確認したディレクトリへ移動します。ちなみにWindowsのCドライブはWSLで/mnt/c/に変換されます。先ほど確認したbin、srcが見えています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~ $ cd /mnt/c/Users/tomakabe/go/
/mnt/c/Users/tomakabe/go $ ls
bin  src
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ではここで実験。試しにパッケージをインポートしてみましょう。定番の&lt;a href=&#34;https://godoc.org/golang.org/x/tools/cmd/goimports&#34;&gt;goimports&lt;/a&gt;をインポートしてみます。わざとらしいですが、なんだか嫌な予感がします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/mnt/c/Users/tomakabe/go $ go get -v golang.org/x/tools/cmd/goimports
Fetching https://golang.org/x/tools/cmd/goimports?go-get=1
Parsing meta tags from https://golang.org/x/tools/cmd/goimports?go-get=1 (status code 200)
get &amp;quot;golang.org/x/tools/cmd/goimports&amp;quot;: found meta tag get.metaImport{Prefix:&amp;quot;golang.org/x/tools&amp;quot;, VCS:&amp;quot;git&amp;quot;, RepoRoot:&amp;quot;https://go.googlesource.com/tools&amp;quot;} at https://golang.org/x/tools/cmd/goimports?go-get=1
get &amp;quot;golang.org/x/tools/cmd/goimports&amp;quot;: verifying non-authoritative meta tag
Fetching https://golang.org/x/tools?go-get=1
Parsing meta tags from https://golang.org/x/tools?go-get=1 (status code 200)
golang.org/x/tools (download)
created GOPATH=/home/tomakabe/go; see &#39;go help gopath&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;嫌な予感は予定調和で的中します。GOPATHがいらっしゃらないので、/home/tomakabe/go とみなしてしまいました。先ほど確認した際、$HOMEはきれいな状態でした、が。新たにお作りになられたようです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/mnt/c/Users/tomakabe/go $ ls ~/
go
/mnt/c/Users/tomakabe/go $ ls ~/go
bin  src
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これではWSLとWindowsで、ソースもバイナリーも別々の管理になってしまいます。これはつらい。ああ、GOPATHを共有できればいいのに。&lt;/p&gt;

&lt;h3 id=&#34;そぞろ歩き-その3-解決編&#34;&gt;そぞろ歩き その3(解決編)&lt;/h3&gt;

&lt;p&gt;そこで登場するのが、WSLENVです。Windowsで作業します。Windowsの環境変数GOPATHを、環境変数WSLENVへスイッチとともに設定します。/pスイッチは、「この環境変数はパスを格納しているから、いい感じにして」という指定です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Users\tomakabe\go\src\github.com\ToruMakabe\work&amp;gt; setx WSLENV &amp;quot;$env:WSLENV`:GOPATH/p&amp;quot;

成功: 指定した値は保存されました。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;いい感じって何よ。それは環境に合わせたパス表現の変換です。WSLで見てみましょう。WSLENVを読ませる必要があるため、VS Codeを再起動します。そして、ターミナルで確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/mnt/c/Users/tomakabe/go $ echo $GOPATH
/mnt/c/Users/tomakabe/go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;GOPATHが読めるようになりました。かつ、Windowsのパス表現であるC:\Users\tomakabe\goから、WSLの表現である/mnt/c/Users/tomakabe/goへと変換して渡しています。素晴らしい。これでGOPATHはひとつになり、ソースやバイナリー、パッケージの管理を統一できます。&lt;/p&gt;

&lt;p&gt;ではWSLでサンプルコードを触ってみましょう。ソースのあるディレクトリへ移動します。ソースと先ほどビルドしたexeがあります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/mnt/c/Users/tomakabe/go $ cd src/github.com/ToruMakabe/work/
/mnt/c/Users/tomakabe/go/src/github.com/ToruMakabe/work $ ls
hello.exe  hello.go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WSL上でビルドします。ELFバイナリー hello が作られました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/mnt/c/Users/tomakabe/go/src/github.com/ToruMakabe/work $ go build hello.go
/mnt/c/Users/tomakabe/go/src/github.com/ToruMakabe/work $ ls
hello  hello.exe  hello.go
/mnt/c/Users/tomakabe/go/src/github.com/ToruMakabe/work $ file ./hello
./hello: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, not stripped
/mnt/c/Users/tomakabe/go/src/github.com/ToruMakabe/work $ ./hello
Hello Go on the new WSL
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;

&lt;p&gt;代表例としてGoの開発環境で説明しましたが、WSLENVは他の用途でも応用できるでしょう。スイッチの説明など、詳細は先ほど紹介した、&lt;a href=&#34;https://blogs.msdn.microsoft.com/commandline/2017/12/22/share-environment-vars-between-wsl-and-windows/&#34;&gt;こちら&lt;/a&gt;を。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>TerraformでAzureのシークレットを受け渡す(ACI/AKS編)</title>
      <link>https://ToruMakabe.github.io/post/terraform_azure_secret/</link>
      <pubDate>Fri, 27 Apr 2018 17:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/terraform_azure_secret/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;動機&#34;&gt;動機&lt;/h2&gt;

&lt;p&gt;システム開発、運用の現場では、しばしばシークレットの受け渡しをします。代表例はデータベースの接続文字列です。データベース作成時に生成した接続文字列をアプリ側で設定するのですが、ひとりでコピペするにせよ、チームメンバー間で受け渡すにせよ、めんどくさく、危険が危ないわけです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;いちいちポータルやCLIで接続文字列を出力、コピーして、アプリの設定ファイルや環境変数にペーストしなければいけない

&lt;ul&gt;
&lt;li&gt;めんどくさいし手が滑る&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;データベース管理者がアプリ開発者に接続文字列を何らかの手段で渡さないといけない

&lt;ul&gt;
&lt;li&gt;メールとかチャットとかファイルサーバーとか勘弁&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;もしくはアプリ開発者にデータベースの接続文字列が読める権限を与えなければいけない

&lt;ul&gt;
&lt;li&gt;本番でも、それやる？&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;kubernetes(k8s)のSecretをいちいちkubectlを使って作りたくない

&lt;ul&gt;
&lt;li&gt;Base64符号化とか、うっかり忘れる&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;つらいですね。シークレットなんて意識したくないのが人情。そこで、Terraformを使った解決法を。&lt;/p&gt;

&lt;h2 id=&#34;シナリオ&#34;&gt;シナリオ&lt;/h2&gt;

&lt;p&gt;Azureでコンテナーを使うシナリオを例に紹介します。ACI(Azure Container Instances)とAKS(Azure Container Service - k8s)の2パターンです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Nodeとデータストアを組み合わせた、&lt;a href=&#34;https://github.com/ToruMakabe/ImpressAzureBookNode&#34;&gt;Todoアプリケーション&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;コンテナーイメージは&lt;a href=&#34;https://hub.docker.com/r/torumakabe/nodetodo/&#34;&gt;Docker Hub&lt;/a&gt;にある&lt;/li&gt;
&lt;li&gt;コンテナーでデータストアを運用したくないので、データストアはマネージドサービスを使う&lt;/li&gt;
&lt;li&gt;データストアはCosmos DB(MongoDB API)&lt;/li&gt;
&lt;li&gt;Cosmos DBへのアクセスに必要な属性をTerraformで参照し、接続文字列(MONGO_URL)を作る

&lt;ul&gt;
&lt;li&gt;接続文字列の渡し方はACI/AKSで異なる&lt;/li&gt;
&lt;li&gt;ACI

&lt;ul&gt;
&lt;li&gt;コンテナー作成時に環境変数として接続文字列を渡す&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;AKS

&lt;ul&gt;
&lt;li&gt;k8sのSecretとして接続文字列をストアする&lt;/li&gt;
&lt;li&gt;コンテナー作成時にSecretを参照し、環境変数として渡す&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;検証環境&#34;&gt;検証環境&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Azure Cloud Shell

&lt;ul&gt;
&lt;li&gt;Terraform v0.11.7&lt;/li&gt;
&lt;li&gt;Terraformの認証はCloud Shell組み込み&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Terraform Azure Provider v1.4&lt;/li&gt;
&lt;li&gt;Terraform kubernetes Provider v1.1&lt;/li&gt;
&lt;li&gt;AKS kubernetes 1.9.6&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;aciの場合&#34;&gt;ACIの場合&lt;/h2&gt;

&lt;p&gt;ざっと以下の流れです。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;リソースグループ作成&lt;/li&gt;
&lt;li&gt;Cosmos DBアカウント作成&lt;/li&gt;
&lt;li&gt;ACIコンテナーグループ作成 (Cosmos DB属性から接続文字列を生成)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;var.で参照している変数は、別ファイルに書いています。&lt;/p&gt;

&lt;p&gt;[main.tf]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;resource &amp;quot;azurerm_resource_group&amp;quot; &amp;quot;rg&amp;quot; {
  name     = &amp;quot;${var.resource_group_name}&amp;quot;
  location = &amp;quot;${var.resource_group_location}&amp;quot;
}

resource &amp;quot;random_integer&amp;quot; &amp;quot;ri&amp;quot; {
  min = 10000
  max = 99999
}

resource &amp;quot;azurerm_cosmosdb_account&amp;quot; &amp;quot;db&amp;quot; {
  name                = &amp;quot;your-cosmos-db-${random_integer.ri.result}&amp;quot;
  location            = &amp;quot;${azurerm_resource_group.rg.location}&amp;quot;
  resource_group_name = &amp;quot;${azurerm_resource_group.rg.name}&amp;quot;
  offer_type          = &amp;quot;Standard&amp;quot;
  kind                = &amp;quot;MongoDB&amp;quot;

  enable_automatic_failover = true

  consistency_policy {
    consistency_level       = &amp;quot;BoundedStaleness&amp;quot;
    max_interval_in_seconds = 10
    max_staleness_prefix    = 200
  }

  geo_location {
    location          = &amp;quot;${azurerm_resource_group.rg.location}&amp;quot;
    failover_priority = 0
  }

  geo_location {
    location          = &amp;quot;${var.failover_location}&amp;quot;
    failover_priority = 1
  }
}

resource &amp;quot;azurerm_container_group&amp;quot; &amp;quot;aci-todo&amp;quot; {
  name                = &amp;quot;aci-todo&amp;quot;
  location            = &amp;quot;${azurerm_resource_group.rg.location}&amp;quot;
  resource_group_name = &amp;quot;${azurerm_resource_group.rg.name}&amp;quot;
  ip_address_type     = &amp;quot;public&amp;quot;
  dns_name_label      = &amp;quot;yourtodo&amp;quot;
  os_type             = &amp;quot;linux&amp;quot;

  container {
    name   = &amp;quot;todo&amp;quot;
    image  = &amp;quot;torumakabe/nodetodo&amp;quot;
    cpu    = &amp;quot;1&amp;quot;
    memory = &amp;quot;1.5&amp;quot;
    port   = &amp;quot;8080&amp;quot;

    environment_variables {
      &amp;quot;MONGO_URL&amp;quot; = &amp;quot;mongodb://${azurerm_cosmosdb_account.db.name}:${azurerm_cosmosdb_account.db.primary_master_key}@${azurerm_cosmosdb_account.db.name}.documents.azure.com:10255/?ssl=true&amp;quot;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;containerのenvironment_variablesブロックでCosmos DBの属性を参照し、接続文字列を生成しています。簡単ですね。これで接続文字列コピペ作業から解放されます。&lt;/p&gt;

&lt;h2 id=&#34;aks&#34;&gt;AKS&lt;/h2&gt;

&lt;p&gt;AKSの場合、流れは以下の通りです。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;リソースグループ作成&lt;/li&gt;
&lt;li&gt;Cosmos DBアカウント作成&lt;/li&gt;
&lt;li&gt;AKSクラスター作成&lt;/li&gt;
&lt;li&gt;k8s Secretを作成 (Cosmos DB属性から接続文字列生成)&lt;/li&gt;
&lt;li&gt;k8s Secretをコンテナーの環境変数として参照し、アプリをデプロイ&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;[main.tf]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;resource &amp;quot;azurerm_resource_group&amp;quot; &amp;quot;rg&amp;quot; {
  name     = &amp;quot;${var.resource_group_name}&amp;quot;
  location = &amp;quot;${var.resource_group_location}&amp;quot;
}

resource &amp;quot;random_integer&amp;quot; &amp;quot;ri&amp;quot; {
  min = 10000
  max = 99999
}

resource &amp;quot;azurerm_cosmosdb_account&amp;quot; &amp;quot;db&amp;quot; {
  name                = &amp;quot;your-cosmos-db-${random_integer.ri.result}&amp;quot;
  location            = &amp;quot;${azurerm_resource_group.rg.location}&amp;quot;
  resource_group_name = &amp;quot;${azurerm_resource_group.rg.name}&amp;quot;
  offer_type          = &amp;quot;Standard&amp;quot;
  kind                = &amp;quot;MongoDB&amp;quot;

  enable_automatic_failover = true

  consistency_policy {
    consistency_level       = &amp;quot;BoundedStaleness&amp;quot;
    max_interval_in_seconds = 10
    max_staleness_prefix    = 200
  }

  geo_location {
    location          = &amp;quot;${azurerm_resource_group.rg.location}&amp;quot;
    failover_priority = 0
  }

  geo_location {
    location          = &amp;quot;${var.failover_location}&amp;quot;
    failover_priority = 1
  }
}

resource &amp;quot;azurerm_kubernetes_cluster&amp;quot; &amp;quot;aks&amp;quot; {
  name                = &amp;quot;yourakstf&amp;quot;
  location            = &amp;quot;${azurerm_resource_group.rg.location}&amp;quot;
  resource_group_name = &amp;quot;${azurerm_resource_group.rg.name}&amp;quot;
  dns_prefix          = &amp;quot;yourakstf&amp;quot;
  kubernetes_version  = &amp;quot;1.9.6&amp;quot;

  linux_profile {
    admin_username = &amp;quot;${var.admin_username}&amp;quot;

    ssh_key {
      key_data = &amp;quot;${var.key_data}&amp;quot;
    }
  }

  agent_pool_profile {
    name            = &amp;quot;default&amp;quot;
    count           = 3
    vm_size         = &amp;quot;Standard_B2ms&amp;quot;
    os_type         = &amp;quot;Linux&amp;quot;
    os_disk_size_gb = 30
  }

  service_principal {
    client_id     = &amp;quot;${var.client_id}&amp;quot;
    client_secret = &amp;quot;${var.client_secret}&amp;quot;
  }
}

provider &amp;quot;kubernetes&amp;quot; {
  host = &amp;quot;${azurerm_kubernetes_cluster.aks.kube_config.0.host}&amp;quot;

  client_certificate     = &amp;quot;${base64decode(azurerm_kubernetes_cluster.aks.kube_config.0.client_certificate)}&amp;quot;
  client_key             = &amp;quot;${base64decode(azurerm_kubernetes_cluster.aks.kube_config.0.client_key)}&amp;quot;
  cluster_ca_certificate = &amp;quot;${base64decode(azurerm_kubernetes_cluster.aks.kube_config.0.cluster_ca_certificate)}&amp;quot;
}

resource &amp;quot;kubernetes_secret&amp;quot; &amp;quot;cosmosdb_secret&amp;quot; {
  metadata {
    name = &amp;quot;cosmosdb-secret&amp;quot;
  }

  data {
    MONGO_URL = &amp;quot;mongodb://${azurerm_cosmosdb_account.db.name}:${azurerm_cosmosdb_account.db.primary_master_key}@${azurerm_cosmosdb_account.db.name}.documents.azure.com:10255/?ssl=true&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cosmos DB、AKSクラスターを作ったのち、kubernetesプロバイダーを使ってSecretを登録しています。複数のプロバイダーを組み合わせられる、Terraformの特長が活きています。&lt;/p&gt;

&lt;p&gt;そしてアプリのデプロイ時に、登録したSecretを指定します。ここからはkubernetesワールドなので、kubectlなどを使います。マニフェストは以下のように。&lt;/p&gt;

&lt;p&gt;[todo.yaml]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: todoapp
spec:
  selector:
    matchLabels:
      app: todoapp
  replicas: 2
  template:
    metadata:
      labels:
        app: todoapp
    spec:
      containers:
        - name: todoapp
          image: torumakabe/nodetodo
          ports:
            - containerPort: 8080
          env:
            - name: MONGO_URL
              valueFrom:
                secretKeyRef:
                  name: cosmosdb-secret
                  key: MONGO_URL
---
apiVersion: v1
kind: Service
metadata:
  name: todoapp
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: 8080
  selector:
    app: todoapp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;シークレットの中身を見ることなく、コピペもせず、もちろんメールやチャットやファイルも使わず、アプリからCosmos DBへ接続できました。&lt;/p&gt;

&lt;p&gt;シークレットに限らず、Terraformの属性参照、変数表現は強力ですので、ぜひ活用してみてください。数多くの&lt;a href=&#34;https://www.terraform.io/docs/providers/azurerm/&#34;&gt;Azureリソース&lt;/a&gt;が対応しています。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>俺のAzure CLI 2018春版</title>
      <link>https://ToruMakabe.github.io/post/myazurecli201804/</link>
      <pubDate>Mon, 09 Apr 2018 15:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/myazurecli201804/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;春の環境リフレッシュ祭り&#34;&gt;春の環境リフレッシュ祭り&lt;/h2&gt;

&lt;p&gt;最近KubernetesのCLI、kubectlを使う機会が多いのですが、なかなかイケてるんですよ。かゆい所に手が届く感じ。そこで、いい機会なのでAzure CLIまわりも最新の機能やツールで整えようか、というのが今回の動機。気づかないうちに、界隈が充実していた。&lt;/p&gt;

&lt;h2 id=&#34;俺のおすすめ-3選&#34;&gt;俺のおすすめ 3選&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;デフォルト設定

&lt;ul&gt;
&lt;li&gt;リソースグループやロケーション、出力形式などのデフォルト設定ができる&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;エイリアス

&lt;ul&gt;
&lt;li&gt;サブコマンドにエイリアスを付けられる&lt;/li&gt;
&lt;li&gt;引数付きの込み入った表現もできる&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;VS Code プラグイン

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=ms-vscode.azurecli&#34;&gt;Azure CLI Toolsプラグイン&lt;/a&gt; でazコマンドの編集をコードアシストしてくれる&lt;/li&gt;
&lt;li&gt;編集画面上でコマンド選択して実行できる&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;デフォルト設定&#34;&gt;デフォルト設定&lt;/h2&gt;

&lt;p&gt;$AZURE_CONFIG_DIR/configファイルで構成設定ができます。$AZURE_CONFIG_DIR の既定値は、Linux/macOS の場合$HOME/.azure、Windowsは%USERPROFILE%.azure。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/cli/azure/azure-cli-configuration?view=azure-cli-latest&#34;&gt;Azure CLI 2.0 の構成&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;まず変えたいところは、コマンドの出力形式。デフォルトはJSON。わたしのお気持ちは、普段はTable形式、掘りたい時だけJSON。なのでデフォルトをtableに変えます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[core]
output = table
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;del&gt;そしてデフォルトのリソースグループを設定します。以前は「デフォルト設定すると、気づかないところで事故るから、やらない」という主義だったのですが、Kubernetesのdefault namespaceの扱いを見て「ああ、これもありかなぁ」と改宗したところ。&lt;/del&gt;
軽く事故ったので、リソースグループのデフォルト設定をいまはやめています。デフォルトのご利用は計画的に。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[defaults]
group = default-ejp-rg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;他にもロケーションやストレージアカウントなどを設定できます。ロケーションはリソースグループの属性を継承させたい、もしくは明示したい場合が多いので、設定していません。&lt;/p&gt;

&lt;p&gt;ということで、急ぎUbuntuの仮想マシンが欲しいぜという場合、az vm createコマンドの必須パラメーター、-gと-lを省略できるようになったので、さくっと以下のコマンドでできるようになりました。&lt;/p&gt;

&lt;p&gt;デフォルト指定したリソースグループを、任意のロケーションに作ってある前提です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az vm create -n yoursmplvm01 --image UbuntuLTS
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;エイリアス&#34;&gt;エイリアス&lt;/h2&gt;

&lt;p&gt;$AZURE_CONFIG_DIR/aliasにエイリアスを書けます。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/cli/azure/azure-cli-extension-alias?view=azure-cli-latest&#34;&gt;Azure CLI 2.0 のエイリアス拡張機能&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;前提はAzure CLI v2.0.28以降です。以下のコマンドでエイリアス拡張を導入できます。現時点ではプレビュー扱いなのでご注意を。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az extension add --name alias
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ひとまずわたしは以下3カテゴリのエイリアスを登録しました。&lt;/p&gt;

&lt;h3 id=&#34;頻繁に打つからできる限り短くしたい系&#34;&gt;頻繁に打つからできる限り短くしたい系&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;[ls]
command = list

[nw]
command = network

[pip]
command = public-ip

[fa]
command = functionapp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例えばデフォルトリソースグループでパブリックIP公開してるか確認したいな、と思った時は、az network public-ip listじゃなくて、こう打てます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az nw pip ls
Name                  ResourceGroup    Location    Zones    AddressVersion    AllocationMethod      IdleTimeoutInMinutes
ProvisioningState
--------------------  ---------------  ----------  -------  ----------------  ------------------  ----------------------
-------------------
yoursmplvm01PublicIP  default-ejp-rg   japaneast            IPv4              Dynamic                                  4
Succeeded
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;クエリー打つのがめんどくさい系&#34;&gt;クエリー打つのがめんどくさい系&lt;/h3&gt;

&lt;p&gt;VMに紐づいてるパブリックIPを確認したいときは、こんなエイリアス。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[get-vm-pip]
command = vm list-ip-addresses --query [].virtualMachine.network.publicIpAddresses[].ipAddress
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;実行すると。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az get-vm-pip -n yoursmplvm01
Result
-------------
52.185.133.68
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;引数を確認するのがめんどくさい系&#34;&gt;引数を確認するのがめんどくさい系&lt;/h3&gt;

&lt;p&gt;リソースグループを消したくないけど、中身だけ消したいってケース、よくありますよね。そんなエイリアスも作りました。&amp;ndash;template-uriで指定しているGistには、空っぽのAzure Resource Manager デプロイメントテンプレートが置いてあります。このuriをいちいち確認するのがめんどくさいので、エイリアスに。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[empty-rg]
command = group deployment create --mode Complete --template-uri https://gist.githubusercontent.com/ToruMakabe/28ad5177a6de525866027961aa33b1e7/raw/9b455bfc9608c637e1980d9286b7f77e76a5c74b/azuredeploy_empty.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドを打つだけで、リソースグループの中身をバッサリ消せます。投げっぱなしでさっさとPC閉じて帰りたいときは &amp;ndash;no-waitオプションを。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az empty-rg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/cli/azure/azure-cli-extension-alias?view=azure-cli-latest#create-an-alias-command-with-arguments&#34;&gt;位置引数&lt;/a&gt;や&lt;a href=&#34;https://docs.microsoft.com/ja-jp/cli/azure/azure-cli-extension-alias?view=azure-cli-latest#process-arguments-using-jinja2-templates&#34;&gt;Jinja2テンプレート&lt;/a&gt;を使ったエイリアスも作れるので、込み入ったブツを、という人は挑戦してみてください。&lt;/p&gt;

&lt;h2 id=&#34;vs-code-プラグイン-azure-cli-tools&#34;&gt;VS Code プラグイン (Azure CLI Tools )&lt;/h2&gt;

&lt;p&gt;Azure CLIのVS Code向けプラグインがあります。コードアシストと編集画面からの実行が2大機能。紹介ページのGifアニメを見るのが分かりやすいです。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=ms-vscode.azurecli&#34;&gt;Azure CLI Tools&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;プラグインを入れて、拡張子.azcliでファイルを作ればプラグインが効きます。長いコマンドを補完支援付きでコーディングしたい、スクリプトを各行実行して確認しながら作りたい、なんて場合におすすめです。&lt;/p&gt;

&lt;h2 id=&#34;注意点&#34;&gt;注意点&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;エイリアスには補完が効かない

&lt;ul&gt;
&lt;li&gt;bashでのCLI実行、VS Code Azure CLI Toolsともに、現時点(&lt;sup&gt;2018&lt;/sup&gt;&amp;frasl;&lt;sub&gt;4&lt;/sub&gt;)でエイリアスには補完が効きません&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ソースコード管理に不要なファイルを含めない

&lt;ul&gt;
&lt;li&gt;$AZURE_CONFIG_DIR/ 下には、aliasやconfigの他に、認証トークンやプロファイルといったシークレット情報が置かれます。なのでGitなどでソースコード管理する場合は、aliasとconfig以外は除外したほうがいいでしょう&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
      
    </item>
    
    <item>
      <title>TerraformでAzure VM/VMSSの最新のカスタムイメージを指定する方法</title>
      <link>https://ToruMakabe.github.io/post/azure_terraform_customimage_desc/</link>
      <pubDate>Fri, 06 Apr 2018 18:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_terraform_customimage_desc/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;カスタムイメージではlatest指定できない&#34;&gt;カスタムイメージではlatest指定できない&lt;/h2&gt;

&lt;p&gt;Azure Marketplaceで提供されているVM/VMSSのイメージは、latest指定により最新のイメージを取得できます。いっぽうでカスタムイメージの場合、同様の属性を管理していないので、できません。&lt;/p&gt;

&lt;p&gt;ではVM/VMSSを作成するとき、どうやって最新のカスタムイメージ名を指定すればいいでしょうか。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;最新のイメージ名を確認のうえ、手で指定する&lt;/li&gt;
&lt;li&gt;自動化パイプラインで、イメージ作成とVM/VMSS作成ステップでイメージ名を共有する&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;2のケースは、JenkinsでPackerとTerraformを同じジョブで流すケースがわかりやすい。変数BUILD_NUMBERを共有すればいいですね。でもイメージに変更がなく、Terraformだけ流したい時、パイプラインを頭から流してイメージ作成をやり直すのは、無駄なわけです。&lt;/p&gt;

&lt;h2 id=&#34;terraformではイメージ名取得に正規表現とソートが可能&#34;&gt;Terraformではイメージ名取得に正規表現とソートが可能&lt;/h2&gt;

&lt;p&gt;Terraformでは見出しの通り、捗る表現ができます。&lt;/p&gt;

&lt;p&gt;イメージを取得するとき、name_regexでイメージ名を引っ張り、sort_descendingを指定すればOK。以下の例は、イメージ名をubuntu1604-xxxxというルールで作ると決めた場合の例です。イメージを作るたびに末尾をインクリメントしてください。ソートはイメージ名全体の&lt;a href=&#34;https://github.com/terraform-providers/terraform-provider-azurerm/blob/master/azurerm/data_source_image.go#L164&#34;&gt;文字列比較&lt;/a&gt;なので、末尾の番号の決めた桁は埋めること。&lt;/p&gt;

&lt;p&gt;ということで降順で最上位、つまり最新のイメージ名を取得できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data &amp;quot;azurerm_image&amp;quot; &amp;quot;poc&amp;quot; {
  name_regex          = &amp;quot;ubuntu1604-[0-9]*&amp;quot;
  sort_descending     = true
  resource_group_name = &amp;quot;${var.managed_image_resource_group_name}&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;あとはVM/VMSSリソース定義内で、取得したイメージのidを渡します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  storage_profile_image_reference {
    id = &amp;quot;${data.azurerm_image.poc.id}&amp;quot;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;便利である。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azure MarketplaceからMSI対応でセキュアなTerraform環境を整える</title>
      <link>https://ToruMakabe.github.io/post/azure_msi_terraform/</link>
      <pubDate>Fri, 30 Mar 2018 16:30:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_msi_terraform/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;terraformのプロビジョニングがmarketplaceから可能に&#34;&gt;TerraformのプロビジョニングがMarketplaceから可能に&lt;/h2&gt;

&lt;p&gt;Terraform使ってますか。Azureのリソースプロビジョニングの基本はAzure Resource Manager Template Deployである、がわたしの持論ですが、Terraformを使う/併用する方がいいな、というケースは結構あります。使い分けは&lt;a href=&#34;https://www.slideshare.net/ToruMakabe/azure-infrastructure-as-code&#34;&gt;この資料&lt;/a&gt;も参考に。&lt;/p&gt;

&lt;p&gt;さて、先日Azure Marketplaceから&lt;a href=&#34;https://azuremarketplace.microsoft.com/en-us/marketplace/apps/azure-oss.terraform&#34;&gt;Terraform入りの仮想マシン&lt;/a&gt;をプロビジョニングできるようになりました。Ubuntuに以下のアプリが導入、構成されます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Terraform (latest)&lt;/li&gt;
&lt;li&gt;Azure CLI 2.0&lt;/li&gt;
&lt;li&gt;Managed Service Identity (MSI) VM Extension&lt;/li&gt;
&lt;li&gt;Unzip&lt;/li&gt;
&lt;li&gt;JQ&lt;/li&gt;
&lt;li&gt;apt-transport-https&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;いろいろセットアップしてくれるのでしみじみ便利なのですが、ポイントはManaged Service Identity (MSI)です。&lt;/p&gt;

&lt;h2 id=&#34;シークレットをコードにベタ書きする問題&#34;&gt;シークレットをコードにベタ書きする問題&lt;/h2&gt;

&lt;p&gt;MSIの何がうれしいいのでしょう。分かりやすい例を挙げると「GitHubにシークレットを書いたコードをpushする、お漏らし事案」を避ける仕組みです。もちそんそれだけではありませんが。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/active-directory/managed-service-identity/overview&#34;&gt;Azure リソースの管理対象サービス ID (MSI)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;詳細の説明は公式ドキュメントに譲りますが、ざっくり説明すると&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;アプリに認証・認可用のシークレットを書かなくても、アプリの動く仮想マシン上にあるローカルエンドポイントにアクセスすると、Azureのサービスを使うためのトークンが得られるよ&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;です。&lt;/p&gt;

&lt;p&gt;GitHub上に疑わしいシークレットがないかスキャンする&lt;a href=&#34;https://azure.microsoft.com/ja-jp/blog/managing-azure-secrets-on-github-repositories/&#34;&gt;取り組み&lt;/a&gt;もはじまっているのですが、できればお世話になりなくない。MSIを活用しましょう。&lt;/p&gt;

&lt;h2 id=&#34;terraformはmsiに対応している&#34;&gt;TerraformはMSIに対応している&lt;/h2&gt;

&lt;p&gt;TerraformでAzureのリソースをプロビジョニングするには、もちろん認証・認可が必要です。従来はサービスプリンシパルを作成し、そのIDやシークレットをTerraformの実行環境に配布していました。でも、できれば配布したくないですよね。実行環境を特定の仮想マシンに限定し、MSIを使えば、解決できます。&lt;/p&gt;

&lt;p&gt;ところでMSIを使うには、ローカルエンドポイントにトークンを取りに行くよう、アプリを作らなければいけません。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.terraform.io/docs/providers/azurerm/authenticating_via_msi.html&#34;&gt;Authenticating to Azure Resource Manager using Managed Service Identity&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Terraformは対応済みです。環境変数 ARM_USE_MSI をtrueにしてTerraformを実行すればOK。&lt;/p&gt;

&lt;h2 id=&#34;試してみよう&#34;&gt;試してみよう&lt;/h2&gt;

&lt;p&gt;実は、すでに使い方を解説した公式ドキュメントがあります。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/terraform/terraform-vm-msi&#34;&gt;Azure Marketplace イメージを使用して管理対象サービス ID を使用する Terraform Linux 仮想マシンを作成する&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;手順は十分なのですが、理解を深めるための補足情報が、もうちょっと欲しいところです。なので補ってみましょう。&lt;/p&gt;

&lt;h3 id=&#34;marketplaceからterraform入り仮想マシンを作る&#34;&gt;MarketplaceからTerraform入り仮想マシンを作る&lt;/h3&gt;

&lt;p&gt;まず、Marketplaceからのデプロイでどんな仮想マシンが作られたのか、気になります。デプロイに利用されたテンプレートをのぞいてみましょう。注目は以下3つのリソースです。抜き出します。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;MSI VM拡張の導入&lt;/li&gt;
&lt;li&gt;VMに対してリソースグループスコープでContributorロールを割り当て&lt;/li&gt;
&lt;li&gt;スクリプト実行 VM拡張でTerraform関連のプロビジョニング&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;[snip]
        {
            &amp;quot;type&amp;quot;: &amp;quot;Microsoft.Compute/virtualMachines/extensions&amp;quot;,
            &amp;quot;name&amp;quot;: &amp;quot;[concat(parameters(&#39;vmName&#39;),&#39;/MSILinuxExtension&#39;)]&amp;quot;,
            &amp;quot;apiVersion&amp;quot;: &amp;quot;2017-12-01&amp;quot;,
            &amp;quot;location&amp;quot;: &amp;quot;[parameters(&#39;location&#39;)]&amp;quot;,
            &amp;quot;properties&amp;quot;: {
                &amp;quot;publisher&amp;quot;: &amp;quot;Microsoft.ManagedIdentity&amp;quot;,
                &amp;quot;type&amp;quot;: &amp;quot;ManagedIdentityExtensionForLinux&amp;quot;,
                &amp;quot;typeHandlerVersion&amp;quot;: &amp;quot;1.0&amp;quot;,
                &amp;quot;autoUpgradeMinorVersion&amp;quot;: true,
                &amp;quot;settings&amp;quot;: {
                    &amp;quot;port&amp;quot;: 50342
                },
                &amp;quot;protectedSettings&amp;quot;: {}
            },
            &amp;quot;dependsOn&amp;quot;: [
                &amp;quot;[concat(&#39;Microsoft.Compute/virtualMachines/&#39;, parameters(&#39;vmName&#39;))]&amp;quot;
            ]
        },
        {
            &amp;quot;type&amp;quot;: &amp;quot;Microsoft.Authorization/roleAssignments&amp;quot;,
            &amp;quot;name&amp;quot;: &amp;quot;[variables(&#39;resourceGuid&#39;)]&amp;quot;,
            &amp;quot;apiVersion&amp;quot;: &amp;quot;2017-09-01&amp;quot;,
            &amp;quot;properties&amp;quot;: {
                &amp;quot;roleDefinitionId&amp;quot;: &amp;quot;[variables(&#39;contributor&#39;)]&amp;quot;,
                &amp;quot;principalId&amp;quot;: &amp;quot;[reference(concat(resourceId(&#39;Microsoft.Compute/virtualMachines/&#39;, parameters(&#39;vmName&#39;)),&#39;/providers/Microsoft.ManagedIdentity/Identities/default&#39;),&#39;2015-08-31-PREVIEW&#39;).principalId]&amp;quot;,
                &amp;quot;scope&amp;quot;: &amp;quot;[concat(&#39;/subscriptions/&#39;, subscription().subscriptionId, &#39;/resourceGroups/&#39;, resourceGroup().name)]&amp;quot;
            },
            &amp;quot;dependsOn&amp;quot;: [
                &amp;quot;[resourceId(&#39;Microsoft.Compute/virtualMachines/extensions/&#39;, parameters(&#39;vmName&#39;),&#39;MSILinuxExtension&#39;)]&amp;quot;
            ]
        },
        {
            &amp;quot;type&amp;quot;: &amp;quot;Microsoft.Compute/virtualMachines/extensions&amp;quot;,
            &amp;quot;name&amp;quot;: &amp;quot;[concat(parameters(&#39;vmName&#39;),&#39;/customscriptextension&#39;)]&amp;quot;,
            &amp;quot;apiVersion&amp;quot;: &amp;quot;2017-03-30&amp;quot;,
            &amp;quot;location&amp;quot;: &amp;quot;[parameters(&#39;location&#39;)]&amp;quot;,
            &amp;quot;properties&amp;quot;: {
                &amp;quot;publisher&amp;quot;: &amp;quot;Microsoft.Azure.Extensions&amp;quot;,
                &amp;quot;type&amp;quot;: &amp;quot;CustomScript&amp;quot;,
                &amp;quot;typeHandlerVersion&amp;quot;: &amp;quot;2.0&amp;quot;,
                &amp;quot;autoUpgradeMinorVersion&amp;quot;: true,
                &amp;quot;settings&amp;quot;: {
                    &amp;quot;fileUris&amp;quot;: [
                        &amp;quot;[concat(parameters(&#39;artifactsLocation&#39;), &#39;/scripts/infra.sh&#39;, parameters(&#39;artifactsLocationSasToken&#39;))]&amp;quot;,
                        &amp;quot;[concat(parameters(&#39;artifactsLocation&#39;), &#39;/scripts/install.sh&#39;, parameters(&#39;artifactsLocationSasToken&#39;))]&amp;quot;,
                        &amp;quot;[concat(parameters(&#39;artifactsLocation&#39;), &#39;/scripts/azureProviderAndCreds.tf&#39;, parameters(&#39;artifactsLocationSasToken&#39;))]&amp;quot;
                    ]
                },
                &amp;quot;protectedSettings&amp;quot;: {
                    &amp;quot;commandToExecute&amp;quot;: &amp;quot;[concat(&#39;bash infra.sh &amp;amp;&amp;amp; bash install.sh &#39;, variables(&#39;installParm1&#39;), variables(&#39;installParm2&#39;), variables(&#39;installParm3&#39;), variables(&#39;installParm4&#39;), &#39; -k &#39;, listKeys(resourceId(&#39;Microsoft.Storage/storageAccounts&#39;, variables(&#39;stateStorageAccountName&#39;)), &#39;2017-10-01&#39;).keys[0].value, &#39; -l &#39;, reference(concat(resourceId(&#39;Microsoft.Compute/virtualMachines/&#39;, parameters(&#39;vmName&#39;)),&#39;/providers/Microsoft.ManagedIdentity/Identities/default&#39;),&#39;2015-08-31-PREVIEW&#39;).principalId)]&amp;quot;
                }
            },
            &amp;quot;dependsOn&amp;quot;: [
                &amp;quot;[resourceId(&#39;Microsoft.Authorization/roleAssignments&#39;, variables(&#39;resourceGuid&#39;))]&amp;quot;
            ]
        }
[snip]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;vmにログインし-環境を確認&#34;&gt;VMにログインし、環境を確認&lt;/h3&gt;

&lt;p&gt;では出来上がったVMにsshし、いろいろのぞいてみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh your-vm-public-ip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Terraformのバージョンは、現時点で最新の0.11.5が入っています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ terraform -v
Terraform v0.11.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;環境変数ARM_USE_MSIはtrueに設定されています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ echo $ARM_USE_MSI
true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;MSIも有効化されています(SystemAssigned)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az vm identity show -g tf-msi-poc-ejp-rg -n tfmsipocvm01
{
  &amp;quot;additionalProperties&amp;quot;: {},
  &amp;quot;identityIds&amp;quot;: null,
  &amp;quot;principalId&amp;quot;: &amp;quot;aaaa-aaaa-aaaa-aaaa-aaaa&amp;quot;,
  &amp;quot;tenantId&amp;quot;: &amp;quot;tttt-tttt-tttt-tttt&amp;quot;,
  &amp;quot;type&amp;quot;: &amp;quot;SystemAssigned&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;さて、このVMはMSIが使えるようになったわけですが、操作できるリソースのスコープは、このVMが属するリソースグループに限定されてます。新たなリソースグループを作成したい場合は、ロールを付与し、スコープを広げます。~/にtfEnv.shというスクリプトが用意されています。用意されたスクリプトを実行すると、サブスクリプションスコープのContributorがVMに割り当てられます。必要に応じて変更しましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls
tfEnv.sh  tfTemplate

$ cat tfEnv.sh
az login
az role assignment create  --assignee &amp;quot;aaaa-aaaa-aaaa-aaaa-aaaa&amp;quot; --role &#39;b24988ac-6180-42a0-ab88-20f7382dd24c&#39;  --scope /subscriptions/&amp;quot;cccc-cccc-cccc-cccc&amp;quot;

$ . ~/tfEnv.sh
To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code HOGEHOGE to authenticate.
[snip]
{
  &amp;quot;additionalProperties&amp;quot;: {},
  &amp;quot;canDelegate&amp;quot;: null,
  &amp;quot;id&amp;quot;: &amp;quot;/subscriptions/cccc-cccc-cccc-cccc/providers/Microsoft.Authorization/roleAssignments/ffff-ffff-ffff-ffff&amp;quot;,
  &amp;quot;name&amp;quot;: &amp;quot;ffff-ffff-ffff-ffff&amp;quot;,
  &amp;quot;principalId&amp;quot;: &amp;quot;aaaa-aaaa-aaaa-aaaa-aaaa&amp;quot;,
  &amp;quot;roleDefinitionId&amp;quot;: &amp;quot;/subscriptions/cccc-cccc-cccc-cccc/providers/Microsoft.Authorization/roleDefinitions/b24988ac-6180-42a0-ab88-20f7382dd24c&amp;quot;,
  &amp;quot;scope&amp;quot;: &amp;quot;/subscriptions/cccc-cccc-cccc-cccc&amp;quot;,
  &amp;quot;type&amp;quot;: &amp;quot;Microsoft.Authorization/roleAssignments&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ちなみに、role id &amp;ldquo;b24988ac-6180-42a0-ab88-20f7382dd24c&amp;rdquo;はContributorを指します。&lt;/p&gt;

&lt;p&gt;tfTemplateというディレクトリも用意されているようです。2つのファイルがあります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls tfTemplate/
azureProviderAndCreds.tf  remoteState.tf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;azureProviderAndCreds.tfは、tfファイルのテンプレートです。コメントアウトと説明のとおり、MSIを使う場合には、このテンプレートは必要ありません。subscription_idとtenant_idは、VMのプロビジョニング時に環境変数にセットされています。そしてclient_idとclient_secretは、MSIを通じて取得されます。明示的に変えたい時のみ指定しましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat tfTemplate/azureProviderAndCreds.tf
#
#
# Provider and credential snippet to add to configurations
# Assumes that there&#39;s a terraform.tfvars file with the var values
#
# Uncomment the creds variables if using service principal auth
# Leave them commented to use MSI auth
#
#variable subscription_id {}
#variable tenant_id {}
#variable client_id {}
#variable client_secret {}

provider &amp;quot;azurerm&amp;quot; {
#    subscription_id = &amp;quot;${var.subscription_id}&amp;quot;
#    tenant_id = &amp;quot;${var.tenant_id}&amp;quot;
#    client_id = &amp;quot;${var.client_id}&amp;quot;
#    client_secret = &amp;quot;${var.client_secret}&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;remoteState.tfは、TerraformのstateをAzureのBlob上に置く場合に使います。Blobの&lt;a href=&#34;https://azure.microsoft.com/en-us/blog/soft-delete-for-azure-storage-blobs-now-in-public-preview/&#34;&gt;soft delete&lt;/a&gt;が使えるようになったこともあり、事件や事故を考慮すると、できればstateはローカルではなくBlobで管理したいところです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat tfTemplate/remoteState.tf
terraform {
 backend &amp;quot;azurerm&amp;quot; {
  storage_account_name = &amp;quot;storestaterandomid&amp;quot;
  container_name       = &amp;quot;terraform-state&amp;quot;
  key                  = &amp;quot;prod.terraform.tfstate&amp;quot;
  access_key           = &amp;quot;KYkCz88z+7yoyoyoiyoyoyoiyoyoyoiyoiTDZRbrwAWIPWD+rU6g==&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;soft delete設定は、別途 &lt;a href=&#34;https://docs.microsoft.com/en-us/cli/azure/storage/blob/service-properties/delete-policy?view=azure-cli-latest#az-storage-blob-service-properties-delete-policy-update&#34;&gt;az storage blob service-properties delete-policy update&lt;/a&gt; コマンドで行ってください。&lt;/p&gt;

&lt;h3 id=&#34;プロビジョニングしてみる&#34;&gt;プロビジョニングしてみる&lt;/h3&gt;

&lt;p&gt;ではTerraformを動かしてみましょう。サブディレクトリsampleを作り、そこで作業します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir sample
$ cd sample/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;stateはBlobで管理しましょう。先ほどのremoteState.tfを実行ディレクトリにコピーします。アクセスキーが入っていますので、このディレクトリをコード管理システム配下に置くのであれば、.gitignoreなどで除外をお忘れなく。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cp ../tfTemplate/remoteState.tf ./
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここのキーが残ってしまうのが現時点での課題。ストレージのキー問題は&lt;a href=&#34;https://feedback.azure.com/forums/217298-storage/suggestions/14831712-allow-user-based-access-to-blob-containers-for-su&#34;&gt;対応がはじまったので&lt;/a&gt;、いずれ解決するはずです。&lt;/p&gt;

&lt;p&gt;ではTerraformで作るリソースを書きます。さくっとACI上にnginxコンテナーを作りましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vim main.tf
resource &amp;quot;azurerm_resource_group&amp;quot; &amp;quot;tf-msi-poc&amp;quot; {
    name     = &amp;quot;tf-msi-poc-aci-wus-rg&amp;quot;
    location = &amp;quot;West US&amp;quot;
}

resource &amp;quot;random_integer&amp;quot; &amp;quot;random_int&amp;quot; {
    min = 100
    max = 999
}

resource &amp;quot;azurerm_container_group&amp;quot; &amp;quot;aci-example&amp;quot; {
    name                = &amp;quot;aci-cg-${random_integer.random_int.result}&amp;quot;
    location            = &amp;quot;${azurerm_resource_group.tf-msi-poc.location}&amp;quot;
    resource_group_name = &amp;quot;${azurerm_resource_group.tf-msi-poc.name}&amp;quot;
    ip_address_type     = &amp;quot;public&amp;quot;
    dns_name_label      = &amp;quot;tomakabe-aci-cg-${random_integer.random_int.result}&amp;quot;
    os_type             = &amp;quot;linux&amp;quot;

    container {
        name    = &amp;quot;nginx&amp;quot;
        image   = &amp;quot;nginx&amp;quot;
        cpu     = &amp;quot;0.5&amp;quot;
        memory  = &amp;quot;1.0&amp;quot;
        port    = &amp;quot;80&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;init、plan、アプラーイ。アプライ王子。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ terraform init
$ terraform plan
$ terraform apply -auto-approve
[snip]
Apply complete! Resources: 3 added, 0 changed, 0 destroyed.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;できたか確認。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az container show -g tf-msi-poc-aci-wus-rg -n aci-cg-736 -o table
Name        ResourceGroup          ProvisioningState    Image    IP:ports         CPU/Memory       OsType    Location
----------  ---------------------  -------------------  -------  ---------------  ---------------  --------  ----------
aci-cg-736  tf-msi-poc-aci-wus-rg  Succeeded            nginx    13.91.90.117:80  0.5 core/1.0 gb  Linux     westus
$ curl 13.91.90.117
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
[snip]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;おまけ&#34;&gt;おまけ&lt;/h2&gt;

&lt;p&gt;サービスプリンシパルは、アプリに対して権限を付与するために必要な仕組みなのですが、使わなくなった際に消し忘れることが多いです。意識して消さないと、散らかり放題。&lt;/p&gt;

&lt;p&gt;MSIの場合、対象のVMを消すとそのプリンシパルも消えます。爽快感ほとばしる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az ad sp show --id aaaa-aaaa-aaaa-aaaa-aaaa
Resource &#39;aaaa-aaaa-aaaa-aaaa-aaaa&#39; does not exist or one of its queried reference-property objects are not present.
&lt;/code&gt;&lt;/pre&gt;
</content>
      
    </item>
    
    <item>
      <title>Azure DNS Private Zonesの動きを確認する</title>
      <link>https://ToruMakabe.github.io/post/azure_private_dns_preview/</link>
      <pubDate>Tue, 27 Mar 2018 00:10:30 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_private_dns_preview/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;プライベートゾーンのパブリックプレビュー開始&#34;&gt;プライベートゾーンのパブリックプレビュー開始&lt;/h2&gt;

&lt;p&gt;Azure DNSのプライベートゾーン対応が、全リージョンでパブリックプレビューとなりました。ゾーンとプレビューのプライベートとパブリックが入り混じって、なにやら紛らわしいですが。&lt;/p&gt;

&lt;p&gt;さて、このプライベートゾーン対応ですが、名前のとおりAzure DNSをプライベートな仮想ネットワーク(VNet)で使えるようになります。加えて、しみじみと嬉しい便利機能がついています。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Split-Horizonに対応します。VNet内からの問い合わせにはプライベートゾーン、それ以外からはパブリックゾーンのレコードを返します。&lt;/li&gt;
&lt;li&gt;仮想マシンの作成時、プライベートゾーンへ自動でホスト名を追加します。&lt;/li&gt;
&lt;li&gt;プライベートゾーンとVNetをリンクして利用します。複数のVNetをリンクすることが可能です。&lt;/li&gt;
&lt;li&gt;リンクの種類として、仮想マシンホスト名の自動登録が行われるVNetをRegistration VNet、名前解決(正引き)のみ可能なResolution VNetがあります。&lt;/li&gt;
&lt;li&gt;プライベートゾーンあたり、Registration VNetの現時点の上限数は1、Resolution VNetは10です。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;公式ドキュメントは&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/dns/private-dns-overview&#34;&gt;こちら&lt;/a&gt;。現時点の&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/dns/private-dns-overview#limitations&#34;&gt;制約もまとまっている&lt;/a&gt;ので、目を通しておきましょう。&lt;/p&gt;

&lt;h2 id=&#34;動きを見てみよう&#34;&gt;動きを見てみよう&lt;/h2&gt;

&lt;p&gt;公式ドキュメントには&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/dns/private-dns-scenarios&#34;&gt;想定シナリオ&lt;/a&gt;があり、これを読めばできることがだいたい分かります。ですが、名前解決は呼吸のようなもの、体に叩き込みたいお気持ちです。手を動かして確認します。&lt;/p&gt;

&lt;h3 id=&#34;事前に準備する環境&#34;&gt;事前に準備する環境&lt;/h3&gt;

&lt;p&gt;下記リソースを先に作っておきます。手順は割愛。ドメイン名はexample.comとしましたが、適宜読み替えてください。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;VNet *2

&lt;ul&gt;
&lt;li&gt;vnet01&lt;/li&gt;
&lt;li&gt;subnet01

&lt;ul&gt;
&lt;li&gt;subnet01-nsg (allow ssh)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;vnet02&lt;/li&gt;
&lt;li&gt;subnet01

&lt;ul&gt;
&lt;li&gt;subnet01-nsg (allow ssh)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Azure DNS Public Zone

&lt;ul&gt;
&lt;li&gt;example.com&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;azure-cliへdns拡張を導入&#34;&gt;Azure CLIへDNS拡張を導入&lt;/h3&gt;

&lt;p&gt;プレビュー機能をCLIに導入します。いずれ要らなくなるかもしれませんので、要否は&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/dns/private-dns-getstarted-cli#to-installuse-azure-dns-private-zones-feature-public-preview&#34;&gt;公式ドキュメント&lt;/a&gt;で確認してください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az extension add --name dns
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;プライベートゾーンの作成&#34;&gt;プライベートゾーンの作成&lt;/h3&gt;

&lt;p&gt;既存のゾーンを確認します。パブリックゾーンがあります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az network dns zone list -o table
ZoneName      ResourceGroup             RecordSets    MaxRecordSets
------------  ----------------------  ------------  ---------------
example.com   common-global-rg                   2             5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;プライベートゾーンを作成します。Registration VNetとしてvnet01をリンクします。&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/dns/private-dns-overview#limitations&#34;&gt;現時点の制約&lt;/a&gt;で、リンク時にはVNet上にVMが無い状態にする必要があります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az network dns zone create -g private-dns-poc-ejp-rg -n example.com --zone-type Private --registration-vnets vnet01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同じ名前のゾーンが2つになりました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az network dns zone list -o table
ZoneName      ResourceGroup             RecordSets    MaxRecordSets
------------  ----------------------  ------------  ---------------
example.com   common-global-rg                   2             5000
example.com   private-dns-poc-ejp-rg             1             5000
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;registration-vnetへvmを作成&#34;&gt;Registration VNetへVMを作成&lt;/h3&gt;

&lt;p&gt;VMを2つ作ります。1つにはインターネット経由でsshするので、パブリックIPを割り当てます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ BASE_NAME=&amp;quot;private-dns-poc-ejp&amp;quot;
$ az network public-ip create -n vm01-pip -g ${BASE_NAME}-rg
$ az network nic create -g ${BASE_NAME}-rg -n vm01-nic --public-ip-address vm01-pip --vnet vnet01 --subnet subnet01
$ az vm create -g ${BASE_NAME}-rg -n vm01 --image Canonical:UbuntuServer:16.04.0-LTS:latest --size Standard_B1s --nics vm01-nic
$ az network nic create -g ${BASE_NAME}-rg -n vm02-nic --vnet vnet01 --subnet subnet01
$ az vm create -g ${BASE_NAME}-rg -n vm02 --image Canonical:UbuntuServer:16.04.0-LTS:latest --size Standard_B1s --nics vm02-nic
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;パブリックipをパブリックゾーンへ登録&#34;&gt;パブリックIPをパブリックゾーンへ登録&lt;/h3&gt;

&lt;p&gt;Split-Horizonの動きを確認したいので、パブリックIPをパブリックゾーンへ登録します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az network public-ip show -g private-dns-poc-ejp-rg -n vm01-pip --query ipAddress
&amp;quot;13.78.84.84&amp;quot;
$ az network dns record-set a add-record -g common-global-rg -z example.com -n vm01 -a 13.78.84.84
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;パブリックゾーンで名前解決できることを確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nslookup vm01.example.com
Server:         103.5.140.1
Address:        103.5.140.1#53

Non-authoritative answer:
Name:   vm01.example.com
Address: 13.78.84.84
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;registration-vnetの動きを確認&#34;&gt;Registration VNetの動きを確認&lt;/h3&gt;

&lt;p&gt;vnet01のvm01へ、パブリックIP経由でsshします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh vm01.example.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同じRegistration VNet上のvm02を正引きします。ドメイン名無し、ホスト名だけでnslookupすると、VNetの内部ドメイン名がSuffixになります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vm01:~$ nslookup vm02
Server:         168.63.129.16
Address:        168.63.129.16#53

Non-authoritative answer:
Name:   vm02.aioh0amlfdze5drhlpb1ktqwxd.lx.internal.cloudapp.net
Address: 10.0.0.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ドメイン名をつけてみましょう。Nameはvnet01にリンクしたプライベートゾーンのドメイン名になりました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vm01:~$ nslookup vm02.example.com
Server:         168.63.129.16
Address:        168.63.129.16#53

Non-authoritative answer:
Name:   vm02.example.com
Address: 10.0.0.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;逆引きもできます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vm01:~$ nslookup 10.0.0.5
Server:         168.63.129.16
Address:        168.63.129.16#53

Non-authoritative answer:
5.0.0.10.in-addr.arpa   name = vm02.example.com.

Authoritative answers can be found from:
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;split-horizonの動きを確認&#34;&gt;Split-Horizonの動きを確認&lt;/h3&gt;

&lt;p&gt;さて、いま作業をしているvm01には、インターネット経由でパブリックゾーンで名前解決してsshしたわけですが、プライベートなVNet内でnslookupするとどうなるでしょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vm01:~$ nslookup vm01.example.com
Server:         168.63.129.16
Address:        168.63.129.16#53

Non-authoritative answer:
Name:   vm01.example.com
Address: 10.0.0.4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;プライベートゾーンで解決されました。Split-Horizonが機能していることが分かります。&lt;/p&gt;

&lt;p&gt;あ、どうでもいいことですが、Split-Horizonって戦隊モノの必殺技みたいなネーミングですね。叫びながら地面に拳を叩きつけたい感じ。&lt;/p&gt;

&lt;h3 id=&#34;resolution-vnetの動きを確認&#34;&gt;Resolution VNetの動きを確認&lt;/h3&gt;

&lt;p&gt;vnet02を作成し、Resolution VNetとしてプライベートゾーンとリンクします。そして、vnet02にvm03を作ります。vm03へのsshまで一気に進めます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ BASE_NAME=&amp;quot;private-dns-poc-ejp&amp;quot;
$ az network vnet create -g ${BASE_NAME}-rg -n vnet02 --address-prefix 10.1.0.0/16 --subnet-name subnet01
$ az network vnet subnet update -g ${BASE_NAME}-rg -n subnet01 --vnet-name vnet02 --network-security-group subnet01-nsg
$ az network public-ip create -n vm03-pip -g ${BASE_NAME}-rg
$ az network dns zone update -g private-dns-poc-ejp-rg -n example.com --resolution-vnets vnet02
$ az network nic create -g ${BASE_NAME}-rg -n vm03-nic --public-ip-address vm03-pip --vnet vnet02 --subnet subnet01
$ az vm create -g ${BASE_NAME}-rg -n vm03 --image Canonical:UbuntuServer:16.04.0-LTS:latest --size Standard_B1s --nics vm03-nic
$ az network public-ip show -g private-dns-poc-ejp-rg -n vm03-pip --query ipAddress
&amp;quot;13.78.54.133&amp;quot;
$ ssh 13.78.54.133
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;名前解決の確認が目的なので、vnet01/02間はPeeringしません。&lt;/p&gt;

&lt;p&gt;では、vnet01上のvm01を正引きします。ドメイン名を指定しないと、解決できません。vnet02上にvm01がある、と指定されたと判断するからです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vm03:~$ nslookup vm01
Server:         168.63.129.16
Address:        168.63.129.16#53

** server can&#39;t find vm01: SERVFAIL
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ではプライベートゾーンのドメイン名をつけてみます。解決できました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vm03:~$ nslookup vm01.example.com
Server:         168.63.129.16
Address:        168.63.129.16#53

Non-authoritative answer:
Name:   vm01.example.com
Address: 10.0.0.4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Resolution VNetからは、逆引きできません。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vm03:~$ nslookup 10.0.0.4
Server:         168.63.129.16
Address:        168.63.129.16#53

** server can&#39;t find 4.0.0.10.in-addr.arpa: NXDOMAIN
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ところでRegistration VNetからResolution VNetのホスト名をnslookupするとどうなるでしょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh vm01.example.com
vm01:~$ nslookup vm03
Server:         168.63.129.16
Address:        168.63.129.16#53

** server can&#39;t find vm03: SERVFAIL

vm01:~$ nslookup vm03.example.com
Server:         168.63.129.16
Address:        168.63.129.16#53

** server can&#39;t find vm03.example.com: NXDOMAIN
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ドメイン名あり、なしに関わらず、名前解決できません。VNetが別なのでVNetの内部DNSで解決できない、また、Resolution VNetのVMはレコードがプライベートゾーンに自動登録されないことが分かります。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>AzureのAvailability Zonesへ分散するVMSSをTerraformで作る</title>
      <link>https://ToruMakabe.github.io/post/az_vmss_terraform/</link>
      <pubDate>Mon, 26 Mar 2018 00:08:30 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/az_vmss_terraform/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;動機&#34;&gt;動機&lt;/h2&gt;

&lt;p&gt;Terraform Azure Provider 1.3.0で、VMSSを作る際にAvailability Zonesを指定できるように&lt;a href=&#34;https://github.com/terraform-providers/terraform-provider-azurerm/pull/811&#34;&gt;なりました&lt;/a&gt;。Availability Zonesはインフラの根っこの仕組みなので、現在(&lt;sup&gt;2018&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;)限定されたリージョンで長めのプレビュー期間がとられています。ですが、GAやグローバル展開を見据え、素振りしておきましょう。&lt;/p&gt;

&lt;h2 id=&#34;前提条件&#34;&gt;前提条件&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Availability Zones対応リージョンを選びます。現在は&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/availability-zones/az-overview#regions-that-support-availability-zones&#34;&gt;5リージョン&lt;/a&gt;です。この記事ではEast US 2とします。&lt;/li&gt;
&lt;li&gt;Availability Zonesのプレビューに&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/availability-zones/az-overview&#34;&gt;サインアップ&lt;/a&gt;済みとします。&lt;/li&gt;
&lt;li&gt;bashでsshの公開鍵が~/.ssh/id_rsa.pubにあると想定します。&lt;/li&gt;
&lt;li&gt;動作確認した環境は以下です。

&lt;ul&gt;
&lt;li&gt;Terraform 0.11.2&lt;/li&gt;
&lt;li&gt;Terraform Azure Provider 1.3.0&lt;/li&gt;
&lt;li&gt;WSL (ubuntu 16.04)&lt;/li&gt;
&lt;li&gt;macos (High Sierra 10.13.3)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;コード&#34;&gt;コード&lt;/h2&gt;

&lt;p&gt;以下のファイルを同じディレクトリに作成します。&lt;/p&gt;

&lt;h3 id=&#34;terraform-メインコード&#34;&gt;Terraform メインコード&lt;/h3&gt;

&lt;p&gt;VMSSと周辺リソースを作ります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;最終行近くの &amp;ldquo;zones = [1, 2, 3]&amp;rdquo; がポイントです。これだけで、インスタンスを散らす先のゾーンを指定できます。&lt;/li&gt;
&lt;li&gt;クロスゾーン負荷分散、冗長化するため、Load BalancerとパブリックIPのSKUをStandardにします。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[main.tf]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;resource &amp;quot;azurerm_resource_group&amp;quot; &amp;quot;poc&amp;quot; {
  name     = &amp;quot;${var.resource_group_name}&amp;quot;
  location = &amp;quot;East US 2&amp;quot;
}

resource &amp;quot;azurerm_virtual_network&amp;quot; &amp;quot;poc&amp;quot; {
  name                = &amp;quot;vnet01&amp;quot;
  resource_group_name = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  location            = &amp;quot;${azurerm_resource_group.poc.location}&amp;quot;
  address_space       = [&amp;quot;10.0.0.0/16&amp;quot;]
}

resource &amp;quot;azurerm_subnet&amp;quot; &amp;quot;poc&amp;quot; {
  name                      = &amp;quot;subnet01&amp;quot;
  resource_group_name       = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  virtual_network_name      = &amp;quot;${azurerm_virtual_network.poc.name}&amp;quot;
  address_prefix            = &amp;quot;10.0.2.0/24&amp;quot;
  network_security_group_id = &amp;quot;${azurerm_network_security_group.poc.id}&amp;quot;
}

resource &amp;quot;azurerm_network_security_group&amp;quot; &amp;quot;poc&amp;quot; {
  name                = &amp;quot;nsg01&amp;quot;
  resource_group_name = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  location            = &amp;quot;${azurerm_resource_group.poc.location}&amp;quot;

  security_rule = [
    {
      name                       = &amp;quot;allow_http&amp;quot;
      priority                   = 100
      direction                  = &amp;quot;Inbound&amp;quot;
      access                     = &amp;quot;Allow&amp;quot;
      protocol                   = &amp;quot;Tcp&amp;quot;
      source_port_range          = &amp;quot;*&amp;quot;
      destination_port_range     = &amp;quot;80&amp;quot;
      source_address_prefix      = &amp;quot;*&amp;quot;
      destination_address_prefix = &amp;quot;*&amp;quot;
    },
    {
      name                       = &amp;quot;allow_ssh&amp;quot;
      priority                   = 101
      direction                  = &amp;quot;Inbound&amp;quot;
      access                     = &amp;quot;Allow&amp;quot;
      protocol                   = &amp;quot;Tcp&amp;quot;
      source_port_range          = &amp;quot;*&amp;quot;
      destination_port_range     = &amp;quot;22&amp;quot;
      source_address_prefix      = &amp;quot;*&amp;quot;
      destination_address_prefix = &amp;quot;*&amp;quot;
    },
  ]
}

resource &amp;quot;azurerm_public_ip&amp;quot; &amp;quot;poc&amp;quot; {
  name                         = &amp;quot;pip01&amp;quot;
  resource_group_name          = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  location                     = &amp;quot;${azurerm_resource_group.poc.location}&amp;quot;
  public_ip_address_allocation = &amp;quot;static&amp;quot;
  domain_name_label            = &amp;quot;${var.scaleset_name}&amp;quot;

  sku = &amp;quot;Standard&amp;quot;
}

resource &amp;quot;azurerm_lb&amp;quot; &amp;quot;poc&amp;quot; {
  name                = &amp;quot;lb01&amp;quot;
  resource_group_name = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  location            = &amp;quot;${azurerm_resource_group.poc.location}&amp;quot;

  frontend_ip_configuration {
    name                 = &amp;quot;fipConf01&amp;quot;
    public_ip_address_id = &amp;quot;${azurerm_public_ip.poc.id}&amp;quot;
  }

  sku = &amp;quot;Standard&amp;quot;
}

resource &amp;quot;azurerm_lb_backend_address_pool&amp;quot; &amp;quot;poc&amp;quot; {
  name                = &amp;quot;bePool01&amp;quot;
  resource_group_name = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  loadbalancer_id     = &amp;quot;${azurerm_lb.poc.id}&amp;quot;
}

resource &amp;quot;azurerm_lb_rule&amp;quot; &amp;quot;poc&amp;quot; {
  name                           = &amp;quot;lbRule&amp;quot;
  resource_group_name            = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  loadbalancer_id                = &amp;quot;${azurerm_lb.poc.id}&amp;quot;
  protocol                       = &amp;quot;Tcp&amp;quot;
  frontend_port                  = 80
  backend_port                   = 80
  frontend_ip_configuration_name = &amp;quot;fipConf01&amp;quot;
  backend_address_pool_id        = &amp;quot;${azurerm_lb_backend_address_pool.poc.id}&amp;quot;
  probe_id                       = &amp;quot;${azurerm_lb_probe.poc.id}&amp;quot;
}

resource &amp;quot;azurerm_lb_probe&amp;quot; &amp;quot;poc&amp;quot; {
  name                = &amp;quot;http-probe&amp;quot;
  resource_group_name = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  loadbalancer_id     = &amp;quot;${azurerm_lb.poc.id}&amp;quot;
  port                = 80
}

resource &amp;quot;azurerm_lb_nat_pool&amp;quot; &amp;quot;poc&amp;quot; {
  count                          = 3
  name                           = &amp;quot;ssh&amp;quot;
  resource_group_name            = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  loadbalancer_id                = &amp;quot;${azurerm_lb.poc.id}&amp;quot;
  protocol                       = &amp;quot;Tcp&amp;quot;
  frontend_port_start            = 50000
  frontend_port_end              = 50119
  backend_port                   = 22
  frontend_ip_configuration_name = &amp;quot;fipConf01&amp;quot;
}

data &amp;quot;template_cloudinit_config&amp;quot; &amp;quot;poc&amp;quot; {
  gzip          = true
  base64_encode = true

  part {
    content_type = &amp;quot;text/cloud-config&amp;quot;
    content      = &amp;quot;${file(&amp;quot;${path.module}/cloud-config.yaml&amp;quot;)}&amp;quot;
  }
}

resource &amp;quot;azurerm_virtual_machine_scale_set&amp;quot; &amp;quot;poc&amp;quot; {
  name                = &amp;quot;${var.scaleset_name}&amp;quot;
  resource_group_name = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  location            = &amp;quot;${azurerm_resource_group.poc.location}&amp;quot;
  upgrade_policy_mode = &amp;quot;Manual&amp;quot;

  sku {
    name     = &amp;quot;Standard_B1s&amp;quot;
    tier     = &amp;quot;Standard&amp;quot;
    capacity = 3
  }

  storage_profile_image_reference {
    publisher = &amp;quot;Canonical&amp;quot;
    offer     = &amp;quot;UbuntuServer&amp;quot;
    sku       = &amp;quot;16.04-LTS&amp;quot;
    version   = &amp;quot;latest&amp;quot;
  }

  storage_profile_os_disk {
    name              = &amp;quot;&amp;quot;
    caching           = &amp;quot;ReadWrite&amp;quot;
    create_option     = &amp;quot;FromImage&amp;quot;
    managed_disk_type = &amp;quot;Standard_LRS&amp;quot;
  }

  os_profile {
    computer_name_prefix = &amp;quot;pocvmss&amp;quot;
    admin_username       = &amp;quot;${var.admin_username}&amp;quot;
    admin_password       = &amp;quot;&amp;quot;
    custom_data          = &amp;quot;${data.template_cloudinit_config.poc.rendered}&amp;quot;
  }

  os_profile_linux_config {
    disable_password_authentication = true

    ssh_keys {
      path     = &amp;quot;/home/${var.admin_username}/.ssh/authorized_keys&amp;quot;
      key_data = &amp;quot;${file(&amp;quot;~/.ssh/id_rsa.pub&amp;quot;)}&amp;quot;
    }
  }

  network_profile {
    name    = &amp;quot;terraformnetworkprofile&amp;quot;
    primary = true

    ip_configuration {
      name                                   = &amp;quot;PoCIPConfiguration&amp;quot;
      subnet_id                              = &amp;quot;${azurerm_subnet.poc.id}&amp;quot;
      load_balancer_backend_address_pool_ids = [&amp;quot;${azurerm_lb_backend_address_pool.poc.id}&amp;quot;]
      load_balancer_inbound_nat_rules_ids    = [&amp;quot;${element(azurerm_lb_nat_pool.poc.*.id, count.index)}&amp;quot;]
    }
  }

  zones = [1, 2, 3]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;cloud-init-configファイル&#34;&gt;cloud-init configファイル&lt;/h3&gt;

&lt;p&gt;各インスタンスがどのゾーンで動いているか確認したいので、インスタンス作成時にcloud-initでWebサーバーを仕込みます。メタデータからインスタンス名と実行ゾーンを引っ張り、nginxのドキュメントルートに書きます。&lt;/p&gt;

&lt;p&gt;[cloud-config.yaml]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#cloud-config
package_upgrade: true
packages:
  - nginx
runcmd:
  - &#39;echo &amp;quot;[Instance Name]: `curl -H Metadata:true &amp;quot;http://169.254.169.254/metadata/instance/compute/name?api-version=2017-12-01&amp;amp;format=text&amp;quot;`    [Zone]: `curl -H Metadata:true &amp;quot;http://169.254.169.254/metadata/instance/compute/zone?api-version=2017-12-01&amp;amp;format=text&amp;quot;`&amp;quot; &amp;gt; /var/www/html/index.nginx-debian.html&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;インスタンス作成時、パッケージの導入やアップデートに時間をかけたくない場合は、Packerなどで前もってカスタムイメージを作っておくのも手です。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/virtual-machines/linux/build-image-with-packer&#34;&gt;Packer を使用して Azure に Linux 仮想マシンのイメージを作成する方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/terraform/terraform-create-vm-scaleset-network-disks-using-packer-hcl&#34;&gt;Terraform を使用して Packer カスタム イメージから Azure 仮想マシン スケール セットを作成する&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;terraform-変数ファイル&#34;&gt;Terraform 変数ファイル&lt;/h3&gt;

&lt;p&gt;変数は別ファイルへ。&lt;/p&gt;

&lt;p&gt;[variables.tf]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;variable &amp;quot;resource_group_name&amp;quot; {
  default = &amp;quot;your-rg&amp;quot;
}

variable &amp;quot;scaleset_name&amp;quot; {
  default = &amp;quot;yourvmss01&amp;quot;
}

variable &amp;quot;admin_username&amp;quot; {
  default = &amp;quot;yourname&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;実行&#34;&gt;実行&lt;/h2&gt;

&lt;p&gt;では実行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ terraform init
$ terraform plan
$ terraform apply
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5分くらいで完了しました。このサンプルでは、この後のcloud-initのパッケージ処理に時間がかかります。待てない場合は前述の通り、カスタムイメージを使いましょう。&lt;/p&gt;

&lt;p&gt;インスタンスへのsshを通すよう、Load BalancerにNATを設定していますので、cloud-initの進捗は確認できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh -p 50000 yourname@yourvmss01.eastus2.cloudapp.azure.com
$ tail -f /var/log/cloud-init-output.log
Cloud-init v. 17.1 finished at Sun, 25 Mar 2018 10:41:40 +0000. Datasource DataSourceAzure [seed=/dev/sr0].  Up 611.51 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ではWebサーバーにアクセスしてみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ while true; do curl yourvmss01.eastus2.cloudapp.azure.com; sleep 1; done;
[Instance Name]: yourvmss01_2    [Zone]: 3
[Instance Name]: yourvmss01_0    [Zone]: 1
[Instance Name]: yourvmss01_2    [Zone]: 3
[Instance Name]: yourvmss01_1    [Zone]: 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;VMSSのインスタンスがゾーンに分散されたことが分かります。&lt;/p&gt;

&lt;p&gt;では、このままスケールアウトしてみましょう。main.tfのazurerm_virtual_machine_scale_set.poc.sku.capacityを3から4にし、再度applyします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Instance Name]: yourvmss01_1    [Zone]: 2
[Instance Name]: yourvmss01_3    [Zone]: 1
[Instance Name]: yourvmss01_3    [Zone]: 1
[Instance Name]: yourvmss01_1    [Zone]: 2
[Instance Name]: yourvmss01_3    [Zone]: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ダウンタイムなしに、yourvmss01_3が追加されました。すこぶる簡単。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>AKSのService作成時にホスト名を付ける</title>
      <link>https://ToruMakabe.github.io/post/aks_dns/</link>
      <pubDate>Mon, 12 Mar 2018 00:21:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/aks_dns/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;2つのやり口&#34;&gt;2つのやり口&lt;/h2&gt;

&lt;p&gt;Azure Container Service(AKS)はServiceを公開する際、パブリックIPを割り当てられます。でもIPだけじゃなく、ホスト名も同時に差し出して欲しいケースがありますよね。&lt;/p&gt;

&lt;p&gt;わたしの知る限り、2つの方法があります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AKS(k8s) 1.9で対応した&lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/47849&#34;&gt;DNSラベル名付与機能&lt;/a&gt;を使う&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/external-dns&#34;&gt;Kubenetes ExternalDNS&lt;/a&gt;を使ってAzure DNSへAレコードを追加する&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以下、AKS 1.9.2での実現手順です。&lt;/p&gt;

&lt;h2 id=&#34;dnsラベル名付与機能&#34;&gt;DNSラベル名付与機能&lt;/h2&gt;

&lt;p&gt;簡単です。Serviceのannotationsに定義するだけ。試しにnginxをServiceとして公開し、確認してみましょう。&lt;/p&gt;

&lt;p&gt;[nginx-label.yaml]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx
spec:
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: hogeginx
  annotations:
    service.beta.kubernetes.io/azure-dns-label-name: hogeginx
spec:
  selector:
    app: nginx
  type: LoadBalancer
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デプロイ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f nginx-label.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;パブリックIP(EXTERNAL-IP)が割り当てられた後、ラベル名が使えます。ルールは [ラベル名].[リージョン].cloudapp.azure.com です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl hogeginx.eastus.cloudapp.azure.com
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
[snip]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ドメイン名は指定しなくていいから、Service毎にホスト名を固定したいんじゃ、という場合にはこれでOK。&lt;/p&gt;

&lt;h2 id=&#34;kubenetes-externaldns&#34;&gt;Kubenetes ExternalDNS&lt;/h2&gt;

&lt;p&gt;任意のドメイン名を使いたい場合は、Incubatorプロジェクトのひとつ、Kubenetes ExternalDNSを使ってAzure DNSへAレコードを追加する手があります。本家の説明は&lt;a href=&#34;https://github.com/kubernetes-incubator/external-dns/blob/master/docs/tutorials/azure.md&#34;&gt;こちら&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;Kubenetes ExternalDNSは、Azure DNSなどAPIを持つDNSサービスを操作するアプリです。k8sのDeploymentとして動かせます。Route 53などにも対応。&lt;/p&gt;

&lt;p&gt;さて動かしてみましょう。前提として、すでにAzure DNSにゾーンがあるものとします。&lt;/p&gt;

&lt;p&gt;ExternalDNSがDNSゾーンを操作できるよう、サービスプリンシパルを作成しましょう。スコープはDNSゾーンが置かれているリソースグループ、ロールはContributorとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az ad sp create-for-rbac --role=&amp;quot;Contributor&amp;quot; --scopes=&amp;quot;/subscriptions/your-subscription-id/resourceGroups/hoge-dns-rg&amp;quot; -n hogeExtDnsSp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;appId、password、tenantを控えておいてください。次でsecretに使います。&lt;/p&gt;

&lt;p&gt;ではExteralDNSに渡すsecretを作ります。まずJSONファイルに書きます。&lt;/p&gt;

&lt;p&gt;[azure.json]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &amp;quot;tenantId&amp;quot;: &amp;quot;your-tenant&amp;quot;,
    &amp;quot;subscriptionId&amp;quot;: &amp;quot;your-subscription-id&amp;quot;,
    &amp;quot;aadClientId&amp;quot;: &amp;quot;your-appId&amp;quot;,
    &amp;quot;aadClientSecret&amp;quot;: &amp;quot;your-password&amp;quot;,
    &amp;quot;resourceGroup&amp;quot;: &amp;quot;hoge-dns-rg&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;JSONファイルを元に、secretを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create secret generic azure-config-file --from-file=azure.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ExteralDNSのマニフェストを作ります。ドメイン名はexmaple.comとしていますが、使うDNSゾーンに合わせてください。以下はRBACを使っていない環境での書き方です。&lt;/p&gt;

&lt;p&gt;[extdns.yaml]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: external-dns
spec:
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: external-dns
    spec:
      containers:
      - name: external-dns
        image: registry.opensource.zalan.do/teapot/external-dns:v0.4.8
        args:
        - --source=service
        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.
        - --provider=azure
        - --azure-resource-group=hoge-dns-rg # (optional) use the DNS zones from the tutorial&#39;s resource group
        volumeMounts:
        - name: azure-config-file
          mountPath: /etc/kubernetes
          readOnly: true
      volumes:
      - name: azure-config-file
        secret:
          secretName: azure-config-file
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ExternalDNSをデプロイします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f extdns.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ではホスト名を付与するServiceのマニフェストを作りましょう。先ほどのDNSラベル名付与機能と同様、annotationsへ定義します。&lt;/p&gt;

&lt;p&gt;[nginx-extdns.yaml]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx-extdns
spec:
  template:
    metadata:
      labels:
        app: nginx-extdns
    spec:
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: hogeginx-extdns
  annotations:
    external-dns.alpha.kubernetes.io/hostname: hogeginx.example.com
spec:
  selector:
    app: nginx-extdns
  type: LoadBalancer
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デプローイ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f nginx-extdns.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;パブリックIP(EXTERNAL-IP)が割り当てられた後、Aレコードが登録されます。確認してみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az network dns record-set a list -g hoge-dns-rg -z example.com -o table
Name      ResourceGroup       Ttl  Type    Metadata
--------  ----------------  -----  ------  ----------
hogeginx  hoge-dns-rg         300  A
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ゲッツ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl hogeginx.example.com
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
[snip]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Incubatorプロジェクトなので今後大きく変化する可能性がありますが、ご参考になれば。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>AKSのIngress TLS証明書を自動更新する</title>
      <link>https://ToruMakabe.github.io/post/aks_tls_autorenewal/</link>
      <pubDate>Sun, 11 Feb 2018 00:20:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/aks_tls_autorenewal/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;カジュアルな証明書管理方式が欲しい&#34;&gt;カジュアルな証明書管理方式が欲しい&lt;/h2&gt;

&lt;p&gt;ChromeがHTTPサイトに対する警告を&lt;a href=&#34;https://japan.cnet.com/article/35100589/&#34;&gt;強化するそうです&lt;/a&gt;。非HTTPSサイトには、生きづらい世の中になりました。&lt;/p&gt;

&lt;p&gt;さてそうなると、TLS証明書の入手と更新、めんどくさいですね。ガチなサイトでは証明書の維持管理を計画的に行うべきですが、検証とかちょっとした用途で立てるサイトでは、とにかくめんどくさい。カジュアルな方式が望まれます。&lt;/p&gt;

&lt;p&gt;そこで、Azure Container Service(AKS)で使える気軽な方法をご紹介します。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;TLSはIngress(NGINX Ingress Controller)でまとめて終端&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://letsencrypt.org/&#34;&gt;Let&amp;rsquo;s Encypt&lt;/a&gt;から証明書を入手&lt;/li&gt;
&lt;li&gt;Kubenetesのアドオンである&lt;a href=&#34;https://github.com/jetstack/cert-manager/&#34;&gt;cert-manager&lt;/a&gt;で証明書の入手、更新とIngressへの適用を自動化

&lt;ul&gt;
&lt;li&gt;ACME(Automatic Certificate Management Environment)対応&lt;/li&gt;
&lt;li&gt;cert-managerはまだ歴史の浅いプロジェクトだが、&lt;a href=&#34;https://github.com/jetstack/cert-manager/&#34;&gt;kube-lego&lt;/a&gt;の後継として期待&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;なおKubernetes/AKSは開発ペースやエコシステムの変化が速いので要注意。この記事は2018/2/10に書いています。&lt;/p&gt;

&lt;h2 id=&#34;使い方&#34;&gt;使い方&lt;/h2&gt;

&lt;p&gt;AKSクラスターと、Azure DNS上に利用可能なゾーンがあることを前提にします。ない場合、それぞれ公式ドキュメントを参考にしてください。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/aks/kubernetes-walkthrough&#34;&gt;Azure Container Service (AKS) クラスターのデプロイ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/dns/dns-getstarted-cli&#34;&gt;Azure CLI 2.0 で Azure DNS の使用を開始する&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;まずAKSにNGINX Ingress Controllerを導入します。helmで入れるのが楽でしょう。&lt;a href=&#34;http://torumakabe.github.io/post/aks_ingress_quickdeploy/&#34;&gt;この記事&lt;/a&gt;も参考に。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm install stable/nginx-ingress --name my-nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サービスの状況を確認します。NGINX Ingress ControllerにEXTERNAL-IPが割り当てられるまで、待ちます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get svc
NAME                                     TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                     AGE
kubernetes                               ClusterIP      10.0.0.1       &amp;lt;none&amp;gt;           443/TCP                     79d
my-nginx-nginx-ingress-controller        LoadBalancer   10.0.2.105     52.234.148.138   80:30613/TCP,443:30186/TCP   6m
my-nginx-nginx-ingress-default-backend   ClusterIP      10.0.102.246   &amp;lt;none&amp;gt;           80/TCP                     6m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;EXTERNAL-IPが割り当てられたら、Azure DNSで名前解決できるようにします。Azure CLIを使います。Ingressのホスト名をwww.example.comとする例です。このホスト名で、後ほどLet&amp;rsquo;s Encryptから証明書を取得します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az network dns record-set a add-record -z example.com -g your-dnszone-rg -n www -a 52.234.148.138
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cert-managerのソースをGitHubから取得し、contribからhelm installします。いずれstableを使えるようになるでしょう。なお、このAKSクラスターはまだRBACを使っていないので、&amp;rdquo;&amp;ndash;set rbac.create=false&amp;rdquo;オプションを指定しています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/jetstack/cert-manager
$ cd cert-manager/
$ helm install --name cert-manager --namespace kube-system contrib/charts/cert-manager --set rbac.create=false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;では任意の作業ディレクトリに移動し、以下の内容でマニフェストを作ります。cm-issuer-le-staging-sample.yamlとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: certmanager.k8s.io/v1alpha1
kind: Issuer
metadata:
  name: letsencrypt-staging
  namespace: default
spec:
  acme:
    # The ACME server URL
    server: https://acme-staging.api.letsencrypt.org/directory
    # Email address used for ACME registration
    email: hoge@example.com
    # Name of a secret used to store the ACME account private key
    privateKeySecretRef:
      name: letsencrypt-staging
    # Enable the HTTP-01 challenge provider
    http01: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;証明書を発行してもらうLet&amp;rsquo;s EncryptをIssuerとして登録するわけですが、まずはステージングのAPIエンドポイントを指定しています。Let&amp;rsquo;s Encryptには&lt;a href=&#34;https://letsencrypt.org/docs/rate-limits/&#34;&gt;Rate Limit&lt;/a&gt;があり、失敗した時に痛いからです。Let&amp;rsquo;s EncryptのステージングAPIを使うとフェイクな証明書(Fake LE Intermediate X1)が発行されますが、流れの確認やマニフェストの検証は、できます。&lt;/p&gt;

&lt;p&gt;なお、Let&amp;rsquo;s Encryptとのチャレンジには今回、HTTPを使います。DNSチャレンジも&lt;a href=&#34;https://github.com/jetstack/cert-manager/pull/246&#34;&gt;いずれ対応する見込み&lt;/a&gt;です。&lt;/p&gt;

&lt;p&gt;では、Issuerを登録します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f cm-issuer-le-staging-sample.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次は証明書の設定です。マニフェストはcm-cert-le-staging-sample.yamlとします。acme節にACME構成を書きます。チャレンジはHTTP、ingressClassはnginxです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: example-com
  namespace: default
spec:
  secretName: example-com-tls
  issuerRef:
    name: letsencrypt-staging
  commonName: www.example.com
  dnsNames:
  - www.example.com
  acme:
    config:
    - http01:
        ingressClass: nginx
      domains:
      - www.example.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;証明書設定をデプロイします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f cm-cert-le-staging-sample.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;証明書の発行状況を確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe certificate example-com
Name:         example-com
Namespace:    default
[snip]
Events:
  Type     Reason                 Age              From                     Message
  ----     ------                 ----             ----                     -------
  Warning  ErrorCheckCertificate  8m               cert-manager-controller  Error checking existing TLS certificate: secret &amp;quot;example-com-tls&amp;quot; not found
  Normal   PrepareCertificate     8m               cert-manager-controller  Preparing certificate with issuer
  Normal   PresentChallenge       8m               cert-manager-controller  Presenting http-01 challenge for domain www.example.com
  Normal   SelfCheck              8m               cert-manager-controller  Performing self-check for domain www.example.com
  Normal   ObtainAuthorization    7m               cert-manager-controller  Obtained authorization for domain www.example.com
  Normal   IssueCertificate       7m               cert-manager-controller  Issuing certificate...
  Normal   CeritifcateIssued      7m               cert-manager-controller  Certificated issuedsuccessfully
  Normal   RenewalScheduled       7m (x2 over 7m)  cert-manager-controller  Certificate scheduled for renewal in 1438 hours
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;無事に証明書が発行され、更新もスケジュールされました。手順やマニフェストの書きっぷりは問題なさそうです。これをもってステージング完了としましょう。&lt;/p&gt;

&lt;p&gt;ではLet&amp;rsquo;s EncryptのAPIエンドポイントをProduction向けに変更し、新たにIssuer登録します。cm-issuer-le-prod-sample.yamlとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: certmanager.k8s.io/v1alpha1
kind: Issuer
metadata:
  name: letsencrypt-prod
  namespace: default
spec:
  acme:
    # The ACME server URL
    server: https://acme-v01.api.letsencrypt.org/directory
    # Email address used for ACME registration
    email: hoge@example.com
    # Name of a secret used to store the ACME account private key
    privateKeySecretRef:
      name: letsencrypt-prod
    # Enable the HTTP-01 challenge provider
    http01: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デプロイします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f cm-issuer-le-prod-sample.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同様に、Production向けの証明書設定をします。cm-cert-le-prod-sample.yamlとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: prod-example-com
  namespace: default
spec:
  secretName: prod-example-com-tls
  issuerRef:
    name: letsencrypt-prod
  commonName: www.example.com
  dnsNames:
  - www.example.com
  acme:
    config:
    - http01:
        ingressClass: nginx
      domains:
      - www.example.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デプロイします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f cm-cert-le-prod-sample.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;発行状況を確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe certificate prod-example-com
Name:         prod-example-com
Namespace:    default
[snip]
Events:
  Type     Reason                 Age              From                     Message
  ----     ------                 ----             ----                     -------
  Warning  ErrorCheckCertificate  27s              cert-manager-controller  Error checking existing TLS certificate: secret &amp;quot;prod-example-com-tls&amp;quot; not found
  Normal   PrepareCertificate     27s              cert-manager-controller  Preparing certificate with issuer
  Normal   PresentChallenge       26s              cert-manager-controller  Presenting http-01 challenge for domain www.example.com
  Normal   SelfCheck              26s              cert-manager-controller  Performing self-check for domain www.example.com
  Normal   IssueCertificate       7s               cert-manager-controller  Issuing certificate...
  Normal   ObtainAuthorization    7s               cert-manager-controller  Obtained authorization for domain www.example.com
  Normal   RenewalScheduled       6s (x3 over 5m)  cert-manager-controller  Certificate scheduled for renewal in 1438 hours
  Normal   CeritifcateIssued      6s               cert-manager-controller  Certificated issuedsuccessfully
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;証明書が発行され、1438時間(約60日)内の更新がスケジュールされました。&lt;/p&gt;

&lt;p&gt;ではバックエンドを設定して確認してみましょう。バックエンドにNGINXを立て、exposeします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run nginx --image nginx --port 80
$ kubectl expose deployment nginx --type NodePort
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ingressを設定します。ファイル名はingress-nginx-sample.yamlとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
  name: ingress-nginx-sample
spec:
  rules:
    - host: www.example.com
      http:
        paths:
          - path: /
            backend:
              serviceName: nginx
              servicePort: 80
  tls:
    - hosts:
      - www.example.com
      secretName: prod-example-com-tls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デプロイします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f ingress-nginx-sample.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;いざ確認。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl https://www.example.com/
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
[snip]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;便利ですね。Let&amp;rsquo;s Encryptをはじめ、関連プロジェクトに感謝です。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>AKSのNGINX Ingress Controllerのデプロイで悩んだら</title>
      <link>https://ToruMakabe.github.io/post/aks_ingress_quickdeploy/</link>
      <pubDate>Sat, 10 Feb 2018 11:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/aks_ingress_quickdeploy/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;楽したいならhelmで入れましょう&#34;&gt;楽したいならhelmで入れましょう&lt;/h2&gt;

&lt;p&gt;AKSに限った話ではありませんが、Kubernetesにぶら下げるアプリの数が多くなってくると、URLマッピングやTLS終端がしたくなります。方法は色々あるのですが、シンプルな選択肢はNGINX Ingress Controllerでしょう。&lt;/p&gt;

&lt;p&gt;さて、そのNGINX Ingress Controllerのデプロイは&lt;a href=&#34;https://github.com/kubernetes/ingress-nginx/blob/master/deploy/README.md&#34;&gt;GitHubのドキュメント&lt;/a&gt;通りに淡々とやればいいのですが、&lt;a href=&#34;https://github.com/kubernetes/helm&#34;&gt;helm&lt;/a&gt;を使えばコマンド一発です。そのようにドキュメントにも書いてあるのですが、最後の方で出てくるので「それ早く言ってよ」な感じです。&lt;/p&gt;

&lt;p&gt;せっかくなので、Azure(AKS)での使い方をまとめておきます。開発ペースやエコシステムの変化が速いので要注意。この記事は2018/2/10に書いています。&lt;/p&gt;

&lt;h2 id=&#34;使い方&#34;&gt;使い方&lt;/h2&gt;

&lt;p&gt;AKSクラスターと、Azure DNS上に利用可能なゾーンがあることを前提にします。ない場合、それぞれ公式ドキュメントを参考にしてください。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/aks/kubernetes-walkthrough&#34;&gt;Azure Container Service (AKS) クラスターのデプロイ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/dns/dns-getstarted-cli&#34;&gt;Azure CLI 2.0 で Azure DNS の使用を開始する&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ではhelmでNGINX Ingress Controllerを導入します。helmを使っていなければ、&lt;a href=&#34;https://github.com/kubernetes/helm#install&#34;&gt;入れておいてください&lt;/a&gt;。デプロイはこれだけ。Chartは&lt;a href=&#34;https://github.com/kubernetes/charts/tree/master/stable/nginx-ingress&#34;&gt;ここ&lt;/a&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm install stable/nginx-ingress --name my-nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;バックエンドへのつなぎが機能するか、Webアプリを作ってテストします。NGINXとApacheを選びました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run nginx --image nginx --port 80
$ kubectl run apache --image httpd --port 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サービスとしてexposeします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl expose deployment nginx --type NodePort
$ kubectl expose deployment apache --type NodePort
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;現時点のサービスたちを確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get svc
NAME                                     TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)                  AGE
apache                                   NodePort       10.0.244.167   &amp;lt;none&amp;gt;          80:30928/TCP                 14h
kubernetes                               ClusterIP      10.0.0.1       &amp;lt;none&amp;gt;          443/TCP                  79d
my-nginx-nginx-ingress-controller        LoadBalancer   10.0.91.78     13.72.108.187   80:32448/TCP,443:31991/TCP   14h
my-nginx-nginx-ingress-default-backend   ClusterIP      10.0.74.104    &amp;lt;none&amp;gt;          80/TCP                  14h
nginx                                    NodePort       10.0.191.16    &amp;lt;none&amp;gt;          80:30752/TCP                 14h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;AKSの場合はパブリックIPがNGINX Ingress Controllerに割り当てられます。EXTERNAL-IPがpendingの場合は割り当て中なので、しばし待ちます。&lt;/p&gt;

&lt;p&gt;割り当てられたら、EXTERNAL-IPをAzure DNSで名前解決できるようにしましょう。Azure CLIを使います。dev.example.comの例です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az network dns record-set a add-record -z example.com -g your-dnszone-rg -n dev -a 13.72.108.187
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;TLSが終端できるかも検証したいので、Secretを作ります。証明書とキーはLet&amp;rsquo;s Encryptで作っておきました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create secret tls example-tls --key privkey.pem --cert fullchain.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ではIngressを構成しましょう。以下をファイル名ingress-nginx-sample.yamlとして保存します。IngressでTLSを終端し、/へのアクセスは先ほどexposeしたNGINXのサービスへ、/apacheへのアクセスはApacheへ流します。rewrite-targetをannotaionsで指定するのを、忘れずに。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
  name: ingress-nginx-sample
spec:
  rules:
    - host: dev.example.com
      http:
        paths:
          - path: /
            backend:
              serviceName: nginx
              servicePort: 80
          - path: /apache
            backend:
              serviceName: apache
              servicePort: 80
  tls:
    - hosts:
      - dev.example.com
      secretName: example-tls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;あとは反映するだけ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f ingress-nginx-sample.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;curlで確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl https://dev.example.com
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
[snip]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/apacheへのパスも確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl https://dev.example.com/apache
&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;It works!&amp;lt;/h1&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;簡単ですね。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azureのリソースグループ限定 共同作成者をいい感じに作る</title>
      <link>https://ToruMakabe.github.io/post/azure_rg_contributor/</link>
      <pubDate>Mon, 22 Jan 2018 22:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_rg_contributor/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;共同作成者は-ちょっと強い&#34;&gt;共同作成者は、ちょっと強い&lt;/h2&gt;

&lt;p&gt;Azureのリソースグループは、リソースを任意のグループにまとめ、ライフサイクルや権限の管理を一括して行える便利なコンセプトです。&lt;/p&gt;

&lt;p&gt;ユースケースのひとつに、&amp;rdquo;本番とは分離した開発向けリソースグループを作って、アプリ/インフラ開発者に開放したい&amp;rdquo;、があります。新しい技術は試行錯誤で身につくので、こういった環境は重要です。&lt;/p&gt;

&lt;p&gt;なのですが、このようなケースで、権限付与の落とし穴があります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;サブスクリプション所有者が開発用リソースグループを作る&lt;/li&gt;
&lt;li&gt;スコープを開発用リソースグループに限定し、開発者に対し共同作成者ロールを割り当てる&lt;/li&gt;
&lt;li&gt;開発者はリソースグループ限定で、のびのび試行錯誤できて幸せ&lt;/li&gt;
&lt;li&gt;開発者がスッキリしたくなり、リソースグループごとバッサリ削除 (共同作成者なので可能)&lt;/li&gt;
&lt;li&gt;開発者にはサブスクリプションレベルの権限がないため、リソースグループを作成できない&lt;/li&gt;
&lt;li&gt;詰む&lt;/li&gt;
&lt;li&gt;サブスクリプション所有者が、リソースグループ作成と権限付与をやり直し&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;共同作成者ロールから、リソースグループの削除権限だけを除外できると、いいんですが。そこでカスタムロールの出番です。リソースグループ限定、グループ削除権限なしの共同作成者を作ってみましょう。&lt;/p&gt;

&lt;h2 id=&#34;いい感じのカスタムロールを作る&#34;&gt;いい感じのカスタムロールを作る&lt;/h2&gt;

&lt;p&gt;Azureのカスタムロールは、個別リソースレベルで粒度の細かい権限設定ができます。ですが、やり過ぎると破綻するため、シンプルなロールを最小限作る、がおすすめです。&lt;/p&gt;

&lt;p&gt;シンプルに行きましょう。まずはカスタムロールの定義を作ります。role.jsonとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &amp;quot;Name&amp;quot;: &amp;quot;Resource Group Contributor&amp;quot;,
    &amp;quot;IsCustom&amp;quot;: true,
    &amp;quot;Description&amp;quot;: &amp;quot;Lets you manage everything except access to resources, but can not delete Resouce Group&amp;quot;,
    &amp;quot;Actions&amp;quot;: [
        &amp;quot;*&amp;quot;
    ],
    &amp;quot;NotActions&amp;quot;: [
        &amp;quot;Microsoft.Authorization/*/Delete&amp;quot;,
        &amp;quot;Microsoft.Authorization/*/Write&amp;quot;,
        &amp;quot;Microsoft.Authorization/elevateAccess/Action&amp;quot;,
        &amp;quot;Microsoft.Resources/subscriptions/resourceGroups/Delete&amp;quot;
    ],
    &amp;quot;AssignableScopes&amp;quot;: [
        &amp;quot;/subscriptions/your-subscriotion-id&amp;quot;
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;組み込みロールの共同作成者をテンプレに、NotActionsでリソースグループの削除権限を除外しました。AssignableScopesでリソースグループを限定してもいいですが、リソースグループの数だけロールを作るのはつらいので、ここでは指定しません。後からロールを割り当てる時にスコープを指定します。&lt;/p&gt;

&lt;p&gt;では、カスタムロールを作成します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az role definition create --role-definition ./role.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;出力にカスタムロールのIDが入っていますので、控えておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;id&amp;quot;: &amp;quot;/subscriptions/your-subscriotion-id/providers/Microsoft.Authorization/roleDefinitions/your-customrole-id&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;カスタムロールをユーザー-グループ-サービスプリンシパルに割り当てる&#34;&gt;カスタムロールをユーザー、グループ、サービスプリンシパルに割り当てる&lt;/h2&gt;

&lt;p&gt;次に、ユーザー/グループに先ほど作ったカスタムロールを割り当てます。スコープはリソースグループに限定します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az role assignment create --assignee-object-id your-user-or-group-object-id --role your-customrole-id --scope &amp;quot;/subscriptions/your-subscriotion-id/resourceGroups/sample-dev-rg&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サービスプリンシパル作成時に割り当てる場合は、以下のように。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az ad sp create-for-rbac -n &amp;quot;rgcontributor&amp;quot; -p &amp;quot;your-password&amp;quot; --role your-customrole-id --scopes &amp;quot;/subscriptions/your-subscriotion-id/resourceGroups/sample-dev-rg&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;余談ですが、&amp;rdquo;az ad sp create-for-rbac&amp;rdquo;コマンドはAzure ADアプリケーションを同時に作るため、別途アプリを作ってサービスプリンシパルと紐づける、という作業が要りません。&lt;/p&gt;

&lt;h2 id=&#34;試してみる&#34;&gt;試してみる&lt;/h2&gt;

&lt;p&gt;ログインして試してみましょう。サービスプリンシパルの例です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az login --service-principal -u &amp;quot;http://rgcontributor&amp;quot; -p &amp;quot;your-password&amp;quot; -t &amp;quot;your-tenant-id&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;検証したサブスクリプションには多数のリソースグループがあるのですが、スコープで指定したものだけが見えます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az group list -o table
Name              Location    Status
----------------  ----------  ---------
sample-dev-rg  japaneast   Succeeded
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このリソースグループに、VMを作っておきました。リストはしませんが、ストレージやネットワークなど関連リソースもこのグループにあります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az vm list -o table
Name              ResourceGroup     Location
----------------  ----------------  ----------
sampledevvm01     sample-dev-rg  japaneast
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;試しにリソースグループを作ってみます。サブスクリプションスコープの権限がないため怒られます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az group create -n rgc-poc-rg -l japaneast
The client &#39;aaaaa-bbbbb-ccccc-ddddd-eeeee&#39; with object id &#39;aaaaa-bbbbb-ccccc-ddddd-eeeee&#39; does not have authorization to perform action &#39;Microsoft.Resources/subscriptions/resourcegroups/write&#39; over scope &#39;/subscriptions/your-subscriotion-id/resourcegroups/rgc-poc-rg&#39;.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;リソースグループを消してみます。消すかい？ -&amp;gt; y -&amp;gt; ダメ、という、持ち上げて落とす怒り方です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az group delete -n sample-dev-rg
Are you sure you want to perform this operation? (y/n): y
The client &#39;aaaaa-bbbbb-ccccc-ddddd-eeeee&#39; with object id &#39;aaaaa-bbbbb-ccccc-ddddd-eeeee&#39; does not have authorization to perform action &#39;Microsoft.Resources/subscriptions/resourcegroups/delete&#39; over scope &#39;/subscriptions/your-subscriotion-id/resourcegroups/sample-dev-rg&#39;.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;でもリソースグループのリソースを一括削除したい&#34;&gt;でもリソースグループのリソースを一括削除したい&lt;/h2&gt;

&lt;p&gt;でも、リソースグループは消せなくても、リソースをバッサリ消す手段は欲しいですよね。そんな時には空のリソースマネージャーテンプレートを、completeモードでデプロイすると、消せます。&lt;/p&gt;

&lt;p&gt;空テンプレートを、empty.jsonとしましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &amp;quot;$schema&amp;quot;: &amp;quot;http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#&amp;quot;,
    &amp;quot;contentVersion&amp;quot;: &amp;quot;1.0.0.0&amp;quot;,
    &amp;quot;parameters&amp;quot;: {},
    &amp;quot;variables&amp;quot;: {},
    &amp;quot;resources&amp;quot;: [],
    &amp;quot;outputs&amp;quot;: {}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;破壊的空砲を打ちます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az group deployment create --mode complete -g sample-dev-rg --template-file ./empty.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;リソースグループは残ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az group list -o table
Name              Location    Status
----------------  ----------  ---------
sample-dev-rg  japaneast   Succeeded
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;VMは消えました。リストしませんが、他の関連リソースもバッサリ消えています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az vm list -o table

&lt;/code&gt;&lt;/pre&gt;
</content>
      
    </item>
    
    <item>
      <title>TerraformでAzure サンプル 2018/1版</title>
      <link>https://ToruMakabe.github.io/post/terraform_azure_sample_201801/</link>
      <pubDate>Mon, 08 Jan 2018 16:30:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/terraform_azure_sample_201801/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;サンプルのアップデート&#34;&gt;サンプルのアップデート&lt;/h2&gt;

&lt;p&gt;年末にリポジトリの大掃除をしていて、2年前に書いたTerraform &amp;amp; Azureの&lt;a href=&#34;http://torumakabe.github.io/post/azure_tf_fundamental_rules/&#34;&gt;記事&lt;/a&gt;に目が止まりました。原則はいいとして、&lt;a href=&#34;https://github.com/ToruMakabe/Terraform_Azure_Sample&#34;&gt;サンプル&lt;/a&gt;は2年物で腐りかけです。ということでアップデートします。&lt;/p&gt;

&lt;h2 id=&#34;インパクトの大きな変更点&#34;&gt;インパクトの大きな変更点&lt;/h2&gt;

&lt;p&gt;Terraformの、ここ2年の重要なアップデートは以下でしょうか。Azure視点で。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;BackendにAzure Blobを使えるようになった&lt;/li&gt;
&lt;li&gt;Workspaceで同一コード・複数環境管理ができるようになった&lt;/li&gt;
&lt;li&gt;対応リソースが増えた&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://registry.terraform.io/&#34;&gt;Terraform Module Registry&lt;/a&gt;が公開された&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;更新版サンプルの方針&#34;&gt;更新版サンプルの方針&lt;/h2&gt;

&lt;p&gt;重要アップデートをふまえ、以下の方針で新サンプルを作りました。&lt;/p&gt;

&lt;h3 id=&#34;チーム-複数端末での運用&#34;&gt;チーム、複数端末での運用&lt;/h3&gt;

&lt;p&gt;BackendにAzure Blobがサポートされたので、チーム、複数端末でstateの共有がしやすくなりました。ひとつのプロジェクトや環境を、チームメンバーがどこからでも、だけでなく、複数プロジェクトでのstate共有もできます。&lt;/p&gt;

&lt;h3 id=&#34;workspaceの導入&#34;&gt;Workspaceの導入&lt;/h3&gt;

&lt;p&gt;従来は /dev /stage /prodなど、環境別にコードを分けて管理していました。ゆえに環境間のコード同期が課題でしたが、TerraformのWorkspace機能で解決しやすくなりました。リソース定義で ${terraform.workspace} 変数を参照するように書けば、ひとつのコードで複数環境を扱えます。&lt;/p&gt;

&lt;p&gt;要件によっては、従来通り環境別にコードを分けた方がいいこともあるでしょう。環境間の差分が大きい、開発とデプロイのタイミングやライフサイクルが異なるなど、Workspaceが使いづらいケースもあるでしょう。その場合は無理せず従来のやり方で。今回のサンプルは「Workspaceを使ったら何ができるか？」を考えるネタにしてください。&lt;/p&gt;

&lt;h3 id=&#34;module-terraform-module-registryの活用&#34;&gt;Module、Terraform Module Registryの活用&lt;/h3&gt;

&lt;p&gt;TerraformのModuleはとても強力な機能なのですが、あーでもないこーでもないと、こだわり過ぎるとキリがありません。「うまいやり方」を見てから使いたいのが人情です。そこでTerraform Module Registryを活かします。お墨付きのVerifiedモジュールが公開されていますので、そのまま使うもよし、ライセンスを確認の上フォークするのもよし、です。&lt;/p&gt;

&lt;h3 id=&#34;リソースグループは環境ごとに準備し-管理をterraformから分離&#34;&gt;リソースグループは環境ごとに準備し、管理をTerraformから分離&lt;/h3&gt;

&lt;p&gt;AzureのリソースをプロビジョニングするTerraformコードの多くは、Azureのリソースグループを管理下に入れている印象です。すなわちdestroyするとリソースグループごとバッサリ消える。わかりやすいけど破壊的。&lt;/p&gt;

&lt;p&gt;TerraformはApp ServiceやACIなどPaaS、アプリ寄りのリソースも作成できるようになってきたので、アプリ開発者にTerraformを開放したいケースが増えてきています。dev環境をアプリ開発者とインフラ技術者がコラボして育て、そのコードをstageやprodにデプロイする、など。&lt;/p&gt;

&lt;p&gt;ところで。TerraformのWorkspaceは、こんな感じで簡単に切り替えられます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform workspace select prod
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;みなまで言わなくても分かりますね。悲劇はプラットフォーム側で回避しましょう。今回のサンプルではリソースグループをTerraform管理下に置かず、別途作成します。Terraformからはdata resourcesとしてRead Onlyで参照する実装です。環境別のリソースグループを作成し、dev環境のみアプリ開発者へ権限を付与します。&lt;/p&gt;

&lt;h2 id=&#34;サンプル解説&#34;&gt;サンプル解説&lt;/h2&gt;

&lt;p&gt;サンプルは&lt;a href=&#34;https://github.com/ToruMakabe/Terraform_Azure_Sample_201801&#34;&gt;GitHub&lt;/a&gt;に置きました。合わせてご確認ください。&lt;/p&gt;

&lt;p&gt;このコードをapplyすると、以下のリソースが出来上がります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;NGINX on Ubuntu Webサーバー VMスケールセット&lt;/li&gt;
&lt;li&gt;VMスケールセット向けロードバランサー&lt;/li&gt;
&lt;li&gt;踏み台サーバー&lt;/li&gt;
&lt;li&gt;上記を配置するネットワーク (仮想ネットワーク、サブネット、NSG)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;リポジトリ構造&#34;&gt;リポジトリ構造&lt;/h3&gt;

&lt;p&gt;サンプルのリポジトリ構造です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;├── modules
│   ├── computegroup
│   │   ├── main.tf
│   │   ├── os
│   │   │   ├── outputs.tf
│   │   │   └── variables.tf
│   │   ├── outputs.tf
│   │   └── variables.tf
│   ├── loadbalancer
│   │   ├── main.tf
│   │   ├── outputs.tf
│   │   └── variables.tf
│   └── network
│       ├── main.tf
│       ├── outputs.tf
│       └── variables.tf
└── projects
    ├── project_a
    │   ├── backend.tf
    │   ├── main.tf
    │   ├── outputs.tf
    │   └── variables.tf
    └── shared
        ├── backend.tf
        ├── main.tf
        ├── outputs.tf
        └── variables.tf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/modulesには&lt;a href=&#34;https://registry.terraform.io/browse?provider=azurerm&#34;&gt;Terraform Module Registry&lt;/a&gt;でVerifiedされているモジュールをフォークしたコードを入れました。フォークした理由は、リソースグループをdata resource化して参照のみにしたかったためです。&lt;/p&gt;

&lt;p&gt;そして、/projectsに2つのプロジェクトを作りました。プロジェクトでリソースとTerraformの実行単位、stateを分割します。sharedで土台となる仮想ネットワークと踏み台サーバー関連リソース、project_aでVMスケールセットとロードバランサーを管理します。&lt;/p&gt;

&lt;p&gt;このボリュームだとプロジェクトを分割する必然性は低いのですが、以下のケースにも対応できるように分けました。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;アプリ開発者がproject_a下でアプリ関連リソースに集中したい&lt;/li&gt;
&lt;li&gt;性能観点で分割したい (Terraformはリソース量につれて重くなりがち)&lt;/li&gt;
&lt;li&gt;有事を考慮し影響範囲を分割したい&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;プロジェクト間では、stateをremote_stateを使って共有します。サンプルではsharedで作成した仮想ネットワークのサブネットIDを&lt;a href=&#34;https://github.com/ToruMakabe/Terraform_Azure_Sample_201801/blob/master/projects/shared/outputs.tf#L1&#34;&gt;output&lt;/a&gt;し、project_aで参照できるよう&lt;a href=&#34;https://github.com/ToruMakabe/Terraform_Azure_Sample_201801/blob/master/projects/project_a/backend.tf.sample#L10&#34;&gt;定義&lt;/a&gt;しています。&lt;/p&gt;

&lt;h2 id=&#34;使い方&#34;&gt;使い方&lt;/h2&gt;

&lt;h3 id=&#34;前提&#34;&gt;前提&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Linux、WSL、macOSなどbash環境の実行例です&lt;/li&gt;
&lt;li&gt;SSHの公開鍵をTerraform実行環境の ~/.ssh/id_rsa.pub として準備してください&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;管理者向けのサービスプリンシパルを用意する&#34;&gt;管理者向けのサービスプリンシパルを用意する&lt;/h3&gt;

&lt;p&gt;インフラのプロビジョニングの主体者、管理者向けのサービスプリンシパルを用意します。リソースグループを作成できる権限が必要です。&lt;/p&gt;

&lt;p&gt;もしなければ作成します。組み込みロールでは、サブスクリプションに対するContributorが妥当でしょう。&lt;a href=&#34;https://www.terraform.io/docs/providers/azurerm/authenticating_via_service_principal.html&#34;&gt;Terraformのドキュメント&lt;/a&gt;も参考に。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az ad sp create-for-rbac --role=&amp;quot;Contributor&amp;quot; --scopes=&amp;quot;/subscriptions/SUBSCRIPTION_ID&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;出力されるappId、password、tenantを控えます。既存のサービスプリンシパルを使うのであれば、同情報を確認してください。&lt;/p&gt;

&lt;p&gt;なお参考までに。Azure Cloud ShellなどAzure CLIが導入されている環境では、特に認証情報の指定なしでterraform planやapply時にAzureのリソースにアクセスできます。TerraformがCLIの認証トークンを&lt;a href=&#34;https://github.com/terraform-providers/terraform-provider-azurerm/blob/master/azurerm/helpers/authentication/config.go&#34;&gt;使う&lt;/a&gt;からです。&lt;/p&gt;

&lt;p&gt;そしてBackendをAzure Blobとする場合、Blobにアクセスするためのキーが別途必要です。ですが、残念ながらBackendロジックでキーを得る際に、このトークンが&lt;a href=&#34;https://github.com/hashicorp/terraform/blob/master/backend/remote-state/azure/backend.go&#34;&gt;使われません&lt;/a&gt;。キーを明示することもできますが、Blobのアクセスキーは漏洩時のリカバリーが大変です。できれば直に扱いたくありません。&lt;/p&gt;

&lt;p&gt;サービスプリンシパル認証であれば、Azureリソースへのプロビジョニング、Backendアクセスどちらも&lt;a href=&#34;https://www.terraform.io/docs/backends/types/azurerm.html&#34;&gt;対応できます&lt;/a&gt;。これがこのサンプルでサービスプリンシパル認証を選んだ理由です。&lt;/p&gt;

&lt;h3 id=&#34;管理者の環境変数を設定する&#34;&gt;管理者の環境変数を設定する&lt;/h3&gt;

&lt;p&gt;Terraformが認証関連で必要な情報を環境変数で設定します。先ほど控えた情報を使います。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export ARM_SUBSCRIPTION_ID=&amp;quot;&amp;lt;your subscription id&amp;gt;&amp;quot;
export ARM_CLIENT_ID=&amp;quot;&amp;lt;your servicce principal appid&amp;gt;&amp;quot;
export ARM_CLIENT_SECRET=&amp;quot;&amp;lt;your service principal password&amp;gt;&amp;quot;
export ARM_TENANT_ID=&amp;quot;&amp;lt;your service principal tenant&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;workspaceを作る&#34;&gt;Workspaceを作る&lt;/h3&gt;

&lt;p&gt;開発(dev)/ステージング(stage)/本番(prod)、3つのWorkspaceを作る例です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform workspace new dev
terraform workspace new stage
terraform workspace new prod
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;リソースグループを作る&#34;&gt;リソースグループを作る&lt;/h3&gt;

&lt;p&gt;まずWorkspace別にリソースグループを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az group create -n tf-sample-dev-rg -l japaneast
az group create -n tf-sample-stage-rg -l japaneast
az group create -n tf-sample-prod-rg -l japaneast
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;リソースグループ名にはルールがあります。Workspace別にリソースグループを分離するため、Terraformのコードで ${terraform.workspace} 変数を使っているためです。この変数は実行時に評価されます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data &amp;quot;azurerm_resource_group&amp;quot; &amp;quot;resource_group&amp;quot; {
  name = &amp;quot;${var.resource_group_name}-${terraform.workspace}-rg&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;${var.resource_group_name} は接頭辞です。サンプルではvariables.tfで&amp;rdquo;tf-sample&amp;rdquo;と指定しています。&lt;/p&gt;

&lt;p&gt;次にBackend、state共有向けリソースグループを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az group create -n tf-sample-state-rg -l japaneast
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このリソースグループは、各projectのbackend.tfで指定しています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform {
  backend &amp;quot;azurerm&amp;quot; {
    resource_group_name  = &amp;quot;tf-sample-state-rg&amp;quot;
    storage_account_name = &amp;quot;&amp;lt;your storage account name&amp;gt;&amp;quot;
    container_name       = &amp;quot;tfstate-project-a&amp;quot;
    key                  = &amp;quot;terraform.tfstate&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最後にアプリ開発者がリソースグループtf-sample-dev-rg、tf-sample-state-rgへアクセスできるよう、アプリ開発者向けサービスプリンシパルを作成します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az ad sp create-for-rbac --role=&amp;quot;Contributor&amp;quot; --scopes &amp;quot;/subscriptions/&amp;lt;your subscription id&amp;gt;/resourceGroups/tf-sample-dev-rg&amp;quot; &amp;quot;/subscriptions/&amp;lt;your subscription id&amp;gt;/resourceGroups/tf-sample-state-rg&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;出力されるappId、password、tenantは、アプリ開発者向けに控えておきます。&lt;/p&gt;

&lt;h3 id=&#34;backendを準備する&#34;&gt;Backendを準備する&lt;/h3&gt;

&lt;p&gt;project別にストレージアカウントとコンテナーを作ります。tf-sample-state-rgに&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ストレージアカウント (名前は任意)&lt;/li&gt;
&lt;li&gt;コンテナー *2 (tfstate-project-a, tfstate-shared)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;を作ってください。GUIでもCLIでも、お好きなやり方で。&lt;/p&gt;

&lt;p&gt;その後、project_a/backend.tf.sample、shared/backend.tf.sampleをそれぞれbackend.tfにリネームし、先ほど作ったストレージアカウント名を指定します。以下はproject_a/backend.tf.sampleの例。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform {
  backend &amp;quot;azurerm&amp;quot; {
    resource_group_name  = &amp;quot;tf-sample-state-rg&amp;quot;
    storage_account_name = &amp;quot;&amp;lt;your storage account name&amp;gt;&amp;quot;
    container_name       = &amp;quot;tfstate-project-a&amp;quot;
    key                  = &amp;quot;terraform.tfstate&amp;quot;
  }
}

data &amp;quot;terraform_remote_state&amp;quot; &amp;quot;shared&amp;quot; {
  backend = &amp;quot;azurerm&amp;quot;

  config {
    resource_group_name  = &amp;quot;tf-sample-state-rg&amp;quot;
    storage_account_name = &amp;quot;&amp;lt;your storage account name&amp;gt;&amp;quot;
    container_name       = &amp;quot;tfstate-shared&amp;quot;
    key                  = &amp;quot;terraform.tfstateenv:${terraform.workspace}&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これで準備完了です。&lt;/p&gt;

&lt;h3 id=&#34;実行&#34;&gt;実行&lt;/h3&gt;

&lt;p&gt;Workspaceをdevに切り替えます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform workspace select dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;まずは土台となるリソースを作成するsharedから。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd shared
terraform init
terraform plan
terraform apply
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;土台となるリソースが作成されたら、次はproject_aを。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd ../project_a
terraform init
terraform plan
terraform apply
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここでは割愛しますが、dev向けサービスプリンシパルで認証しても、dev Workspaceではplan、apply可能です。&lt;/p&gt;

&lt;p&gt;dev Workspaceでコードが育ったら、stage/prod Workspaceに切り替えて実行します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform workspace select stage
[以下devと同様の操作]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然、dev向けサービスプリンシパルで認証している場合は、stage/prodでのplan、apply、もちろんdestroyも失敗します。stage/prod リソースグループにアクセスする権限がないからです。&lt;/p&gt;

&lt;h2 id=&#34;参考情報&#34;&gt;参考情報&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/terraform/&#34;&gt;Terraform on Azure のドキュメント&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/terraform-providers/terraform-provider-azurerm/tree/master/examples&#34;&gt;サンプル集 on GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
      
    </item>
    
    <item>
      <title>Windows上でLinux向けGoバイナリをDockerでビルドする</title>
      <link>https://ToruMakabe.github.io/post/golang_build_onwin_tolnx_docker/</link>
      <pubDate>Mon, 04 Dec 2017 22:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/golang_build_onwin_tolnx_docker/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;小ネタです&#34;&gt;小ネタです&lt;/h2&gt;

&lt;p&gt;Goはクロスプラットフォーム開発しやすい言語なのですが、Windows上でLinux向けバイナリーをビルドするなら、gccが要ります。正直なところ入れたくありません。なのでDockerでやります。&lt;/p&gt;

&lt;h2 id=&#34;条件&#34;&gt;条件&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Docker for Windows

&lt;ul&gt;
&lt;li&gt;Linuxモード&lt;/li&gt;
&lt;li&gt;ドライブ共有&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;powershell窓で実行&#34;&gt;PowerShell窓で実行&lt;/h2&gt;

&lt;p&gt;ビルドしたいGoのソースがあるディレクトリで以下のコマンドを実行します。Linux向けバイナリーが同じディレクトリに出来ます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run --rm -it -e GOPATH=/go --mount type=bind,source=${env:GOPATH},target=/go --mount type=bind,source=${PWD},target=/work -w /work golang:1.9.2-alpine go build -a -tags netgo -installsuffix netgo -o yourapp_linux
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;golang:1.9.2-alpine DockerイメージはGOPATHに/goを&lt;a href=&#34;https://github.com/docker-library/golang/blob/0f5ee2149d00dcdbf48fca05acf582e45d8fa9a5/1.9/alpine3.6/Dockerfile&#34;&gt;設定して&lt;/a&gt;ビルドされていますが、念のため実行時にも設定&lt;/li&gt;
&lt;li&gt;-v オプションでのマウントは&lt;a href=&#34;https://docs.docker.com/engine/admin/volumes/bind-mounts/&#34;&gt;非推奨&lt;/a&gt;になったので &amp;ndash;mount で&lt;/li&gt;
&lt;li&gt;スタティックリンク&lt;/li&gt;
&lt;/ul&gt;
</content>
      
    </item>
    
    <item>
      <title>Azure Blob アップローダーをGoで書いた、そしてその理由</title>
      <link>https://ToruMakabe.github.io/post/azblob_golang/</link>
      <pubDate>Tue, 28 Nov 2017 08:45:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azblob_golang/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;azure-blob-アップローダーをgoで書いた&#34;&gt;Azure Blob アップローダーをGoで書いた&lt;/h2&gt;

&lt;p&gt;ふたつほど理由があり、GolangでAzure Blobのファイルアップローダーを書きました。&lt;/p&gt;

&lt;h2 id=&#34;ひとつめの理由-sdkが新しくなったから&#34;&gt;ひとつめの理由: SDKが新しくなったから&lt;/h2&gt;

&lt;p&gt;最近公式ブログで&lt;a href=&#34;https://azure.microsoft.com/en-us/blog/preview-the-new-azure-storage-sdk-for-go-storage-sdks-roadmap/&#34;&gt;紹介された&lt;/a&gt;通り、Azure Storage SDK for Goが再設計され、プレビューが始まりました。GoはDockerやKubernetes、Terraformなど最近話題のプラットフォームやツールを書くのに使われており、ユーザーも増えています。再設計してもっと使いやすくしてちょ、という要望が多かったのも、うなずけます。&lt;/p&gt;

&lt;p&gt;ということで、新しいSDKで書いてみたかった、というのがひとつめの理由です。ローカルにあるファイルを読んでBlobにアップロードするコードは、こんな感じ。&lt;/p&gt;

&lt;h3 id=&#34;2018-6-17-更新&#34;&gt;(2018/6/17) 更新&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;SDKバージョンを 2017-07-29 へ変更&lt;/li&gt;
&lt;li&gt;関数 UploadStreamToBlockBlob を UploadFileToBlockBlob に変更&lt;/li&gt;
&lt;li&gt;Parallelism オプションを追加&lt;/li&gt;
&lt;li&gt;ヘルパー関数 handleErrors を追加&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
	&amp;quot;context&amp;quot;
	&amp;quot;flag&amp;quot;
	&amp;quot;fmt&amp;quot;
	&amp;quot;log&amp;quot;
	&amp;quot;net/url&amp;quot;
	&amp;quot;os&amp;quot;

	&amp;quot;github.com/Azure/azure-storage-blob-go/2017-07-29/azblob&amp;quot;
)

var (
	accountName    string
	accountKey     string
	containerName  string
	fileName       string
	blockSize      int64
	blockSizeBytes int64
	parallelism    int64
)

func init() {
	flag.StringVar(&amp;amp;accountName, &amp;quot;account-name&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;(Required) Storage Account Name&amp;quot;)
	flag.StringVar(&amp;amp;accountKey, &amp;quot;account-key&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;(Required) Storage Account Key&amp;quot;)
	flag.StringVar(&amp;amp;containerName, &amp;quot;c&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;(Required - short option) Blob Container Name&amp;quot;)
	flag.StringVar(&amp;amp;containerName, &amp;quot;container-name&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;(Required) Blob Container Name&amp;quot;)
	flag.StringVar(&amp;amp;fileName, &amp;quot;f&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;(Required - short option) Upload filename&amp;quot;)
	flag.StringVar(&amp;amp;fileName, &amp;quot;file&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;(Required) Upload filename&amp;quot;)
	flag.Int64Var(&amp;amp;blockSize, &amp;quot;b&amp;quot;, 4, &amp;quot;(Optional - short option) Blob Blocksize (MB) - From 1 to 100. Max filesize depends on this value. Max filesize = Blocksize * 50,000 blocks&amp;quot;)
	flag.Int64Var(&amp;amp;blockSize, &amp;quot;blocksize&amp;quot;, 4, &amp;quot;(Optional) Blob Blocksize (MB) - From 1 to 100. Max filesize depends on this value. Max filesize = Blocksize * 50,000 blocks&amp;quot;)
	flag.Int64Var(&amp;amp;parallelism, &amp;quot;p&amp;quot;, 5, &amp;quot;(Optional - short option) Parallelism - From 0 to 32. Default 5.&amp;quot;)
	flag.Int64Var(&amp;amp;parallelism, &amp;quot;parallelism&amp;quot;, 5, &amp;quot;(Optional) Parallelism - From 0 to 32. Default 5.&amp;quot;)
	flag.Parse()

	if (blockSize &amp;lt; 1) || (blockSize &amp;gt; 100) {
		fmt.Println(&amp;quot;Blocksize must be from 1MB to 100MB&amp;quot;)
		os.Exit(1)
	}
	blockSizeBytes = blockSize * 1024 * 1024

	if (parallelism &amp;lt; 0) || (parallelism &amp;gt; 32) {
		fmt.Println(&amp;quot;Parallelism must be from 0 to 32&amp;quot;)
		os.Exit(1)
	}
}

func handleErrors(err error) {
	if err != nil {
		if serr, ok := err.(azblob.StorageError); ok { // This error is a Service-specific
			switch serr.ServiceCode() { // Compare serviceCode to ServiceCodeXxx constants
			case azblob.ServiceCodeContainerAlreadyExists:
				fmt.Println(&amp;quot;Received 409. Container already exists&amp;quot;)
				return
			}
		}
		log.Fatal(err)
	}
}

func main() {
	file, err := os.Open(fileName)
	handleErrors(err)
	defer file.Close()

	fileSize, err := file.Stat()
	handleErrors(err)

	u, _ := url.Parse(fmt.Sprintf(&amp;quot;https://%s.blob.core.windows.net/%s/%s&amp;quot;, accountName, containerName, fileName))
	blockBlobURL := azblob.NewBlockBlobURL(*u, azblob.NewPipeline(azblob.NewSharedKeyCredential(accountName, accountKey), azblob.PipelineOptions{}))

	ctx := context.Background()

	fmt.Println(&amp;quot;Uploading block blob...&amp;quot;)
	response, err := azblob.UploadFileToBlockBlob(ctx, file, blockBlobURL,
		azblob.UploadToBlockBlobOptions{
			BlockSize: blockSizeBytes,
			Progress: func(bytesTransferred int64) {
				fmt.Printf(&amp;quot;Uploaded %d of %d bytes.\n&amp;quot;, bytesTransferred, fileSize.Size())
			},
			Parallelism: uint16(parallelism),
		})
	handleErrors(err)
	_ = response // Avoid compiler&#39;s &amp;quot;declared and not used&amp;quot; error

	fmt.Println(&amp;quot;Done&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以前のSDKと比較し、スッキリ書けるようになりました。進行状況もPipelineパッケージを使って、楽に取れるようになっています。ブロック分割のロジックを書く必要もなくなりました。ブロックサイズを指定すればOK。&lt;/p&gt;

&lt;p&gt;ちなみにファイルサイズがブロックサイズで割り切れると最終ブロックの転送がエラーになるバグを見つけたのですが、&lt;a href=&#34;https://github.com/Azure/azure-storage-blob-go/issues/8&#34;&gt;修正してもらった&lt;/a&gt;ので、次のリリースでは解決していると思います。&lt;/p&gt;

&lt;h2 id=&#34;ふたつめの理由-レガシー対応&#34;&gt;ふたつめの理由: レガシー対応&lt;/h2&gt;

&lt;p&gt;Blobのアップロードが目的であれば、Azure CLIをインストールすればOK。以上。なのですが、残念ながらそれができないケースがあります。&lt;/p&gt;

&lt;p&gt;たとえば。Azure CLI(2.0)はPythonで書かれています。なので、Pythonのバージョンや依存パッケージの兼ね合いで、「ちょっとそれウチのサーバーに入れるの？汚さないでくれる？ウチはPython2.6よ」と苦い顔をされることが、あるんですね。気持ちはわかります。立場の数だけ正義があります。Docker?その1歩半くらい前の話です。&lt;/p&gt;

&lt;p&gt;ですが、オンプレのシステムからクラウドにデータをアップロードして処理したい、なんていうニーズが急増している昨今、あきらめたくないわけであります。どうにか既存環境に影響なく入れられないものかと。そこでシングルバイナリーを作って、ポンと置いて、動かせるGoは尊いわけです。&lt;/p&gt;

&lt;p&gt;ファイルのアップロードだけでなく、Azureにちょっとした処理を任せたい、でもそれはいじりづらいシステムの上なのねん、って話は、結構多いんですよね。ということでシングルバイナリーを作って、ポンと置いて、動かせるGoは尊いわけです。大事なことなので2回書きました。&lt;/p&gt;

&lt;p&gt;C#やNode、Python SDKと比較してGoのそれはまだ物足りないところも多いわけですが、今後注目ということで地道に盛り上がっていこうと思います。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>自動化を愛するWindows使いへ Boxstarterのすすめ</title>
      <link>https://ToruMakabe.github.io/post/intro_boxstarter/</link>
      <pubDate>Fri, 13 Oct 2017 14:30:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/intro_boxstarter/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;windowsのセットアップどうする問題&#34;&gt;Windowsのセットアップどうする問題&lt;/h2&gt;

&lt;p&gt;そろそろFall Creators Updateが来ますね。これを機にクリーンインストールしようか、という人も多いのではないでしょうか。端末って使っているうちに汚れていく宿命なので、わたしは定期的に「こうあるべき」という状態に戻します。年に2～3回はスッキリしたい派なので、アップデートはいいタイミングです。&lt;/p&gt;

&lt;p&gt;でもクリーンインストールすると、設定やアプリケーションの導入をGUIでやり直すのが、すこぶるめんどくせぇわけです。自動化したいですね。そこでBoxstarterをおすすめします。便利なのに、意外に知られていない。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://boxstarter.org/&#34;&gt;Boxstarter&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;わたしはマイクロソフトの仲間、Jessieの&lt;a href=&#34;https://blog.jessfraz.com/post/windows-for-linux-nerds/&#34;&gt;ポスト&lt;/a&gt;で知りました。サンクスJessie。&lt;/p&gt;

&lt;h2 id=&#34;boxstarterで出来ること&#34;&gt;Boxstarterで出来ること&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;シンプルなスクリプトで

&lt;ul&gt;
&lt;li&gt;Windowsの各種設定&lt;/li&gt;
&lt;li&gt;Chocolateyパッケージの導入&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;設定ファイルをネットワーク経由で読み込める

&lt;ul&gt;
&lt;li&gt;Gistから&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ベアメタルでも仮想マシンでもOK&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;実行手順&#34;&gt;実行手順&lt;/h2&gt;

&lt;p&gt;手順は&lt;a href=&#34;http://boxstarter.org/Learn/WebLauncher&#34;&gt;Boxstarterのサイト&lt;/a&gt;で紹介されています。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;スクリプトを作る&lt;/li&gt;
&lt;li&gt;Gistに上げる&lt;/li&gt;
&lt;li&gt;Boxstarterを導入する&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;PowerShell 3以降であれば&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;. { iwr -useb http://boxstarter.org/bootstrapper.ps1 } | iex; get-boxstarter -Force
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Gist上のスクリプトを指定して実行する&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;なお2017/10/13時点で、Boxstarterサイトのサンプルにはtypoがあるので注意 (-PackageNameオプション)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Install-BoxstarterPackage -PackageName &amp;quot;https://gist.githubusercontent.com/ToruMakabe/976ceab239ec930f8651cfd72087afac/raw/4fc77a1d08f078869962ae82233b2f8abc32d31f/boxstarter.txt&amp;quot; -DisableReboots
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上。&lt;/p&gt;

&lt;h2 id=&#34;サンプルスクリプト&#34;&gt;サンプルスクリプト&lt;/h2&gt;

&lt;p&gt;スクリプトは&lt;a href=&#34;https://gist.github.com/ToruMakabe/976ceab239ec930f8651cfd72087afac&#34;&gt;こんな感じ&lt;/a&gt;に書きます。&lt;/p&gt;

&lt;p&gt;ちなみに、わたしの環境です。こまごまとした設定やツールの導入はもちろん、Hyper-Vやコンテナ、Windows Subsystem for Linuxの導入も、一気にやっつけます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Learn more: http://boxstarter.org/Learn/WebLauncher

# Chocolateyパッケージがないもの、パッケージ更新が遅いものは別途入れます。メモです。
# Install manually (Ubuntu, VS, snip, Azure CLI/PS/Storage Explorer, Terraform, Go, 1Password 6, Driver Management Tool)

#---- TEMPORARY ---
Disable-UAC

#--- Fonts ---
choco install inconsolata
  
#--- Windows Settings ---
# 可能な設定はここで確認 --&amp;gt; [Boxstarter WinConfig Features](http://boxstarter.org/WinConfig)
Disable-GameBarTips

Set-WindowsExplorerOptions -EnableShowHiddenFilesFoldersDrives -EnableShowFileExtensions
Set-TaskbarOptions -Size Small -Dock Bottom -Combine Full -Lock

Set-ItemProperty -Path HKCU:\Software\Microsoft\Windows\CurrentVersion\Explorer\Advanced -Name NavPaneShowAllFolders -Value 1

#--- Windows Subsystems/Features ---
choco install Microsoft-Hyper-V-All -source windowsFeatures
choco install Microsoft-Windows-Subsystem-Linux -source windowsfeatures
choco install containers -source windowsfeatures

#--- Tools ---
choco install git.install
choco install yarn
choco install sysinternals
choco install 7zip

#--- Apps ---
choco install googlechrome
choco install docker-for-windows
choco install microsoft-teams
choco install slack
choco install putty
choco install visualstudiocode

#--- Restore Temporary Settings ---
Enable-UAC
Enable-MicrosoftUpdate
Install-WindowsUpdate -acceptEula
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;便利。&lt;/p&gt;

&lt;p&gt;ちなみにわたしはドキュメント類はOneDrive、コードはプライベートGit/GitHub、エディタの設定はVisual Studio Code &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=Shan.code-settings-sync&#34;&gt;Settings Sync拡張&lt;/a&gt;を使っているので、Boxstarterと合わせ、 環境の再現は2～3時間もあればできます。最近、バックアップからのリストアとか、してないです。&lt;/p&gt;

&lt;p&gt;新しい端末の追加もすぐできるので、物欲が捗るという副作用もあります。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azure VPN Gateway Active/Active構成のスループット検証(リージョン内)</title>
      <link>https://ToruMakabe.github.io/post/azure_vpngw_act_act_perf/</link>
      <pubDate>Sun, 08 Oct 2017 10:30:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_vpngw_act_act_perf/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;動機&#34;&gt;動機&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kogelog.com/&#34;&gt;焦げlogさん&lt;/a&gt;で、とても興味深いエントリを拝見しました。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kogelog.com/2017/10/06/20171006-01/&#34;&gt;Azure VPN ゲートウェイをアクティブ/アクティブ構成した場合にスループットが向上するのか検証してみました&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;確かにActive/Active構成にはスループット向上を期待したくなります。その伸びが測定されており、胸が熱くなりました。ですが、ちょっと気になったのは&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;※それと、VpnGw3 よりも VpnGw2 のほうがスループットがよかったのが一番の謎ですが…&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ここです。VPN GatewayのSKU、VpnGw3とVpnGw2には小さくない価格差があり、その基準はスループットです。ここは現状を把握しておきたいところ。すごく。&lt;/p&gt;

&lt;p&gt;そこで、焦げlogさんの検証パターンの他に、追加で検証しました。それは同一リージョン内での測定です。リージョン内でVPNを張るケースはまれだと思いますが、リージョンが分かれることによる&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;遅延&lt;/li&gt;
&lt;li&gt;リージョン間通信に関するサムシング&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;を除き、VPN Gateway自身のスループットを測定したいからです。焦げlogさんの測定は東日本/西日本リージョン間で行われたので、その影響を確認する価値はあるかと考えました。&lt;/p&gt;

&lt;h2 id=&#34;検証方針&#34;&gt;検証方針&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;同一リージョン(東日本)に、2つのVNETを作る&lt;/li&gt;
&lt;li&gt;それぞれのVNETにVPN Gatewayを配置し、接続する&lt;/li&gt;
&lt;li&gt;比較しやすいよう、焦げlogさんの検証と条件を合わせる

&lt;ul&gt;
&lt;li&gt;同じ仮想マシンサイズ: DS3_V2&lt;/li&gt;
&lt;li&gt;同じストレージ: Premium Storage Managed Disk&lt;/li&gt;
&lt;li&gt;同じOS: Ubuntu 16.04&lt;/li&gt;
&lt;li&gt;同じツール: ntttcp&lt;/li&gt;
&lt;li&gt;同じパラメータ: ntttcp -r -m 16,*,&lt;IP&gt; -t 300&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;送信側 VNET1 -&amp;gt; 受信側 VNET2 のパターンに絞る&lt;/li&gt;
&lt;li&gt;スループットのポテンシャルを引き出す検証はしない&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;結果&#34;&gt;結果&lt;/h2&gt;

&lt;h3 id=&#34;vpngw1-650mbps&#34;&gt;VpnGW1(650Mbps)&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;パターン　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;送信側GW構成　　　　　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;受信側GW構成　　　　　　　　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;送信側スループット　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　受信側スループット&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　スループット平均&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　パターン1との比較&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン1　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;677.48Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;676.38Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;676.93Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン2　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;674.34Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;673.85Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;674.10Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;99%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン3　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;701.19Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;699.91Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;700.55Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;103%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;vpngw2-1gbps&#34;&gt;VpnGW2(1Gbps)&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;パターン　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;送信側GW構成　　　　　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;受信側GW構成　　　　　　　　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;送信側スループット　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　受信側スループット&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　スループット平均&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　パターン1との比較&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン1　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;813.09Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;805.60Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;809.35Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン2　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.18Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.18Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.18Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン3　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.02Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;256%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;vpngw3-1-25gbps&#34;&gt;VpnGW3(1.25Gbps)&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;パターン　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;送信側GW構成　　　　　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;受信側GW構成　　　　　　　　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;送信側スループット　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　受信側スループット&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　スループット平均&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　パターン1との比較&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン1　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;958.56Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;953.72Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;956.14Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン2　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.39Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.39Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.39Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン3　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.19Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.19Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.19Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;234%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;sku視点-パターン1-act-stb-to-act-stb&#34;&gt;SKU視点 パターン1(Act/Stb to Act/Stb)&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;SKU　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　スループット平均&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　VpnGw1との比較&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw1　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;676.93Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw2　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;809.35Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;119%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw3　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;956.14Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;141%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;sku視点-パターン2-act-stb-to-act-act&#34;&gt;SKU視点 パターン2(Act/Stb to Act/Act)&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;SKU　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　スループット平均&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　VpnGw1との比較&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw1　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;674.10Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw2　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.18Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;179%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw3　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.39Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;211%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;sku視点-パターン3-act-act-to-act-act&#34;&gt;SKU視点 パターン3(Act/Act to Act/Act)&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;SKU　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　スループット平均&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　VpnGw1との比較&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw1　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;700.55Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw2　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;297%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw3　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.19Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;320%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;考察と推奨&#34;&gt;考察と推奨&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;リージョン間の遅延やサムシングを除くと、SKUによるGatewayのスループット差は測定できる

&lt;ul&gt;
&lt;li&gt;Act/Actでないパターン1(Act/Stb to Act/Stb)で、その差がわかる&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;公式ドキュメントの通り、GatewayのAct/Act構成は可用性向上が目的であるため、スループットの向上はボーナスポイントと心得る

&lt;ul&gt;
&lt;li&gt;期待しちゃうのが人情ではありますが&lt;/li&gt;
&lt;li&gt;VpnGw2がコストパフォーマンス的に最適という人が多いかもしれませんね 知らんけど&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
      
    </item>
    
    <item>
      <title>Azure Event GridでBlobイベントを拾う</title>
      <link>https://ToruMakabe.github.io/post/azure_blobevent/</link>
      <pubDate>Tue, 05 Sep 2017 12:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_blobevent/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;event-gridがblobに対応&#34;&gt;Event GridがBlobに対応&lt;/h2&gt;

&lt;p&gt;Event GridがBlobのイベントを拾えるように&lt;a href=&#34;https://azure.microsoft.com/en-us/blog/announcing-azure-blob-storage-events-preview/&#34;&gt;なりました&lt;/a&gt;。まだ申請が必要なプライベートプレビュー段階ですが、使い勝手の良いサービスに育つ予感がします。このたび検証する機会があったので、共有を。&lt;/p&gt;

&lt;p&gt;プレビュー中なので、今後仕様が変わるかもしれないこと、不具合やメンテナンス作業の可能性などは、ご承知おきください。&lt;/p&gt;

&lt;h2 id=&#34;event-gridがblobに対応して何がうれしいか&#34;&gt;Event GridがBlobに対応して何がうれしいか&lt;/h2&gt;

&lt;p&gt;Event Gridは、Azureで発生した様々なイベントを検知してWebhookで通知するサービスです。カスタムトピックも作成できます。&lt;/p&gt;

&lt;p&gt;イベントの発生元をPublisherと呼びますが、このたびPublisherとしてAzureのBlobがサポートされました。Blobの作成、削除イベントを検知し、Event GridがWebhookで通知します。通知先はHandlerと呼びます。Publisherとそこで拾うイベント、Handlerを紐づけるのがSubscriptionです。Subscriptionにはフィルタも定義できます。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/ff3644c9-58ab-4729-8939-66a83ab0605d.png&#34; alt=&#34;コンセプト&#34; title=&#34;Concept&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Event Gridに期待する理由はいくつかあります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;フィルタ

&lt;ul&gt;
&lt;li&gt;特定のBlobコンテナーにあるjpegファイルの作成イベントのみで発火させる、なんてことができます&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;信頼性

&lt;ul&gt;
&lt;li&gt;リトライ機能があるので、Handlerが一時的に黙ってしまっても対応できます&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;スケールと高スループット

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/azure-functions/functions-bindings-storage-blob#blob-storage-triggers-and-bindings&#34;&gt;Azure Functions Blobトリガー&lt;/a&gt;のようにHandler側で定期的にスキャンする必要がありません。これまではファイル数が多いとつらかった&lt;/li&gt;
&lt;li&gt;具体的な数値はプレビュー後に期待しましょう&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ファンアウト

&lt;ul&gt;
&lt;li&gt;ひとつのイベントを複数のHandlerに紐づけられます&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Azureの外やサードパーティーとの連携

&lt;ul&gt;
&lt;li&gt;Webhookでシンプルにできます&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;前提条件&#34;&gt;前提条件&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Publisherに設定できるストレージアカウントはBlobストレージアカウントのみです。汎用ストレージアカウントは対応していません&lt;/li&gt;
&lt;li&gt;現時点ではWest Central USリージョンのみで提供しています&lt;/li&gt;
&lt;li&gt;プライベートプレビューは申請が必要です&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Azure CLIの下記コマンドでプレビューに申請できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az provider register --namespace  Microsoft.EventGrid
az feature register --name storageEventSubscriptions --namespace Microsoft.EventGrid
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドで確認し、statusが&amp;rdquo;Registered&amp;rdquo;であれば使えます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az feature show --name storageEventSubscriptions --namespace Microsoft.EventGrid
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;使い方&#34;&gt;使い方&lt;/h2&gt;

&lt;p&gt;ストレージアカウントの作成からSubscription作成までの流れを追ってみましょう。&lt;/p&gt;

&lt;p&gt;リソースグループを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az group create -n blobeventpoc-rg -l westcentralus
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Blobストレージアカウントを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az storage account create -n blobeventpoc01 -l westcentralus -g blobeventpoc-rg --sku Standard_LRS --kind BlobStorage --access-tier Hot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ではいよいよEvent GridのSubscriptionを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az eventgrid resource event-subscription create --endpoint https://requestb.in/y4jgj2x0 -n blobeventpocsub-jpg --prov
ider-namespace Microsoft.Storage --resource-type storageAccounts --included-event-types Microsoft.Storage.BlobCreated
-g blobeventpoc-rg --resource-name blobeventpoc01 --subject-ends-with jpg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下はパラメーターの補足です。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&amp;ndash;endpoint

&lt;ul&gt;
&lt;li&gt;Handlerのエンドポイントを指定します。ここではテストのために&lt;a href=&#34;https://requestb.in/&#34;&gt;RequestBin&lt;/a&gt;に作ったエンドポイントを指定します&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&amp;ndash;included-event-types

&lt;ul&gt;
&lt;li&gt;イベントの種類をフィルタします。Blobの削除イベントは不要で、作成のみ拾いたいため、Microsoft.Storage.BlobCreatedを指定します&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&amp;ndash;subject-ends-with

&lt;ul&gt;
&lt;li&gt;対象ファイルをフィルタします。Blob名の末尾文字列がjpgであるBlobのみイベントの対象にしました&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;では作成したストレージアカウントにBlobコンテナーを作成し、jpgファイルを置いてみましょう。テストには&lt;a href=&#34;https://azure.microsoft.com/ja-jp/features/storage-explorer/&#34;&gt;Azure Storage Explorer&lt;/a&gt;が便利です。&lt;/p&gt;

&lt;p&gt;RequestBinにWebhookが飛び、中身を見られます。スキーマの確認は&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-overview#event-schema&#34;&gt;こちら&lt;/a&gt;から。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[{
  &amp;quot;topic&amp;quot;: &amp;quot;/subscriptions/xxxxx-xxxxx-xxxxx-xxxxx/resourceGroups/blobeventpoc-rg/providers/Microsoft.Storage/storageAccounts/blobeventpoc01&amp;quot;,
  &amp;quot;subject&amp;quot;: &amp;quot;/blobServices/default/containers/images/blobs/handsomeyoungman.jpg&amp;quot;,
  &amp;quot;eventType&amp;quot;: &amp;quot;Microsoft.Storage.BlobCreated&amp;quot;,
  &amp;quot;eventTime&amp;quot;: &amp;quot;2017-09-02T02:25:15.2635962Z&amp;quot;,
  &amp;quot;id&amp;quot;: &amp;quot;f3ff6b96-001e-001d-6e92-23bdea0684d2&amp;quot;,
  &amp;quot;data&amp;quot;: {
    &amp;quot;api&amp;quot;: &amp;quot;PutBlob&amp;quot;,
    &amp;quot;clientRequestId&amp;quot;: &amp;quot;f3cab560-8f85-11e7-bad1-53b58c70ab53&amp;quot;,
    &amp;quot;requestId&amp;quot;: &amp;quot;f3ff6b96-001e-001d-6e92-23bdea000000&amp;quot;,
    &amp;quot;eTag&amp;quot;: &amp;quot;0x8D4F1A9D8A6703A&amp;quot;,
    &amp;quot;contentType&amp;quot;: &amp;quot;image/jpeg&amp;quot;,
    &amp;quot;contentLength&amp;quot;: 42497,
    &amp;quot;blobType&amp;quot;: &amp;quot;BlockBlob&amp;quot;,
    &amp;quot;url&amp;quot;: &amp;quot;https://blobeventpoc01.blob.core.windows.net/images/handsomeyoungman.jpg&amp;quot;,
    &amp;quot;sequencer&amp;quot;: &amp;quot;0000000000000BAB0000000000060986&amp;quot;,
    &amp;quot;storageDiagnostics&amp;quot;: {
      &amp;quot;batchId&amp;quot;: &amp;quot;f3a538cf-5b88-4bbf-908a-20a37c65e238&amp;quot;
    }
  }
}]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.jpgだけじゃなくて.jpegも使われるかもしれませんね。ということで、エンドポイントが同じでフィルタ定義を変えたSubscriptionを追加します。&amp;ndash;subject-ends-withをjpegとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az eventgrid resource event-subscription create --endpoint https://requestb.in/y4jgj2x0 -n blobeventpocsub-jpeg --pro
vider-namespace Microsoft.Storage --resource-type storageAccounts --included-event-types Microsoft.Storage.BlobCreated -
g blobeventpoc-rg --resource-name blobeventpoc01 --subject-ends-with jpeg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;すると、拡張子.jpegのファイルをアップロードしても発火しました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[{
  &amp;quot;topic&amp;quot;: &amp;quot;/subscriptions/xxxxx-xxxxx-xxxxx-xxxxx/resourceGroups/blobeventpoc-rg/providers/Microsoft.Storage/storageAccounts/blobeventpoc01&amp;quot;,
  &amp;quot;subject&amp;quot;: &amp;quot;/blobServices/default/containers/images/blobs/handsomeyoungman.jpeg&amp;quot;,
  &amp;quot;eventType&amp;quot;: &amp;quot;Microsoft.Storage.BlobCreated&amp;quot;,
  &amp;quot;eventTime&amp;quot;: &amp;quot;2017-09-02T02:36:33.827967Z&amp;quot;,
  &amp;quot;id&amp;quot;: &amp;quot;e8b036ee-001e-00e7-4994-23740d06225b&amp;quot;,
  &amp;quot;data&amp;quot;: {
    &amp;quot;api&amp;quot;: &amp;quot;PutBlob&amp;quot;,
    &amp;quot;clientRequestId&amp;quot;: &amp;quot;883ff7e0-8f87-11e7-bad1-53b58c70ab53&amp;quot;,
    &amp;quot;requestId&amp;quot;: &amp;quot;e8b036ee-001e-00e7-4994-23740d000000&amp;quot;,
    &amp;quot;eTag&amp;quot;: &amp;quot;0x8D4F1AB6D1B24F6&amp;quot;,
    &amp;quot;contentType&amp;quot;: &amp;quot;image/jpeg&amp;quot;,
    &amp;quot;contentLength&amp;quot;: 42497,
    &amp;quot;blobType&amp;quot;: &amp;quot;BlockBlob&amp;quot;,
    &amp;quot;url&amp;quot;: &amp;quot;https://blobeventpoc01.blob.core.windows.net/images/handsomeyoungman.jpeg&amp;quot;,
    &amp;quot;sequencer&amp;quot;: &amp;quot;0000000000000BAB0000000000060D42&amp;quot;,
    &amp;quot;storageDiagnostics&amp;quot;: {
      &amp;quot;batchId&amp;quot;: &amp;quot;9ec5c091-061d-4111-ad82-52d9803ce373&amp;quot;
    }
  }
}]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;azure-functionsに画像リサイズファンクションを作って連携してみる&#34;&gt;Azure Functionsに画像リサイズファンクションを作って連携してみる&lt;/h2&gt;

&lt;p&gt;Gvent Grid側の動きが確認できたので、サンプルアプリを作って検証してみましょう。Azure Functions上に画像ファイルのサイズを変えるHandlerアプリを作ってみます。&lt;/p&gt;

&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;

&lt;p&gt;当初想定したのは、ひとつのファンクションで、トリガーはEventGrid、入出力バインドにBlob、という作りでした。ですが、以下のように設計を変えました。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ToruMakabe/Images/master/blobevent-function-bindings.png&#34; alt=&#34;Bindings&#34; title=&#34;Bindings&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Using &lt;a href=&#34;https://functions-visualizer.azurewebsites.net/&#34;&gt;Azure Functions Bindings Visualizer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;その理由はEvent Grid Blobイベントのペイロードです。Blobファイル名がURLで渡されます。Azure FunctionsのBlob入出力バインド属性、&amp;rdquo;path&amp;rdquo;にURLは使えません。使えるのはコンテナー名+ファイル名です。&lt;/p&gt;

&lt;p&gt;入出力バインドを使わず、アプリのロジック内でStorage SDKを使って入出力してもいいのですが、Azure Functionsの魅力のひとつは宣言的にトリガーとバインドを定義し、アプリをシンプルに書けることなので、あまりやりたくないです。&lt;/p&gt;

&lt;p&gt;そこでイベントを受けてファイル名を取り出してQueueに入れるファンクションと、そのQueueをトリガーに画像をリサイズするファンクションに分けました。&lt;/p&gt;

&lt;p&gt;なお、この悩みはAzureの開発チームも認識しており、Functions側で対応する方針とのことです。&lt;/p&gt;

&lt;h3 id=&#34;handler&#34;&gt;Handler&lt;/h3&gt;

&lt;p&gt;C#(csx)で、Event GridからのWebhookを受けるHandlerを作ります。PublisherがBlobの場合、ペイロードにBlobのURLが入っていますので、そこからファイル名を抽出します。そして、そのファイル名をQueueに送ります。ファンクション名はBlobEventHandlerとしました。なおEventGridTriggerテンプレートは、現在は[試験段階]シナリオに入っています。&lt;/p&gt;

&lt;p&gt;[run.csx]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#r &amp;quot;Newtonsoft.json&amp;quot;
using Microsoft.Azure.WebJobs.Extensions.EventGrid;

public static void Run(EventGridEvent eventGridEvent, out string outputQueueItem, TraceWriter log)
{
    string imageUrl = eventGridEvent.Data[&amp;quot;url&amp;quot;].ToString();
    outputQueueItem = System.IO.Path.GetFileName(imageUrl);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Event GridのWebJobs拡張向けパッケージを指定します。&lt;/p&gt;

&lt;p&gt;[project.json]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
&amp;quot;frameworks&amp;quot;: {
  &amp;quot;net46&amp;quot;:{
    &amp;quot;dependencies&amp;quot;: {
      &amp;quot;Microsoft.Azure.WebJobs.Extensions.EventGrid&amp;quot;: &amp;quot;1.0.0-beta1-10006&amp;quot;
    }
  }
 }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;トリガーとバインドは以下の通りです。&lt;/p&gt;

&lt;p&gt;[function.json]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;bindings&amp;quot;: [
    {
      &amp;quot;type&amp;quot;: &amp;quot;eventGridTrigger&amp;quot;,
      &amp;quot;name&amp;quot;: &amp;quot;eventGridEvent&amp;quot;,
      &amp;quot;direction&amp;quot;: &amp;quot;in&amp;quot;
    },
    {
      &amp;quot;type&amp;quot;: &amp;quot;queue&amp;quot;,
      &amp;quot;name&amp;quot;: &amp;quot;outputQueueItem&amp;quot;,
      &amp;quot;queueName&amp;quot;: &amp;quot;imagefilename&amp;quot;,
      &amp;quot;connection&amp;quot;: &amp;quot;AzureWebJobsStorage&amp;quot;,
      &amp;quot;direction&amp;quot;: &amp;quot;out&amp;quot;
    }
  ],
  &amp;quot;disabled&amp;quot;: false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;resizer&#34;&gt;Resizer&lt;/h3&gt;

&lt;p&gt;Queueをトリガーに、Blobから画像ファイルを取り出し、縮小、出力するファンクションを作ります。ファンクション名はResizerとしました。&lt;/p&gt;

&lt;p&gt;[run.csx]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;using ImageResizer;

public static void Run(string myQueueItem, Stream inputBlob, Stream outputBlob, TraceWriter log)
{
  var imageBuilder = ImageResizer.ImageBuilder.Current;
  var size = imageDimensionsTable[ImageSize.Small];

  imageBuilder.Build(inputBlob, outputBlob,
    new ResizeSettings(size.Item1, size.Item2, FitMode.Max, null), false);

}

public enum ImageSize
{
  Small
}

private static Dictionary&amp;lt;ImageSize, Tuple&amp;lt;int, int&amp;gt;&amp;gt; imageDimensionsTable = new Dictionary&amp;lt;ImageSize, Tuple&amp;lt;int, int&amp;gt;&amp;gt;()
{
  { ImageSize.Small, Tuple.Create(100, 100) }
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ImageResizerのパッケージを指定します。&lt;/p&gt;

&lt;p&gt;[project.json]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
&amp;quot;frameworks&amp;quot;: {
  &amp;quot;net46&amp;quot;:{
    &amp;quot;dependencies&amp;quot;: {
      &amp;quot;ImageResizer&amp;quot;: &amp;quot;4.1.9&amp;quot;
    }
  }
 }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;トリガーとバインドは以下の通りです。{QueueTrigger}メタデータで、QueueのペイロードをBlobのpathに使います。ペイロードにはファイル名が入っています。&lt;/p&gt;

&lt;p&gt;また、画像を保存するBlobストレージアカウントの接続文字列は、環境変数BLOB_IMAGESへ事前に設定しています。なお、リサイズ後の画像を格納するBlobコンテナーは、&amp;rdquo;images-s&amp;rdquo;として別途作成しました。コンテナー&amp;rdquo;images&amp;rdquo;をイベントの発火対象コンテナーとして、Subscriptionにフィルタを定義したいからです。&lt;/p&gt;

&lt;p&gt;[function.json]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;bindings&amp;quot;: [
    {
      &amp;quot;name&amp;quot;: &amp;quot;myQueueItem&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;queueTrigger&amp;quot;,
      &amp;quot;direction&amp;quot;: &amp;quot;in&amp;quot;,
      &amp;quot;queueName&amp;quot;: &amp;quot;imagefilename&amp;quot;,
      &amp;quot;connection&amp;quot;: &amp;quot;AzureWebJobsStorage&amp;quot;
    },
    {
      &amp;quot;name&amp;quot;: &amp;quot;inputBlob&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;blob&amp;quot;,
      &amp;quot;path&amp;quot;: &amp;quot;images/{QueueTrigger}&amp;quot;,
      &amp;quot;connection&amp;quot;: &amp;quot;BLOB_IMAGES&amp;quot;,
      &amp;quot;direction&amp;quot;: &amp;quot;in&amp;quot;
    },
    {
      &amp;quot;name&amp;quot;: &amp;quot;outputBlob&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;blob&amp;quot;,
      &amp;quot;path&amp;quot;: &amp;quot;images-s/{QueueTrigger}&amp;quot;,
      &amp;quot;connection&amp;quot;: &amp;quot;BLOB_IMAGES&amp;quot;,
      &amp;quot;direction&amp;quot;: &amp;quot;out&amp;quot;
    }
  ],
  &amp;quot;disabled&amp;quot;: false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Handlerの準備が整いました。最後にEvent GridのSubscriptionを作成します。Azure FunctionsのBlobEventHandlerのトークン付きエンドポイントは、ポータルの[統合]で確認できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az eventgrid resource event-subscription create --endpoint &amp;quot;https://blobeventpoc.azurewebsites.net/admin/exte
nsions/EventGridExtensionConfig?functionName=BlobEventHandler&amp;amp;code=tokenTOKEN1234567890==&amp;quot; -n blobeventpocsub-jpg --provider-namespace Microsoft.Storage --resource-type storageAccounts --included-event-types &amp;quot;Microsoft.Storage.BlobCreated&amp;quot; -g blobeventpoc-rg --resource-name blobeventpoc01 --subject-begins-with &amp;quot;/blobServices/default/containers/images/&amp;quot;  --subject-ends-with jpg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これで、コンテナー&amp;rdquo;images&amp;rdquo;にjpgファイルがアップロードされると、コンテナー&amp;rdquo;images-s&amp;rdquo;に、リサイズされた同じファイル名の画像ファイルが出来上がります。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azureでグローバルにデータをコピーするとどのくらい時間がかかるのか</title>
      <link>https://ToruMakabe.github.io/post/azureblobcopy_perf/</link>
      <pubDate>Tue, 13 Jun 2017 17:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azureblobcopy_perf/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;ファイルコピーの需要は根強い&#34;&gt;ファイルコピーの需要は根強い&lt;/h2&gt;

&lt;p&gt;グローバルでAzureを使うとき、データをどうやって同期、複製するかは悩みの種です。Cosmos DBなどリージョン間でデータ複製してくれるサービスを使うのが、楽ですし、おすすめです。&lt;/p&gt;

&lt;p&gt;でも、ファイルコピーを無くせないもろもろの事情もあります。となると、「地球の裏側へのファイルコピーに、どんだけ時間かかるのよ」は、課題です。&lt;/p&gt;

&lt;h2 id=&#34;調べてみた&#34;&gt;調べてみた&lt;/h2&gt;

&lt;p&gt;ということで、いくつかのパターンで調べたので参考までに。測定環境は以下の通り。&lt;/p&gt;

&lt;h3 id=&#34;ツールと実行環境&#34;&gt;ツールと実行環境&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;AzCopy 6.1.0&lt;/li&gt;
&lt;li&gt;Azure PowerShell 4.1.0&lt;/li&gt;
&lt;li&gt;Windows 10 1703&lt;/li&gt;
&lt;li&gt;ThinkPad X1 Carbon 2017, Core i7-7600U 2.8GHz, 16GB Memory&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;アクセス回線パターン&#34;&gt;アクセス回線パターン&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;一般的な回線&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;自宅(川崎)&lt;/li&gt;
&lt;li&gt;OCN光 100M マンションタイプ&lt;/li&gt;
&lt;li&gt;宅内は802.11ac(5GHz)&lt;/li&gt;
&lt;li&gt;川崎でアクセス回線に入り、横浜(保土ヶ谷)の局舎からインターネットへ&lt;/li&gt;
&lt;li&gt;ゲートウェイ名から推測&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;いい感じの回線&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;日本マイクロソフト 品川オフィス&lt;/li&gt;
&lt;li&gt;1Gbps 有線&lt;/li&gt;
&lt;li&gt;Azureデータセンターへ「ネットワーク的に近くて広帯域」&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;コピーするファイル&#34;&gt;コピーするファイル&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;総容量: 約60GB

&lt;ul&gt;
&lt;li&gt;6160ファイル&lt;/li&gt;
&lt;li&gt;1MB * 5000, 10MB * 1000, 100MB * 100, 500MB * 50, 1000MB * 10&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Linux fallocateコマンドで作成&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ファイル形式パターン&#34;&gt;ファイル形式パターン&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;ファイル、Blobそのまま送る (6160ファイル)&lt;/li&gt;
&lt;li&gt;ディスクイメージで送る (1ファイル)

&lt;ul&gt;
&lt;li&gt;Managed Diskとしてアタッチした100GBの領域にファイルシステムを作成し、6160ファイルを配置&lt;/li&gt;
&lt;li&gt;転送前にデタッチ、エクスポート(Blob SAS形式)&lt;/li&gt;
&lt;li&gt;AzCopyではなくAzure PowerShellでコピー指示 (AzCopyにBlob SAS指定オプションが見当たらなかった)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;対象のazureリージョン&#34;&gt;対象のAzureリージョン&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;東日本 (マスター、複製元と位置づける)&lt;/li&gt;
&lt;li&gt;米国中南部 (太平洋越え + 米国内を見たい)&lt;/li&gt;
&lt;li&gt;ブラジル南部&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;転送パターン&#34;&gt;転送パターン&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;ユーザー拠点の端末からAzureリージョン: AzCopy Upload&lt;/li&gt;
&lt;li&gt;Azureリージョン間 (Storage to Storage)

&lt;ul&gt;
&lt;li&gt;ファイル: AzCopy Copy&lt;/li&gt;
&lt;li&gt;イメージ: PowerShell Start-AzureStorageBlobCopy&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;結果&#34;&gt;結果&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;形式　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;コピー元　　　　　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;コピー先　　　　　　　　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;コマンド　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　並列数&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　実行時間(時:分:秒)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;自宅&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 東日本&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Upload&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;07:55:22&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;自宅&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 米国中南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Upload&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10:22:30&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;自宅&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure ブラジル南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Upload&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12:46:37&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;オフィス&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 東日本&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Upload&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;00:20:47&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;オフィス&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 米国中南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Upload&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;00:45.11&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;オフィス&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure ブラジル南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Upload&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;02:07.58&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 東日本&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 米国中南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Copy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;N/A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;00:28:55&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;イメージ　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 東日本&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 米国中南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PowerShell&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;N/A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;00:11:11&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 東日本&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure ブラジル南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Copy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;N/A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;00.25:33&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;イメージ　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 東日本&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure ブラジル南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PowerShell&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;N/A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;00.09:20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;考察&#34;&gt;考察&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;アクセス回線の差が大きく影響&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;自宅パターンでプロバイダから帯域制限されていたかは不明 (自宅からAzure東日本まで16Mbpsくらいは出た)&lt;/li&gt;
&lt;li&gt;アクセス回線が細い場合はユーザー拠点から「まとめて」送らないほうがいい&lt;/li&gt;
&lt;li&gt;こまめに送る&lt;/li&gt;
&lt;li&gt;Azure内でデータを生成する&lt;/li&gt;
&lt;li&gt;もしくはExpressRouteを引く (自宅で、とは言っていない)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;アクセス回線が細い場合、AzCopy Uploadの並列数を下げる&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AzCopyのデフォルト並列数は実行環境のCPUコア数 *8だが、今回実施した端末での並列数(4コア * 8 = 32)ではかえって性能が劣化した&lt;/li&gt;
&lt;li&gt;アクセス回線に合わせて並列数は調整する&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Azureのリージョン間コピーは早い&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Azureバックボーンを通るから&lt;/li&gt;
&lt;li&gt;端末よりAzureストレージのほうがリソース的に強いし負荷分散しているから&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;地理的な距離感覚だけで考えてはダメ&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;地理的な近さではなく、ネットワーク的な近さと太さ&lt;/li&gt;
&lt;li&gt;Azureバックボーンを使うと日本とブラジルの間でもそれなりのスループット&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ファイル数が多いときはイメージで送るのも手&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ファイル数がコピー時間に影響する (1 vs 6160)&lt;/li&gt;
&lt;li&gt;そもそもアプリがBlobとして使うのか、ファイルシステムとして使うかにもよるが&amp;hellip;&lt;/li&gt;
&lt;li&gt;もしファイルシステムとして、であれば有効な手段&lt;/li&gt;
&lt;li&gt;エクスポートのひと手間は考慮&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Azureバックボーンを使うと、意外にブラジル近い&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;土管か(ない&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Azureバックボーンの帯域にはSLAがありませんが、意識して仕組みを作ると得をします。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azureユーザー視点のLatency測定 2017/4版</title>
      <link>https://ToruMakabe.github.io/post/azure_latency/</link>
      <pubDate>Sun, 09 Apr 2017 15:15:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_latency/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;関東の片隅で遅延を測る&#34;&gt;関東の片隅で遅延を測る&lt;/h2&gt;

&lt;p&gt;Twitterで「東阪の遅延って最近どのくらい？」と話題になっていたので。首都圏のAzureユーザー視線で測定しようと思います。&lt;/p&gt;

&lt;p&gt;せっかくなので、&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;太平洋のそれも測定しましょう&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/blog/how-microsoft-builds-its-fast-and-reliable-global-network/&#34;&gt;Azureバックボーンを通るリージョン間通信&lt;/a&gt;も測りましょう&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;計測パターン&#34;&gt;計測パターン&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;自宅(神奈川) -&amp;gt; OCN光 -&amp;gt; インターネット -&amp;gt; Azure東日本リージョン&lt;/li&gt;
&lt;li&gt;自宅(神奈川) -&amp;gt; OCN光 -&amp;gt; インターネット -&amp;gt; Azure西日本リージョン&lt;/li&gt;
&lt;li&gt;自宅(神奈川) -&amp;gt; OCN光 -&amp;gt; インターネット -&amp;gt; Azure米国西海岸リージョン&lt;/li&gt;
&lt;li&gt;Azure東日本リージョン -&amp;gt; Azureバックボーン -&amp;gt; Azure西日本リージョン&lt;/li&gt;
&lt;li&gt;Azure東日本リージョン -&amp;gt; Azureバックボーン -&amp;gt; Azure米国西海岸リージョン&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;もろもろの条件&#34;&gt;もろもろの条件&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;遅延測定ツール

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://technet.microsoft.com/en-us/sysinternals/psping.aspx&#34;&gt;PsPing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Azure各リージョンにD1_v2/Windows Server 2016仮想マシンを作成しPsPing&lt;/li&gt;
&lt;li&gt;NSGでデフォルト許可されているRDPポートへのPsPing&lt;/li&gt;
&lt;li&gt;VPN接続せず、パブリックIPへPsPing&lt;/li&gt;
&lt;li&gt;リージョン間PsPingは仮想マシンから仮想マシンへ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;自宅Wi-Fi環境

&lt;ul&gt;
&lt;li&gt;802.11ac(5GHz)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;自宅加入インターネット接続サービス

&lt;ul&gt;
&lt;li&gt;OCN 光 マンション 100M&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;OCNゲートウェイ

&lt;ul&gt;
&lt;li&gt;(ほげほげ)hodogaya.kanagawa.ocn.ne.jp&lt;/li&gt;
&lt;li&gt;神奈川県横浜市保土ケ谷区の局舎からインターネットに出ているようです&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;米国リージョン

&lt;ul&gt;
&lt;li&gt;US WEST (カリフォルニア)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;測定結果&#34;&gt;測定結果&lt;/h2&gt;

&lt;h3 id=&#34;1-自宅-神奈川-ocn光-インターネット-azure東日本リージョン&#34;&gt;1. 自宅(神奈川) -&amp;gt; OCN光 -&amp;gt; インターネット -&amp;gt; Azure東日本リージョン&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;TCP connect statistics for 104.41.187.55:3389:
  Sent = 4, Received = 4, Lost = 0 (0% loss),
  Minimum = 11.43ms, Maximum = 15.66ms, Average = 12.88ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-自宅-神奈川-ocn光-インターネット-azure西日本リージョン&#34;&gt;2. 自宅(神奈川) -&amp;gt; OCN光 -&amp;gt; インターネット -&amp;gt; Azure西日本リージョン&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;TCP connect statistics for 52.175.148.28:3389:
  Sent = 4, Received = 4, Lost = 0 (0% loss),
  Minimum = 17.96ms, Maximum = 19.64ms, Average = 18.92ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-自宅-神奈川-ocn光-インターネット-azure米国西海岸リージョン&#34;&gt;3. 自宅(神奈川) -&amp;gt; OCN光 -&amp;gt; インターネット -&amp;gt; Azure米国西海岸リージョン&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;TCP connect statistics for 40.83.220.19:3389:
  Sent = 4, Received = 4, Lost = 0 (0% loss),
  Minimum = 137.73ms, Maximum = 422.56ms, Average = 218.85ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-azure東日本リージョン-azureバックボーン-azure西日本リージョン&#34;&gt;4. Azure東日本リージョン -&amp;gt; Azureバックボーン -&amp;gt; Azure西日本リージョン&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;TCP connect statistics for 52.175.148.28:3389:
  Sent = 4, Received = 4, Lost = 0 (0% loss),
  Minimum = 8.61ms, Maximum = 9.38ms, Average = 9.00ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-azure東日本リージョン-azureバックボーン-azure米国西海岸リージョン&#34;&gt;5. Azure東日本リージョン -&amp;gt; Azureバックボーン -&amp;gt; Azure米国西海岸リージョン&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;TCP connect statistics for 40.83.220.19:3389:
  Sent = 4, Received = 4, Lost = 0 (0% loss),
  Minimum = 106.38ms, Maximum = 107.38ms, Average = 106.65ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Azureバックボーンを通すと首都圏からの遅延が半分になりました。Wi-Fiの有無など、ちょっと条件は違いますが。&lt;/p&gt;

&lt;h2 id=&#34;ひとこと&#34;&gt;ひとこと&lt;/h2&gt;

&lt;p&gt;インターネット、および接続サービスの遅延が性能の上がらない原因になっている場合は、Azureで完結させてみるのも手です。&lt;/p&gt;

&lt;p&gt;たとえば、&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;会社で契約しているインターネット接続サービスが、貧弱&lt;/li&gt;
&lt;li&gt;シリコンバレーの研究所からインターネット経由でデータを取得しているが、遅い&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;こんなケースではAzureを間に入れると、幸せになれるかもしれません。なったユーザーもいらっしゃいます。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azure Resource Manager テンプレートでManaged Diskを作るときのコツ</title>
      <link>https://ToruMakabe.github.io/post/arm_template_managed_disk/</link>
      <pubDate>Thu, 23 Mar 2017 15:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/arm_template_managed_disk/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;お伝えしたいこと&#34;&gt;お伝えしたいこと&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;ARMテンプレートのドキュメントが使いやすくなった&lt;/li&gt;
&lt;li&gt;Visual Studio CodeとAzure Resource Manager Toolsを使おう&lt;/li&gt;
&lt;li&gt;ARMテンプレートでManaged Diskを作る時のコツ&lt;/li&gt;
&lt;li&gt;可用性セットを意識しよう&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;armテンプレートのドキュメントが使いやすくなった&#34;&gt;ARMテンプレートのドキュメントが使いやすくなった&lt;/h2&gt;

&lt;p&gt;docs.microsoft.com の整備にともない、ARMテンプレートのドキュメントも&lt;a href=&#34;https://azure.microsoft.com/ja-jp/blog/azure-resource-manager-template-reference-now-available/&#34;&gt;使いやすくなりました&lt;/a&gt;。ARMテンプレート使いのみなさまは &lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/templates/&#34;&gt;https://docs.microsoft.com/ja-jp/azure/templates/&lt;/a&gt; をブックマークして、サクサク調べちゃってください。&lt;/p&gt;

&lt;h2 id=&#34;visual-studio-codeとazure-resource-manager-toolsを使おう&#34;&gt;Visual Studio CodeとAzure Resource Manager Toolsを使おう&lt;/h2&gt;

&lt;p&gt;これがあまり知られてないようなのでアピールしておきます。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docs.microsoft.com/ja-jp/azure/azure-resource-manager/media/resource-manager-create-first-template/vs-code-show-values.png&#34; alt=&#34;コードアシスト&#34; title=&#34;コードアシスト&#34; /&gt;&lt;/p&gt;

&lt;p&gt;コードアシストしてくれます。&lt;/p&gt;

&lt;p&gt;画面スクロールが必要なほどのJSONをフリーハンドで書けるほど人類は進化していないというのがわたしの見解です。ぜひご活用ください。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/azure-resource-manager/resource-manager-create-first-template?toc=%2fazure%2ftemplates%2ftoc.json&amp;amp;bc=%2Fazure%2Ftemplates%2Fbreadcrumb%2Ftoc.json#get-vs-code-and-extension&#34;&gt;Get VS Code and extension&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;armテンプレートでmanaged-diskを作る時のコツ&#34;&gt;ARMテンプレートでManaged Diskを作る時のコツ&lt;/h2&gt;

&lt;p&gt;Managed Diskが使えるようになって、ARMテンプレートでもストレージアカウントの定義を省略できるようになりました。Managed Diskの実体は内部的にAzureが管理するストレージアカウントに置かれるのですが、ユーザーからは隠蔽されます。&lt;/p&gt;

&lt;p&gt;Managed Diskは &lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/templates/microsoft.compute/disks&#34;&gt;Microsoft.Compute/disks&lt;/a&gt;  で個別に定義できますが、省略もできます。&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/templates/microsoft.compute/virtualmachines&#34;&gt;Microsoft.Compute/virtualMachines&lt;/a&gt; の中に書いてしまうやり口です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;osDisk&amp;quot;: {
  &amp;quot;name&amp;quot;: &amp;quot;[concat(variables(&#39;vmName&#39;),&#39;-md-os&#39;)]&amp;quot;,
  &amp;quot;createOption&amp;quot;: &amp;quot;FromImage&amp;quot;,
  &amp;quot;managedDisk&amp;quot;: {
    &amp;quot;storageAccountType&amp;quot;: &amp;quot;Standard_LRS&amp;quot;
  },
  &amp;quot;diskSizeGB&amp;quot;: 128
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;こんな感じで書けます。ポイントはサイズ指定 &amp;ldquo;diskSizeGB&amp;rdquo; の位置です。&amp;rdquo;managedDisk&amp;rdquo;の下ではありません。おじさんちょっと悩みました。&lt;/p&gt;

&lt;h2 id=&#34;可用性セットを意識しよう&#34;&gt;可用性セットを意識しよう&lt;/h2&gt;

&lt;p&gt;Managed Diskを使う利点のひとつが、可用性セットを意識したディスク配置です。可用性セットに仮想マシンを配置し、かつManaged Diskを使うと、可用性を高めることができます。&lt;/p&gt;

&lt;p&gt;Azureのストレージサービスは、多数のサーバーで構成された分散ストレージで実現されています。そのサーバー群をStorage Unitと呼びます。StampとかClusterと表現されることもあります。Storage Unitは数十のサーバーラック、数百サーバーで構成され、Azureの各リージョンに複数配置されます。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://download.microsoft.com/download/C/0/2/C02C4D26-0472-4688-AC13-199EA321135E/23rdACM_SOSP_WindowsAzureStorage_201110_jpn.pdf&#34;&gt;参考情報:Windows Azure ストレージ: 高可用性と強い一貫性を両立する クラウド ストレージ サービス(PDF)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;可用性セットは、電源とネットワークを共有するグループである&amp;rdquo;障害ドメイン(FD: Fault Domain)&amp;ldquo;を意識して仮想マシンを分散配置する設定です。そして、可用性セットに配置した仮想マシンに割り当てたManaged Diskは、Storage Unitを分散するように配置されます。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://msdnshared.blob.core.windows.net/media/2017/03/92.jpg&#34; alt=&#34;Unmanaged vs Managed&#34; title=&#34;Unmanaged vs Managed&#34; /&gt;&lt;/p&gt;

&lt;p&gt;すなわち、Storage Unitの障害に耐えることができます。Storage Unitは非常に可用性高く設計されており、長期に運用されてきた実績もあるのですが、ダウンする可能性はゼロではありません。可用性セットとManaged Diskの組み合わせは、可用性を追求したいシステムでは、おすすめです。&lt;/p&gt;

&lt;p&gt;さて、この場合の可用性セット定義ですが、以下のように書きます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;type&amp;quot;: &amp;quot;Microsoft.Compute/availabilitySets&amp;quot;,
  &amp;quot;name&amp;quot;: &amp;quot;AvSet01&amp;quot;,
  &amp;quot;apiVersion&amp;quot;: &amp;quot;2016-04-30-preview&amp;quot;,
  &amp;quot;location&amp;quot;: &amp;quot;[resourceGroup().location]&amp;quot;,
  &amp;quot;properties&amp;quot;: {
    &amp;quot;managed&amp;quot;: true,
    &amp;quot;platformFaultDomainCount&amp;quot;: 2,
    &amp;quot;platformUpdateDomainCount&amp;quot;: 5
  }
},
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/templates/microsoft.compute/availabilitysets&#34;&gt;Microsoft.Compute/availabilitySets&lt;/a&gt; を読むと、Managed Diskを使う場合は&amp;rdquo;propaties&amp;rdquo;の&amp;rdquo;managed&amp;rdquo;をtrueにすべし、とあります。なるほど。&lt;/p&gt;

&lt;p&gt;そしてポイントです。合わせて&amp;rdquo;platformFaultDomainCount&amp;rdquo;を指定してください。managedにする場合は必須パラメータです。&lt;/p&gt;

&lt;p&gt;なお、リージョンによって配備されているStorage Unit数には違いがあるのでご注意を。例えば東日本リージョンは2です。3のリージョンもあります。それに合わせて可用性セットの障害ドメイン数を指定します。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/storage/storage-faq-for-disks&#34;&gt;Azure IaaS VM ディスクと Premium 管理ディスクおよび非管理ディスクについてよく寄せられる質問&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Managed Disks を使用する可用性セットでサポートされる障害ドメイン数はいくつですか?

Managed Disks を使用する可用性セットでサポートされる障害ドメイン数は 2 または 3 です。これは、配置されているリージョンによって異なります。
&lt;/code&gt;&lt;/pre&gt;
</content>
      
    </item>
    
    <item>
      <title>Docker for WindowsでインストールレスAzure CLI 2.0環境を作る</title>
      <link>https://ToruMakabe.github.io/post/dockerforwin_azurecli2/</link>
      <pubDate>Tue, 28 Feb 2017 08:00:30 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/dockerforwin_azurecli2/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;azure-cli-2-0版です&#34;&gt;Azure CLI 2.0版です&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://torumakabe.github.io/post/dockerforwin_azurecli/&#34;&gt;Docker for WindowsでインストールレスAzure CLI環境を作る&lt;/a&gt;、のAzure CLI 2.0版です。Azure CLI 2.0の一般提供開始に合わせて書いています。&lt;/p&gt;

&lt;h2 id=&#34;動機&#34;&gt;動機&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Docker for Windows、もっと活用しようぜ&lt;/li&gt;
&lt;li&gt;がんがんアップデートされるAzure CLI2.0をいちいちインストールしたくない、コンテナ引っ張って以上、にしたい&lt;/li&gt;
&lt;li&gt;開発端末の環境を汚したくない、いつでもきれいに作り直せるようにしたい&lt;/li&gt;
&lt;li&gt;WindowsでPythonのバージョン管理するのつらくないですか? コンテナで解決しましょう&lt;/li&gt;
&lt;li&gt;○○レスって言ってみたかった&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;やり口&#34;&gt;やり口&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;もちろんDocker for Windows (on Client Hyper-V) を使う&lt;/li&gt;
&lt;li&gt;いちいちdocker run&amp;hellip;と打たなくていいよう、エイリアス的にPowerShellのfunction &amp;ldquo;az_cli&amp;rdquo; を作る&lt;/li&gt;
&lt;li&gt;&amp;ldquo;az_cli&amp;rdquo;入力にてAzure CLIコンテナを起動&lt;/li&gt;
&lt;li&gt;コンテナとホスト(Windows)間でファイル共有、ホスト側のIDEなりエディタを使えるようにする&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;作業の中身&#34;&gt;作業の中身&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Docker for Windowsを&lt;a href=&#34;https://docs.docker.com/docker-for-windows/install/&#34;&gt;インストール&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;64bit Windows 10 Pro/Enterprise/Education 1511以降に対応&lt;/li&gt;
&lt;li&gt;Hyper-Vの有効化を忘れずに&lt;/li&gt;
&lt;li&gt;Hyper-VとぶつかるVirtualBoxとはお別れです&lt;/li&gt;
&lt;li&gt;モードをLinuxにします。タスクトレイのdockerアイコンを右クリック [Switch to Linux containers]&lt;/li&gt;
&lt;li&gt;ドライブ共有をお忘れなく。 タスクトレイのdockerアイコンを右クリック [settings] &amp;gt; [Shared Drives]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;PowerShell functionを作成

&lt;ul&gt;
&lt;li&gt;のちほど詳しく&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;powershellのfunctionを作る&#34;&gt;PowerShellのfunctionを作る&lt;/h2&gt;

&lt;p&gt;ここが作業のハイライト。&lt;/p&gt;

&lt;p&gt;PowerShellのプロファイルを編集します。ところでエディタはなんでもいいのですが、AzureやDockerをがっつり触る人にはVS Codeがおすすめです。&lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=msazurermtools.azurerm-vscode-tools&#34;&gt;Azure Resource Manager Template&lt;/a&gt;や&lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=PeterJausovec.vscode-docker&#34;&gt;Docker&lt;/a&gt;むけextensionがあります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Workspace\ARM&amp;gt; code $profile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;こんなfunctionを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function az_cli {
   C:\PROGRA~1\Docker\Docker\Resources\bin\docker.exe run -it --rm -v ${HOME}/.azure:/root/.azure -v ${PWD}:/data -w /data azuresdk/azure-cli-python
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;エイリアスでなくfunctionにした理由は、引数です。エイリアスだと引数を渡せないので&lt;/li&gt;
&lt;li&gt;コンテナが溜まるのがいやなので、&amp;ndash;rmで都度消します&lt;/li&gt;
&lt;li&gt;毎度 az login しなくていいよう、トークンが保管されるコンテナの/root/azureディレクトリをホストの${HOME}/.azureと-v オプションで共有します&lt;/li&gt;
&lt;li&gt;ARM TemplateのJSONファイルなど、ホストからファイルを渡したいため、カレントディレクトリ ${PWD} をコンテナと -v オプションで共有します&lt;/li&gt;
&lt;li&gt;コンテナはdocker hubのazuresdk/azure-cli-pythonリポジトリ、latestを引っ張ります。latestで不具合あればバージョン指定してください&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ではテスト。まずはホスト側のファイルを確認。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Workspace\ARM&amp;gt; ls


    ディレクトリ: C:\Workspace\ARM


Mode                LastWriteTime         Length Name
----                -------------         ------ ----
-a----       2017/02/28      8:29           4515 azuredeploy.json
-a----       2017/02/28      8:30            374 azuredeploy.parameters.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;いくつかのファイルがあります。&lt;/p&gt;

&lt;p&gt;コンテナを起動してみましょう。az_cli functionを呼びます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Workspace\ARM&amp;gt; az_cli
bash-4.3#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;コンテナを起動し、入出力をつなぎました。ここからは頭と手をLinuxに切り替えてください。Azure CLI 2.0コンテナは&lt;a href=&#34;https://hub.docker.com/r/azuresdk/azure-cli-python/~/dockerfile/&#34;&gt;alpine linux&lt;/a&gt;ベースです。&lt;/p&gt;

&lt;p&gt;カレントディレクトリを確認。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bash-4.3# pwd
/data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ファイル共有できているか確認。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bash-4.3# ls
azuredeploy.json             azuredeploy.parameters.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;できてますね。&lt;/p&gt;

&lt;p&gt;azコマンドが打てるか確認。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bash-4.3# az --version
azure-cli (2.0.0+dev)

acr (0.1.1b4+dev)
acs (2.0.0+dev)
appservice (0.1.1b5+dev)
batch (0.1.1b4+dev)
cloud (2.0.0+dev)
component (2.0.0+dev)
configure (2.0.0+dev)
container (0.1.1b4+dev)
core (2.0.0+dev)
documentdb (0.1.1b2+dev)
feedback (2.0.0+dev)
iot (0.1.1b3+dev)
keyvault (0.1.1b5+dev)
network (2.0.0+dev)
nspkg (2.0.0+dev)
profile (2.0.0+dev)
redis (0.1.1b3+dev)
resource (2.0.0+dev)
role (2.0.0+dev)
sql (0.1.1b5+dev)
storage (2.0.0+dev)
taskhelp (0.1.1b3+dev)
vm (2.0.0+dev)

Python (Linux) 3.5.2 (default, Dec 27 2016, 21:33:11)
[GCC 5.3.0]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;タブで補完も効きます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bash-4.3# az a
account     acr         acs         ad          appservice
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;しあわせ。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azure N-SeriesでPaintsChainerを動かす</title>
      <link>https://ToruMakabe.github.io/post/paintschainer_on_azure/</link>
      <pubDate>Fri, 03 Feb 2017 18:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/paintschainer_on_azure/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;paintschainer面白い&#34;&gt;PaintsChainer面白い&lt;/h2&gt;

&lt;p&gt;クラスメソッドさんのDevelopers.IOでのエントリ&lt;a href=&#34;http://dev.classmethod.jp/cloud/paintschainer-on-ec2/&#34;&gt;&amp;ldquo;PaintsChainerをAmazon EC2で動かしてみた&amp;rdquo;&lt;/a&gt;が、とても面白いです。&lt;/p&gt;

&lt;p&gt;畳みこみニューラルネットワークを駆使して白黒線画に色付けしちゃうPaintsChainerすごい。EC2のGPUインスタンスでさくっと動かせるのもいいですね。&lt;/p&gt;

&lt;p&gt;せっかくなのでAzureでもやってみようと思います。AzurerはN-Series &amp;amp; NVIDIA-Dockerのサンプルとして、Azurerでない人はUbuntuでPaintsChainerを動かす参考手順として見ていただいてもいいかと。&lt;/p&gt;

&lt;h2 id=&#34;試した環境&#34;&gt;試した環境&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;米国中南部リージョン&lt;/li&gt;
&lt;li&gt;Standard NC6 (6 コア、56 GB メモリ、NVIDIA Tesla K80)&lt;/li&gt;
&lt;li&gt;Ubuntu 16.04&lt;/li&gt;
&lt;li&gt;NSGはSSH(22)の他にHTTP(80)を受信許可&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;導入手順&#34;&gt;導入手順&lt;/h2&gt;

&lt;h3 id=&#34;nvidia-tesla-driversのインストール&#34;&gt;NVIDIA Tesla driversのインストール&lt;/h3&gt;

&lt;p&gt;マイクロソフト公式ドキュメントの通りに導入します。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/virtual-machines/virtual-machines-linux-n-series-driver-setup&#34;&gt;Set up GPU drivers for N-series VMs&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;dockerのインストール&#34;&gt;Dockerのインストール&lt;/h3&gt;

&lt;p&gt;Docker公式ドキュメントの通りに導入します。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/engine/installation/linux/ubuntu/&#34;&gt;Get Docker for Ubuntu&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;nvidia-dockerのインストール&#34;&gt;NVIDIA Dockerのインストール&lt;/h3&gt;

&lt;p&gt;GitHub上のNVIDIAのドキュメント通りに導入します。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/NVIDIA/nvidia-docker&#34;&gt;NVIDIA Docker&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ここまでの作業に問題がないか、確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo nvidia-docker run --rm nvidia/cuda nvidia-smi
Using default tag: latest
latest: Pulling from nvidia/cuda
8aec416115fd: Pull complete
[...]
Status: Downloaded newer image for nvidia/cuda:latest
Fri Feb  3 06:43:18 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 86BF:00:00.0     Off |                    0 |
| N/A   34C    P8    33W / 149W |      0MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;paintschainer-dockerのインストール&#34;&gt;PaintsChainer-Dockerのインストール&lt;/h3&gt;

&lt;p&gt;Liam Jones氏が公開している&lt;a href=&#34;https://github.com/liamjones/PaintsChainer-Docker&#34;&gt;PaintsChainer-Docker&lt;/a&gt;を使って、PaintsChanierコンテナーを起動します。ポートマッピングはコンテナーホストの80番とコンテナーの8000番です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo nvidia-docker run -p 80:8000 liamjones/paintschainer-docker
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;paintschainerを使ってみる&#34;&gt;PaintsChainerを使ってみる&lt;/h2&gt;

&lt;p&gt;VMのパブリックIP、ポート80番にアクセスすると、先ほどコンテナーで起動したPaintsChainerのページが開きます。クラウディアさんの白黒画像ファイルで試してみましょう。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ToruMakabe/Images/master/paintschainer_cloudia.png&#34; alt=&#34;結果&#34; title=&#34;Cloudia&#34; /&gt;&lt;/p&gt;

&lt;p&gt;PaintsChainer、すごいなぁ。
クラウディアさん、おなか寒そうだけど。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azure App Service on LinuxのコンテナをCLIで更新する方法</title>
      <link>https://ToruMakabe.github.io/post/azure_webapponlinux_dockertag/</link>
      <pubDate>Sun, 20 Nov 2016 13:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_webapponlinux_dockertag/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;cliでコンテナを更新したい&#34;&gt;CLIでコンテナを更新したい&lt;/h2&gt;

&lt;p&gt;Connect(); 2016にあわせ、Azure App Service on Linuxのコンテナ対応が&lt;a href=&#34;https://azure.microsoft.com/en-us/blog/app-service-on-linux-now-supports-containers-and-asp-net-core/&#34;&gt;発表&lt;/a&gt;されました。Azure Container Serviceほどタップリマシマシな環境ではなく、サクッと楽してコンテナを使いたい人にオススメです。&lt;/p&gt;

&lt;p&gt;さっそくデプロイの自動化どうすっかな、と検討している人もちらほらいらっしゃるようです。CI/CD側でビルド、テストしたコンテナをAPIなりCLIでApp Serviceにデプロイするやり口、どうしましょうか。&lt;/p&gt;

&lt;p&gt;まだプレビューなのでAzureも、VSTSなどCI/CD側も機能追加が今後あると思いますし、使い方がこなれてベストプラクティスが生まれるとは思いますが、アーリーアダプターなあなた向けに、現時点でできることを書いておきます。&lt;/p&gt;

&lt;h2 id=&#34;azure-cli-2-0&#34;&gt;Azure CLI 2.0&lt;/h2&gt;

&lt;p&gt;Azure CLI 2.0に&amp;rdquo;appservice web config container&amp;rdquo;コマンドがあります。これでコンテナイメージを更新できます。&lt;/p&gt;

&lt;p&gt;すでにyourrepoレポジトリのyourcontainerコンテナ、タグ1.0.0がデプロイされているとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az appservice web config container show -n yourcontainerapp -g YourRG
{
  &amp;quot;DOCKER_CUSTOM_IMAGE_NAME&amp;quot;: &amp;quot;yourrepo/yourcontainer:1.0.0&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;新ビルドのタグ1.0.1をデプロイするには、update -c オプションを使います。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az appservice web config container update -n yourcontainerapp -g YourRG -c &amp;quot;yourrepo/yourcontainer:1.0.1&amp;quot;
{
  &amp;quot;DOCKER_CUSTOM_IMAGE_NAME&amp;quot;: &amp;quot;yourrepo/yourcontainer:1.0.1&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これで更新されます。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>SlackとAzure FunctionsでChatOpsする</title>
      <link>https://ToruMakabe.github.io/post/azure_chatops_onfunctions/</link>
      <pubDate>Fri, 07 Oct 2016 17:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_chatops_onfunctions/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;azure-functionsでやってみよう&#34;&gt;Azure Functionsでやってみよう&lt;/h2&gt;

&lt;p&gt;Azure上でChatOpsしたい、と相談をいただきました。&lt;/p&gt;

&lt;p&gt;AzureでChatOpsと言えば、Auth0のSandrino Di Mattia氏が作った素敵な&lt;a href=&#34;http://fabriccontroller.net/chatops-deploy-and-manage-complete-environments-on-azure-using-slack/&#34;&gt;サンプル&lt;/a&gt;があります。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://fabriccontroller.net/static/chatops-how-this-works.png.pagespeed.ce.lN444drUKd.png&#34; alt=&#34;Azure Runスラッシュ&#34; title=&#34;from fabriccontroller.net&#34; /&gt;&lt;/p&gt;

&lt;p&gt;素晴らしい。これで十分、という気もしますが、実装のバリエーションがあったほうが後々参考になる人も多いかなと思い、Web App/Web JobをAzure Functionsで置き換えてみました。&lt;/p&gt;

&lt;h2 id=&#34;slackからrunbookを実行できて-何がうれしいか&#34;&gt;SlackからRunbookを実行できて、何がうれしいか&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;誰がいつ、どんな文脈でRunbookを実行したかを可視化する&lt;/li&gt;
&lt;li&gt;CLIやAPIをRunbookで隠蔽し、おぼえることを減らす&lt;/li&gt;
&lt;li&gt;CLIやAPIをRunbookで隠蔽し、できることを制限する&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;ブツ&#34;&gt;ブツ&lt;/h2&gt;

&lt;p&gt;Githubに上げておきました。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/ToruMakabe/AZChatOpsSample&#34;&gt;AZChatOpsSample&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;おおまかな流れ&#34;&gt;おおまかな流れ&lt;/h2&gt;

&lt;p&gt;手順書つらいのでポイントだけ。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;SlackのSlash CommandとIncoming Webhookを作る&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;流れは氏の&lt;a href=&#34;http://fabriccontroller.net/chatops-deploy-and-manage-complete-environments-on-azure-using-slack/&#34;&gt;元ネタ&lt;/a&gt;と同じ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ARM TemplateでFunction Appをデプロイ&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Github上のDeployボタンからでもいいですが、パラメータファイルを作っておけばCLIで楽に繰り返せます&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;パラメータファイルのサンプルは&lt;a href=&#34;https://github.com/ToruMakabe/AZChatOpsSample/blob/master/sample.azuredeploy.parameters.json&#34;&gt;sample.azuredeploy.parameters.json&lt;/a&gt;です。GUIでデプロイするにしても、パラメータの意味を理解するためにざっと読むと幸せになれると思います&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Function AppのデプロイはGithubからのCIです。クローンしたリポジトリとブランチを指定してください&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;GithubからのCIは、&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/app-service-deploy-complex-application-predictably/&#34;&gt;はじめてのケースを考慮し&lt;/a&gt;ARM Templateのリソースプロパティ&amp;rdquo;IsManualIntegration&amp;rdquo;をtrueにしています&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Azure Automationのジョブ実行権限を持つサービスプリンシパルが必要です (パラメータ SUBSCRIPTION_ID、TENANT_ID、CLIENT_ID、CLIENT_SECRET で指定)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Azure Automationについて詳しく説明しませんが、Slackから呼び出すRunbookを準備しておいてください。そのAutomationアカウントと所属するリソースグループを指定します&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;作成済みのSlack関連パラメータを指定します&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ARM Templateデプロイ後にkuduのデプロイメントスクリプトが走るので、しばし待つ(Function Appの設定-&amp;gt;継続的インテグレーションの構成から進捗が見えます)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;デプロイ後、Slash Commandで呼び出すhttptrigger function(postJob)のtokenを変更&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kuduでdata/Functions/secrets/postJob.jsonの値を、Slackが生成したSlash Commandのtokenに書き換え&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Slack上で、Slash Commandのリクエスト先URLを変更 (例: &lt;a href=&#34;https://yourchatops.azurewebsites.net/api/postJob?code=TokenTokenToken&#34;&gt;https://yourchatops.azurewebsites.net/api/postJob?code=TokenTokenToken&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ファンクションが動いたら、Slackの指定チャンネルでSlash Commandが打てるようになる&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;/runbook [runbook名] [parm1] [parm2] [parm&amp;hellip;]&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;パラメータはrunbook次第&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Runbookの進捗はIncoming Webhookでslackに通知される&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Runbookのステータスが変わったときに通知&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;よもやま話&#34;&gt;よもやま話&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;SlackのSlash Commandは、3秒以内に返事を返さないとタイムアウトします。なのでいくつか工夫しています。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ファンクションはトリガーされるまで寝ています。また、5分間動きがないとこれまた寝ます(cold状態になる)。寝た子を起こすのには時間がかかるので、Slackの3秒ルールに間に合わない可能性があります。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Azure FunctionsのWebコンソールログが無活動だと30分で停止するので、coldに入る条件も30分と誤解していたのですが、正しくは5分。ソースは&lt;a href=&#34;https://github.com/Azure/azure-webjobs-sdk-script/issues/529&#34;&gt;ここ&lt;/a&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;そこで、4分周期でTimer Triggerし、postJobにダミーPOSTするpingFuncを作りました。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ファンクションのコードに更新があった場合、リロード処理が走ります。リロード後、またしてもトリガーを待って寝てしまうので、コード変更直後にSlash Commandを打つとタイムアウトする可能性大です。あせらずpingまで待ちましょう。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;でもAzure AutomationのAPI応答待ちなど、外部要因で3秒超えちゃう可能性はあります。非同期にしてひとまずSlackに応答返す作りに変えたほうがいいですね。これはSlackのSlash Commandに限らず、呼び出し元に待ってもらえないケース全てに言える考慮点です。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Azure Functionsはまだプレビューなので、&lt;a href=&#34;https://github.com/Azure/azure-webjobs-sdk-script/issues/529&#34;&gt;議論されているとおり&lt;/a&gt;改善の余地が多くあります。期待しましょう。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
      
    </item>
    
    <item>
      <title>Bash on WindowsでNode開発環境を作る</title>
      <link>https://ToruMakabe.github.io/post/bashonwindows_nvm/</link>
      <pubDate>Wed, 14 Sep 2016 15:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/bashonwindows_nvm/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;bash-on-windows-現時点での使いどころ&#34;&gt;Bash on Windows 現時点での使いどころ&lt;/h2&gt;

&lt;p&gt;Windows 10 Anniversary Updateでベータ提供がはじまったBash on Ubuntu on Windows、みなさん使ってますか。わたしは、まだベータなので本気運用ではないのですが、開発ツールを動かすのに使い始めてます。Linux/Macと同じツールが使えるってのは便利です。&lt;/p&gt;

&lt;p&gt;たとえばNodeのバージョン管理。Windowsには&lt;a href=&#34;https://github.com/marcelklehr/nodist&#34;&gt;nodist&lt;/a&gt;がありますが、Linux/Macでは動きません。Linux/Macで使ってる&lt;a href=&#34;https://github.com/creationix/nvm&#34;&gt;NVM&lt;/a&gt;がWindowsで動いたら、いくつもバージョン管理ツールを覚えずに済むのに！あ、Bash on Windowsあるよ！！おお、そうだな！！！という話です。&lt;/p&gt;

&lt;p&gt;最近、Azure FunctionsでNode v6.4.0が&lt;a href=&#34;https://blogs.msdn.microsoft.com/appserviceteam/2016/09/01/azure-functions-0-5-release-august-portal-update/&#34;&gt;使えるようになった&lt;/a&gt;ので、「これからバージョン管理どうすっかな」と考えていた人も多いのでは。それはわたしです。&lt;/p&gt;

&lt;h2 id=&#34;nvmのインストール&#34;&gt;NVMのインストール&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Bash on Ubuntu on Windowsを入れます (&lt;a href=&#34;http://www.atmarkit.co.jp/ait/articles/1608/08/news039.html&#34;&gt;参考&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Bash on Ubuntu on Windowsを起動します&lt;/li&gt;
&lt;li&gt;build-essentialとlibssl-devを入れます&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install build-essential checkinstall
sudo apt-get install libssl-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;インストールスクリプトを流します&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.31.7/install.sh | bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;バージョンアップを考慮し、インストールのつど&lt;a href=&#34;https://github.com/creationix/nvm&#34;&gt;公式ページ&lt;/a&gt;を確認してください。&lt;/p&gt;

&lt;p&gt;以上。&lt;/p&gt;

&lt;h2 id=&#34;nvmの使い方&#34;&gt;NVMの使い方&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;nvm install 6.4.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;指定のバージョンをインストールします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nvm use 6.4.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使うバージョンを指定します。&lt;/p&gt;

&lt;p&gt;簡単ですね。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd /mnt/c/your_work_directory
node ./index.js
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;なんて感じで、書いたコードをテストしちゃってください。&lt;/p&gt;

&lt;p&gt;なお、Visual Studio Code使いの人は&lt;a href=&#34;https://blogs.msdn.microsoft.com/ayatokura/2016/08/06/vsc_windows_bash/&#34;&gt;統合ターミナルをBashにしておく&lt;/a&gt;と、さらに幸せになれます。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azure Functionsで運用管理サーバレス生活(使用量データ取得編)</title>
      <link>https://ToruMakabe.github.io/post/azurefunctions_getusagedata/</link>
      <pubDate>Tue, 13 Sep 2016 17:30:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azurefunctions_getusagedata/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;背景と動機&#34;&gt;背景と動機&lt;/h2&gt;

&lt;p&gt;Azure Functions使ってますか。「サーバレス」という、ネーミングに突っ込みたい衝動を抑えられないカテゴリに属するため損をしている気もしますが、システムのつくり方を変える可能性がある、潜在能力高めなヤツです。キャッチアップして損はないです。&lt;/p&gt;

&lt;p&gt;さて、Azure Functionsを使ってAzureの使用量データを取得、蓄積したいというリクエストを最近いくつかいただきました。いい機会なのでまとめておきます。以下、その背景。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;運用管理業務がビジネスの差別化要素であるユーザは少ない。可能な限り省力化したい。運用管理ソフトの導入維持はもちろん、その土台になるサーバの導入、維持は真っ先に無くしたいオーバヘッド。もうパッチ当てとか監視システムの監視とか、やりたくない。&lt;/li&gt;
&lt;li&gt;Azure自身が持つ運用管理の機能が充実し、また、運用管理SaaS(&lt;a href=&#34;https://www.microsoft.com/ja-jp/server-cloud/products-operations-management-suite.aspx&#34;&gt;MS OMS&lt;/a&gt;、New Relic、Datadogなど)が魅力的になっており、使い始めている。いつかは運用管理サーバを無くしたい。&lt;/li&gt;
&lt;li&gt;でも、それら標準的なサービスでカバーされていない、ちょっとした機能が欲しいことがある。&lt;/li&gt;
&lt;li&gt;Azureリソースの使用量データ取得が一例。Azureでは使用量データを&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/billing-understand-your-bill/&#34;&gt;ポータルからダウンロード&lt;/a&gt;したり、&lt;a href=&#34;https://powerbi.microsoft.com/ja-jp/documentation/powerbi-content-pack-azure-enterprise/&#34;&gt;Power BIで分析&lt;/a&gt;できたりするが、元データは自分でコントロールできるようためておきたい。もちろん手作業なし、自動で。&lt;/li&gt;
&lt;li&gt;ちょっとしたコードを気軽に動かせる仕組みがあるなら、使いたい。インフラエンジニアがさくっと書くレベルで。&lt;/li&gt;
&lt;li&gt;それAzure Functionsで出来るよ。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;方針&#34;&gt;方針&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Azure FunctionsのTimer Triggerを使って、日次で実行&lt;/li&gt;
&lt;li&gt;Azure Resource Usage APIを使って使用量を取得し、ファイルに書き込み&lt;/li&gt;
&lt;li&gt;Nodeで書く (C#のサンプルはたくさんあるので)&lt;/li&gt;
&lt;li&gt;業務、チームでの運用を考慮して、ブラウザでコード書かずにソース管理ツールと繋げる (Githubを使う)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;quick-start&#34;&gt;Quick Start&lt;/h2&gt;

&lt;h3 id=&#34;準備&#34;&gt;準備&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;ところでAzure Funtionsって何よ、って人はまず&lt;a href=&#34;https://blogs.technet.microsoft.com/azure-sa-members/azurefunctions/&#34;&gt;いい資料1&lt;/a&gt;と&lt;a href=&#34;https://buchizo.wordpress.com/2016/06/04/azure-functions-overview-and-under-the-hood/&#34;&gt;いい資料2&lt;/a&gt;でざっと把握を&lt;/li&gt;
&lt;li&gt;AzureのAPIにプログラムからアクセスするため、サービスプリンシパルを作成 (&lt;a href=&#34;https://doc.co/66mYfB&#34;&gt;ここ&lt;/a&gt;とか&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/resource-group-authenticate-service-principal/&#34;&gt;ここ&lt;/a&gt;を参考に)

&lt;ul&gt;
&lt;li&gt;後ほど環境変数に設定するので、Domain(Tenant ID)、Client ID(App ID)、Client Secret(Password)、Subscription IDを控えておいてください&lt;/li&gt;
&lt;li&gt;権限はsubscriptionに対するreaderが妥当でしょう&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Githubのリポジトリを作成 (VSTSやBitbucketも使えます)&lt;/li&gt;
&lt;li&gt;使用量データを貯めるストレージアカウントを作成

&lt;ul&gt;
&lt;li&gt;後ほど環境変数に設定するので、接続文字列を控えておいてください&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;デプロイ&#34;&gt;デプロイ&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Function Appを作成

&lt;ul&gt;
&lt;li&gt;ポータル左上&amp;rdquo;+新規&amp;rdquo; -&amp;gt; Web + モバイル -&amp;gt; Function App&lt;/li&gt;
&lt;li&gt;アプリ名は.azurewebsites.net空間でユニークになるように&lt;/li&gt;
&lt;li&gt;App Seriviceプランは、占有型の&amp;rdquo;クラシック&amp;rdquo;か、共有で実行したぶん課金の&amp;rdquo;動的&amp;rdquo;かを選べます。今回の使い方だと動的がお得でしょう&lt;/li&gt;
&lt;li&gt;メモリは128MBあれば十分です&lt;/li&gt;
&lt;li&gt;他のパラメータはお好みで&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;環境変数の設定

&lt;ul&gt;
&lt;li&gt;Function Appへポータルからアクセス -&amp;gt; Function Appの設定 -&amp;gt; アプリケーション設定の構成 -&amp;gt; アプリ設定&lt;/li&gt;
&lt;li&gt;先ほど控えた環境変数を設定します(CLIENT_ID、DOMAIN、APPLICATION_SECRET、AZURE_SUBSCRIPTION_ID、azfuncpoc_STORAGE)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;サンプルコードを取得

&lt;ul&gt;
&lt;li&gt;githubに置いてますので、作業するマシンにcloneしてください -&amp;gt; &lt;a href=&#34;https://github.com/ToruMakabe/AZFuncTimerTriggerSample&#34;&gt;AZFuncTimerTriggerSample&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;準備済みのGithubリポジトリにpush&lt;/li&gt;
&lt;li&gt;リポジトリとFunction Appを同期

&lt;ul&gt;
&lt;li&gt;Function Appへポータルからアクセス -&amp;gt; Function Appの設定 -&amp;gt; 継続的インテグレーションの構成 -&amp;gt; セットアップ&lt;/li&gt;
&lt;li&gt;Githubリポジトリとブランチを設定し、同期を待ちます&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Nodeのモジュールをインストール

&lt;ul&gt;
&lt;li&gt;&lt;del&gt;Function Appへポータルからアクセス -&amp;gt; Function Appの設定 -&amp;gt; kuduに移動 -&amp;gt; site/wwwroot/getUsageData へ移動&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;このディレクトリが、実行する関数、functionの単位です&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;&amp;ldquo;npm install&amp;rdquo; を実行 (package.jsonの定義に従ってNodeのモジュールが”node_modules&amp;rdquo;へインストールされます)&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;deploy.cmd で自動的にインストールするよう変えました&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;これで、指定ストレージアカウントの&amp;rdquo;usagedata&amp;rdquo;コンテナに日次で使用量データファイルができます。&lt;/p&gt;

&lt;h2 id=&#34;コード解説&#34;&gt;コード解説&lt;/h2&gt;

&lt;p&gt;3つのファイルをデプロイしました。簡単な順に、ざっと解説します。&lt;a href=&#34;https://github.com/ToruMakabe/AZFuncTimerTriggerSample&#34;&gt;コード&lt;/a&gt;を眺めながら読み進めてください。&lt;/p&gt;

&lt;h3 id=&#34;package-json&#34;&gt;package.json&lt;/h3&gt;

&lt;p&gt;主となるコードファイルは後述の&amp;rdquo;index.js&amp;rdquo;ですが、その動作に必要な環境を定義します。依存モジュールのバージョンの違いでトラブらないよう、dependenciesで指定するところがクライマックスです。&lt;/p&gt;

&lt;h3 id=&#34;function-json&#34;&gt;function.json&lt;/h3&gt;

&lt;p&gt;Azure Functionsの特徴である、TriggerとBindingsを定義します。サンプルはTimer Triggerなので、実行タイミングをここに書きます。&amp;rdquo;schedule&amp;rdquo;属性に、cron形式({秒}{分}{時}{日}{月}{曜日})で。&lt;/p&gt;

&lt;p&gt;&amp;ldquo;0 0 0 * * *&amp;rdquo; と指定しているので、毎日0時0分0秒に起動します。UTCです。&lt;/p&gt;

&lt;h3 id=&#34;index-js&#34;&gt;index.js&lt;/h3&gt;

&lt;p&gt;メインロジックです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;先ほど設定した環境変数は、&amp;rdquo;process.env.HOGE&amp;rdquo;を通じ実行時に読み込まれます。認証関連情報はハードコードせず、このやり口で。&lt;/li&gt;
&lt;li&gt;日付関連処理はUTCの明示を徹底しています。Azure Functions実行環境はUTCですが、ローカルでのテストなど他環境を考えると、指定できるところはしておくのがおすすめです。これはクラウドでグローバル展開する可能性があるコードすべてに言えます。&lt;/li&gt;
&lt;li&gt;0時に起動しますが、使用量データ作成遅延の可能性があるので、処理対象は2日前です。お好みで調整してください。詳細仕様は&lt;a href=&#34;https://msdn.microsoft.com/en-us/library/azure/mt219001.aspx&#34;&gt;こちら&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;module.export からが主フローです。asyncを使って、Blobコンテナの作成、使用量データ取得&amp;amp;ファイル書き込みを、順次処理しています。後ほど豆知識で補足します。&lt;/li&gt;
&lt;li&gt;最後にcontext.done()でFunctionsに対してアプリの終了を伝えます。黙って終わるような行儀の悪い子は嫌いです。&lt;/li&gt;
&lt;li&gt;ヘルパー関数たちは最後にまとめてあります。ポイントはcontinuationTokenを使ったループ処理です。

&lt;ul&gt;
&lt;li&gt;Resource Usage API は、レスポンスで返すデータが多い場合に、途中で切って「次はこのトークンで続きからアクセスしてちょ」という動きをします。&lt;/li&gt;
&lt;li&gt;ループが2周目に入った場合は、データを書きだすファイルが分かれます。フォーマットは&amp;rdquo;YYYY-MM-DD_n.json&amp;rdquo;です。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;豆知識-node-on-azure-functions&#34;&gt;豆知識 (Node on Azure Functions)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;通信やI/Oの関数など、非同期処理の拾い忘れ、突き抜けに注意してください

&lt;ul&gt;
&lt;li&gt;NodeはJavascript、シングルスレッドなので時間のかかる処理でブロックしないのが基本です&lt;/li&gt;
&lt;li&gt;Azure FunctionsはNode v6.4.0が使えるのでES6のpromiseが書けるのですが、SDKがまだpromiseを&lt;a href=&#34;https://github.com/Azure/azure-sdk-for-node/issues/1450&#34;&gt;サポートしていない&lt;/a&gt;ので、サポートされるまではcallbackで堅く書きましょう&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Nodeに限った話ではないですが、Azure Functions Timer TriggerはInput/Output Bindingと組み合わせられません

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/functions-reference/#-7&#34;&gt;サポートマトリックス&lt;/a&gt;を確認しましょう&lt;/li&gt;
&lt;li&gt;なのでサンプルではOutput Binding使わずに書きました&lt;/li&gt;
&lt;li&gt;Input/Outputを使える他のTriggerでは、楽なのでぜひ活用してください&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;豆知識-azure-usage-api&#34;&gt;豆知識 (Azure Usage API)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Resource Usage APIは使用量のためのAPIなので、料金に紐づけたい場合は、&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/billing-usage-rate-card-overview/&#34;&gt;Ratecard API&lt;/a&gt;を組み合わせてください&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;それでは、幸せな運用管理サーバレス生活を。&lt;/strong&gt;&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>OMSでLinuxコンテナのログを分析する</title>
      <link>https://ToruMakabe.github.io/post/oms_container_linux/</link>
      <pubDate>Thu, 25 Aug 2016 16:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/oms_container_linux/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;oms-container-solution-for-linux-プレビュー開始&#34;&gt;OMS Container Solution for Linux プレビュー開始&lt;/h2&gt;

&lt;p&gt;OMS Container Solution for Linuxのプレビューが&lt;a href=&#34;https://blogs.technet.microsoft.com/msoms/2016/08/24/announcing-public-preview-oms-container-solution-for-linux/&#34;&gt;はじまりました&lt;/a&gt;。OMSのログ分析機能は500MB/日のログ転送まで無料で使えるので、利用者も多いのではないでしょうか。&lt;/p&gt;

&lt;p&gt;さて、このたびプレビュー開始したLinuxコンテナのログ分析機能、サクッと使えるので紹介します。まだプレビューなので、仕様が変わったらごめんなさい。&lt;/p&gt;

&lt;h2 id=&#34;何ができるか-とその特徴&#34;&gt;何ができるか、とその特徴&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Dockerコンテナに関わるログの収集と分析、ダッシュボード表示

&lt;ul&gt;
&lt;li&gt;収集データの詳細 - &lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/log-analytics-containers/#containers-data-collection-details&#34;&gt;Containers data collection details&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;導入が楽ちん

&lt;ol&gt;
&lt;li&gt;OMSエージェントコンテナを導入し、コンテナホスト上のすべてのコンテナのログ分析ができる&lt;/li&gt;
&lt;li&gt;コンテナホストに直接OMS Agentを導入することもできる&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;1がコンテナ的でいいですよね。実現イメージはこんな感じです。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://msdnshared.blob.core.windows.net/media/2016/08/3-OMS-082416.png&#34; alt=&#34;OMS Agent Installation Type&#34; title=&#34;from microsoft.com&#34; /&gt;&lt;/p&gt;

&lt;p&gt;これであれば、CoreOSのような「コンテナホストはあれこれいじらない」というポリシーのディストリビューションにも対応できます。&lt;/p&gt;

&lt;p&gt;では試しに、1のやり口でUbuntuへ導入してみましょう。&lt;/p&gt;

&lt;h2 id=&#34;手順&#34;&gt;手順&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;OMSのログ分析機能を有効化しワークスペースを作成、IDとKeyを入手 (&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/log-analytics-get-started/&#34;&gt;参考&lt;/a&gt;)

&lt;ul&gt;
&lt;li&gt;Azureのサブスクリプションを持っている場合、&amp;rdquo;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/log-analytics-get-started/#microsoft-azure&#34;&gt;Microsoft Azure を使用した迅速なサインアップ&lt;/a&gt;&amp;ldquo;から読むと、話が早いです&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;OMSポータルのソリューションギャラリーから、&amp;rdquo;Containers&amp;rdquo;を追加&lt;/li&gt;
&lt;li&gt;UbuntuにDockerを導入

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/engine/installation/linux/ubuntulinux/&#34;&gt;参考&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;現在、OMSエージェントが対応するDockerバージョンは 1.11.2までなので、たとえばUbuntu 16.04の場合は sudo apt-get install docker-engine=1.11.2-0~xenial とするなど、バージョン指定してください&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;OMSエージェントコンテナを導入

&lt;ul&gt;
&lt;li&gt;先ほど入手したOMSのワークスペースIDとKeyを入れてください&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;sudo docker run --privileged -d -v /var/run/docker.sock:/var/run/docker.sock -e WSID=&amp;quot;your workspace id&amp;quot; -e KEY=&amp;quot;your key&amp;quot; -h=`hostname` -p 127.0.0.1:25224:25224/udp -p 127.0.0.1:25225:25225 --name=&amp;quot;omsagent&amp;quot; --log-driver=none --restart=always microsoft/oms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上。これでOMSポータルからログ分析ができます。こんな感じで。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://acom.azurecomcdn.net/80C57D/cdn/mediahandler/docarticles/dpsmedia-prod/azure.microsoft.com/en-us/documentation/articles/log-analytics-containers/20160824105310/containers-dash01.png&#34; alt=&#34;Dashboard1&#34; title=&#34;from microsoft.com&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://acom.azurecomcdn.net/80C57D/cdn/mediahandler/docarticles/dpsmedia-prod/azure.microsoft.com/en-us/documentation/articles/log-analytics-containers/20160824105310/containers-dash02.png&#34; alt=&#34;Dashboard2&#34; title=&#34;from microsoft.com&#34; /&gt;&lt;/p&gt;

&lt;p&gt;なんと簡単じゃありませんか。詳細が気になるかたは、&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/log-analytics-containers/&#34;&gt;こちら&lt;/a&gt;から。&lt;/p&gt;

&lt;p&gt;なお、フィードバック&lt;a href=&#34;https://blogs.technet.microsoft.com/msoms/2016/08/24/announcing-public-preview-oms-container-solution-for-linux/&#34;&gt;熱烈歓迎&lt;/a&gt;だそうです。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Docker for WindowsでインストールレスAzure CLI環境を作る</title>
      <link>https://ToruMakabe.github.io/post/dockerforwin_azurecli/</link>
      <pubDate>Wed, 22 Jun 2016 15:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/dockerforwin_azurecli/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;舌の根の乾かぬ内に&#34;&gt;舌の根の乾かぬ内に&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://torumakabe.github.io/post/azure_osstools_iac/&#34;&gt;最近&lt;/a&gt;、VagrantとVirualBoxで似たようなやり口を紹介しましたが、気にしないでください。テクノロジーの進化は早い。&lt;/p&gt;

&lt;h2 id=&#34;動機&#34;&gt;動機&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Docker for Windows(on Client Hyper-V)のベータが一般開放された&lt;/li&gt;
&lt;li&gt;Dockerもそうだが、Hyper-V前提のツールが今後増えそう、となると、それとぶつかるVirtualBoxをぼちぼちやめたい&lt;/li&gt;
&lt;li&gt;月一ペースでアップデートされるAzure CLIをいちいちインストールしたくない、コンテナ引っ張って以上、にしたい&lt;/li&gt;
&lt;li&gt;開発端末の環境を汚したくない、いつでもきれいに作り直せるようにしたい&lt;/li&gt;
&lt;li&gt;○○レスって言ってみたかった&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;やり口&#34;&gt;やり口&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;もちろんDocker for Windows (on Client Hyper-V) を使う&lt;/li&gt;
&lt;li&gt;いちいちdocker run&amp;hellip;と打たなくていいよう、エイリアス的にPowerShellのfunction &amp;ldquo;azure_cli&amp;rdquo; を作る&lt;/li&gt;
&lt;li&gt;&amp;ldquo;azure_cli&amp;rdquo;入力にてAzure CLIコンテナを起動&lt;/li&gt;
&lt;li&gt;コンテナとホスト(Windows)間でファイル共有、ホスト側のIDEなりエディタを使えるようにする&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;作業の中身&#34;&gt;作業の中身&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Docker for Windowsを&lt;a href=&#34;https://docs.docker.com/docker-for-windows/&#34;&gt;インストール&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;64bit Windows 10 Pro/Enterprise/Education 1511以降に対応&lt;/li&gt;
&lt;li&gt;Hyper-Vの有効化を忘れずに&lt;/li&gt;
&lt;li&gt;Hyper-VとぶつかるVirtualBoxとはお別れです&lt;/li&gt;
&lt;li&gt;Docker for Windowsの起動時にIPをとれないケースがありますが、その場合はsettings -&amp;gt; Network から、設定変えずにApplyしてみてください。いまのところこれで対処できています。この辺はベータなので今後の調整を期待しましょう。&lt;/li&gt;
&lt;li&gt;共有ドライブも共有が外れていることが。settings -&amp;gt; Shared Drives で共有しなおしてください。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;PowerShell functionを作成

&lt;ul&gt;
&lt;li&gt;のちほど詳しく&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;powershellのfunctionを作る&#34;&gt;PowerShellのfunctionを作る&lt;/h2&gt;

&lt;p&gt;ここが作業のハイライト。&lt;/p&gt;

&lt;p&gt;PowerShellのプロファイルを編集します。ところでエディタはなんでもいいのですが、AzureやDockerをがっつり触る人にはVS Codeがおすすめです。&lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=msazurermtools.azurerm-vscode-tools&#34;&gt;Azure Resource Manager Template&lt;/a&gt;や&lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=PeterJausovec.vscode-docker&#34;&gt;Docker&lt;/a&gt;むけextensionがあります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Workspace\dockereval\arm&amp;gt; code $profile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;こんなfunctionを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function azure_cli {
   C:\PROGRA~1\Docker\Docker\Resources\bin\docker.exe run -it --rm -v ${HOME}/.azure:/root/.azure -v ${PWD}:/data -w /data microsoft/azure-cli:latest
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;エイリアスでなくfunctionにした理由は、引数です。エイリアスだと引数を渡せないので&lt;/li&gt;
&lt;li&gt;コンテナが溜まるのがいやなので、&amp;ndash;rmで都度消します&lt;/li&gt;
&lt;li&gt;毎度 azure login しなくていいよう、トークンが保管されるコンテナの/root/azureディレクトリをホストの${HOME}/.azureと-v オプションで共有します&lt;/li&gt;
&lt;li&gt;ARM TemplateのJSONファイルなど、ホストからファイルを渡したいため、カレントディレクトリ ${PWD} をコンテナと -v オプションで共有します&lt;/li&gt;
&lt;li&gt;コンテナはdocker hubのMicrosoft公式イメージ、latestを引っ張ります。latestで不具合あればバージョン指定してください&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ではテスト。まずはホスト側のファイルを確認。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Workspace\dockereval\arm&amp;gt; ls


    ディレクトリ: C:\Workspace\dockereval\arm


Mode                LastWriteTime         Length Name
----                -------------         ------ ----
d-----       2016/06/22     11:21                subd
-a----       2016/06/22     10:26           8783 azuredeploy.json
-a----       2016/06/22     11:28            690 azuredeploy.parameters.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;いくつかのファイルとサブディレクトリがあります。&lt;/p&gt;

&lt;p&gt;コンテナを起動してみましょう。azure_cli functionを呼びます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Workspace\dockereval\arm&amp;gt; azure_cli
root@be41d3389a21:/data#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;コンテナを起動し、入出力をつなぎました。ここからは頭と手をLinuxに切り替えてください。公式Azure CLIコンテナは&lt;a href=&#34;https://hub.docker.com/r/microsoft/azure-cli/~/dockerfile/&#34;&gt;debianベース&lt;/a&gt;です。&lt;/p&gt;

&lt;p&gt;ファイル共有できているか確認。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@be41d3389a21:/data# ls
azuredeploy.json  azuredeploy.parameters.json  subd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;できてますね。&lt;/p&gt;

&lt;p&gt;azureコマンドが打てるか確認。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@be41d3389a21:/data# azure -v
0.10.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;しあわせ。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azure X-Plat CLIでResource Policyを設定する</title>
      <link>https://ToruMakabe.github.io/post/azure_cli_resourcepolicy/</link>
      <pubDate>Sat, 21 May 2016 11:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_cli_resourcepolicy/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;azure-x-plat-cliのリリースサイクル&#34;&gt;Azure X-Plat CLIのリリースサイクル&lt;/h2&gt;

&lt;p&gt;OSS/Mac/Linux派なAzurerの懐刀、Azure X-Plat CLIのリリースサイクルは、おおよそ&lt;a href=&#34;https://github.com/Azure/azure-xplat-cli/releases&#34;&gt;月次&lt;/a&gt;です。改善と機能追加を定期的にまわしていくことには意味があるのですが、いっぽう、Azureの機能追加へタイムリーに追随できないことがあります。短期間とはいえ、次のリリースまで空白期間ができてしまうのです。&lt;/p&gt;

&lt;p&gt;たとえば、今回のテーマであるResource Policy。GA直後に公開された&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/resource-manager-policy/&#34;&gt;ドキュメント&lt;/a&gt;に、X-Plat CLIでの使い方が2016/5/21現在書かれていません。おやCLIではできないのかい、と思ってしまいますね。でもその後のアップデートで、できるようになりました。&lt;/p&gt;

&lt;p&gt;機能リリース時点ではCLIでできなかった、でもCLIの月次アップデートで追加された、いまはできる、ドキュメントの更新待ち。こんなパターンは多いので、あきらめずに探ってみてください。&lt;/p&gt;

&lt;h2 id=&#34;ポリシーによるアクセス管理&#34;&gt;ポリシーによるアクセス管理&lt;/h2&gt;

&lt;p&gt;さて本題。リソースの特性に合わせて、きめ細かいアクセス管理をしたいことがあります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;VMやストレージのリソースタグに組織コードを入れること強制し、費用負担の計算に使いたい&lt;/li&gt;
&lt;li&gt;日本国外リージョンのデータセンタを使えないようにしたい&lt;/li&gt;
&lt;li&gt;Linuxのディストリビューションを標準化し、その他のディストリビューションは使えなくしたい&lt;/li&gt;
&lt;li&gt;開発環境リソースグループでは、大きなサイズのインスタンスを使えないようにしたい&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;などなど。こういう課題にポリシーが効きます。&lt;/p&gt;

&lt;p&gt;従来からあるRBACは「役割と人」目線です。「この役割を持つ人は、このリソースを読み取り/書き込み/アクションできる」という表現をします。&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/role-based-access-built-in-roles/&#34;&gt;組み込みロールの一覧&lt;/a&gt;を眺めると、理解しやすいでしょう。&lt;/p&gt;

&lt;p&gt;ですが、RBACは役割と人を切り口にしているので、各リソースの多様な特性にあわせた統一表現が難しいです。たとえばストレージにはディストリビューションという属性はありません。無理してカスタム属性なんかで表現すると破綻しそうです。&lt;/p&gt;

&lt;p&gt;リソース目線でのアクセス管理もあったほうがいい、ということで、ポリシーの出番です。もちろんRBACと、組み合わせできます。&lt;/p&gt;

&lt;h2 id=&#34;x-plat-cliでの定義方法&#34;&gt;X-Plat CLIでの定義方法&lt;/h2&gt;

&lt;p&gt;2016/4リリースの&lt;a href=&#34;https://github.com/Azure/azure-xplat-cli/releases/tag/v0.9.20-April2016&#34;&gt;v0.9.20&lt;/a&gt;から、X-Plat CLIでもResource Policyを定義できます。&lt;/p&gt;

&lt;p&gt;ポリシーの定義、構文はPowerShellと同じなので、公式ドキュメントに任せます。ご一読を。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/resource-manager-policy/&#34;&gt;ポリシーを使用したリソース管理とアクセス制御&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;X-Plat CLI固有部分に絞って紹介します。&lt;/p&gt;

&lt;h3 id=&#34;ポリシー定義ファイルを作る&#34;&gt;ポリシー定義ファイルを作る&lt;/h3&gt;

&lt;p&gt;CLIでインラインに書けるようですが、人類には早すぎる気がします。ここではファイルに。&lt;/p&gt;

&lt;p&gt;例として、作成できるVMのサイズを限定してみましょう。開発環境などでよくあるパターンと思います。VM作成時、Standard_D1～5_v2に当てはまらないVMサイズが指定されると、拒否します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;if&amp;quot;: {
    &amp;quot;allOf&amp;quot;: [
      {
        &amp;quot;field&amp;quot;: &amp;quot;type&amp;quot;,
        &amp;quot;equals&amp;quot;: &amp;quot;Microsoft.Compute/virtualMachines&amp;quot;
      },
      {
        &amp;quot;not&amp;quot;: {
          &amp;quot;field&amp;quot;: &amp;quot;Microsoft.Compute/virtualMachines/sku.name&amp;quot;,
          &amp;quot;in&amp;quot;: [ &amp;quot;Standard_D1_v2&amp;quot;, &amp;quot;Standard_D2_v2&amp;quot;,&amp;quot;Standard_D3_v2&amp;quot;, &amp;quot;Standard_D4_v2&amp;quot;, &amp;quot;Standard_D5_v2&amp;quot; ]
        }
      }
    ]
  },
  &amp;quot;then&amp;quot;: {
    &amp;quot;effect&amp;quot;: &amp;quot;deny&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;policy_deny_vmsize.json というファイル名にしました。では投入。ポリシー名は deny_vmsize とします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ azure policy definition create -n deny_vmsize -p ./policy_deny_vmsize.json
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;info:    Executing command policy definition create
+ Creating policy definition deny_vmsize
data:    PolicyName:             deny_vmsize
data:    PolicyDefinitionId:     /subscriptions/mysubscription/providers/Microsoft.Authorization/policyDefinitions/deny_vmsize
data:    PolicyType:             Custom
data:    DisplayName:
data:    Description:
data:    PolicyRule:             allOf=[field=type, equals=Microsoft.Compute/virtualMachines, field=Microsoft.Compute/virtualMachines/sku.name, in=[Standard_D1_v2, Standard_D2_v2, Standard_D3_v2, Standard_D4_v2, Standard_D5_v2]], effect=deny
info:    policy definition create command OK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;できたみたいです。&lt;/p&gt;

&lt;h3 id=&#34;ポリシーをアサインする&#34;&gt;ポリシーをアサインする&lt;/h3&gt;

&lt;p&gt;では、このポリシーを割り当てます。割り当ての範囲(スコープ)はサブスクリプションとします。リソースグループなど、より細かいスコープも&lt;a href=&#34;https://msdn.microsoft.com/ja-jp/library/azure/mt588464.aspx&#34;&gt;指定可能&lt;/a&gt;です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ azure policy assignment create -n deny_vmsize_assignment -p /subscriptions/mysubscription/providers/Microsoft.Authorization/policyDefinitions/deny_vmsize -s /subscriptions/mysubscription
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;info:    Executing command policy assignment create
+ Creating policy assignment deny_vmsize_assignment
data:    PolicyAssignmentName:     deny_vmsize_assignment
data:    Type:                     Microsoft.Authorization/policyAssignments
data:    DisplayName:
data:    PolicyDefinitionId:       /subscriptions/mysubscription/providers/Microsoft.Authorization/policyDefinitions/deny_vmsize
data:    Scope:                    /subscriptions/mysubscription
info:    policy assignment create command OK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;割り当て完了。では試しに、このサブスクリプションに属するユーザで、Gシリーズのゴジラ級インスタンスを所望してみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ azure vm quick-create -g RPPoC -n rppocvm westus -y Linux -Q &amp;quot;canonical:ubuntuserver:14.04.4-LTS:latest&amp;quot; -u &amp;quot;adminname&amp;quot; -p &amp;quot;adminpass&amp;quot; -z Standard_G5
info:    Executing command vm quick-create
[...snip]
+ Creating VM &amp;quot;rppocvm&amp;quot;
error:   The resource action &#39;Microsoft.Compute/virtualMachines/write&#39; is disallowed by one or more policies. Policy identifier(s): &#39;/subscriptions/mysubscription/providers/Microsoft.Authorization/policyDefinitions/deny_vmsize&#39;.
info:    Error information has been recorded to /root/.azure/azure.err
error:   vm quick-create command failed
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;拒否られました。&lt;/p&gt;

&lt;p&gt;許可されているVMサイズだと。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ azure vm quick-create -g RPPoC -n rppocvm westus -y Linux -Q &amp;quot;canonical:ubuntuserver:14.04.4-LTS:latest&amp;quot; -u &amp;quot;adminname&amp;quot; -p &amp;quot;adminpass&amp;quot; -z Standard_D1_v2
info:    Executing command vm quick-create
[...snip]
info:    vm quick-create command OK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;成功。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>VagrantとDockerによるAzure向けOSS開発・管理端末のコード化</title>
      <link>https://ToruMakabe.github.io/post/azure_osstools_iac/</link>
      <pubDate>Fri, 13 May 2016 18:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_osstools_iac/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;端末だってコード化されたい&#34;&gt;端末だってコード化されたい&lt;/h2&gt;

&lt;p&gt;Infrastructure as Codeは特に騒ぐ話でもなくなってきました。このエントリは、じゃあ端末の開発環境やツール群もコード化しようという話です。結論から書くと、VagrantとDockerを活かします。超絶便利なのにAzure界隈ではあまり使われてない印象。もっと使われていいのではと思い、書いております。&lt;/p&gt;

&lt;h2 id=&#34;解決したい課題&#34;&gt;解決したい課題&lt;/h2&gt;

&lt;p&gt;こんな悩みを解決します。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;WindowsでOSS開発環境、Azure管理ツールのセットアップをするのがめんどくさい&lt;/li&gt;
&lt;li&gt;WindowsもMacも使っているので、どちらでも同じ環境を作りたい&lt;/li&gt;
&lt;li&gt;サーバはLinuxなので手元にもLinux環境欲しいけど、Linuxデスクトップはノーサンキュー&lt;/li&gt;
&lt;li&gt;2016年にもなって長いコードをVimとかEmacsで書きたくない&lt;/li&gt;
&lt;li&gt;Hyper-VとかVirtualboxで仮想マシンのセットアップと起動、後片付けをGUIでするのがいちいちめんどくさい&lt;/li&gt;
&lt;li&gt;仮想マシン起動したあとにターミナル起動-&amp;gt;IP指定-&amp;gt;ID/Passでログインとか、かったるい&lt;/li&gt;
&lt;li&gt;Azure CLIやTerraformなどクラウド管理ツールの進化が頻繁でつらい(月一回アップデートとか)&lt;/li&gt;
&lt;li&gt;でもアップデートのたびに超絶便利機能が追加されたりするので、なるべく追いかけたい&lt;/li&gt;
&lt;li&gt;新メンバーがチームに入るたび、セットアップが大変&lt;/li&gt;
&lt;li&gt;不思議とパソコンが生えてくる部屋に住んでおり、セットアップが大変&lt;/li&gt;
&lt;li&gt;毎度作業のどこかが抜ける、漏れる、間違う 人間だもの&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;やり口&#34;&gt;やり口&lt;/h2&gt;

&lt;p&gt;VagrantとDockerで解決します。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Windows/Macどちらにも対応しているVirtualboxでLinux仮想マシンを作る&lt;/li&gt;
&lt;li&gt;Vagrantでセットアップを自動化する&lt;/li&gt;
&lt;li&gt;Vagrantfile(RubyベースのDSL)でシンプルに環境をコード化する&lt;/li&gt;
&lt;li&gt;Vagrant Puttyプラグインを使って、Windowsでもsshログインを簡素化する&lt;/li&gt;
&lt;li&gt;公式dockerイメージがあるツールは、インストールせずコンテナを引っ張る&lt;/li&gt;
&lt;li&gt;Windows/MacのいまどきなIDEなりエディタを使えるようにする&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;セットアップ概要&#34;&gt;セットアップ概要&lt;/h2&gt;

&lt;p&gt;簡単す。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Virtualboxを&lt;a href=&#34;https://www.virtualbox.org/&#34;&gt;インストール&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Vagrantを&lt;a href=&#34;https://www.vagrantup.com/downloads.html&#34;&gt;インストール&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Vagrant Putty Plugin(vagrant-multi-putty)を&lt;a href=&#34;https://github.com/nickryand/vagrant-multi-putty&#34;&gt;インストール&lt;/a&gt; #Windowsのみ。Puttyは別途入れてください&lt;/li&gt;
&lt;li&gt;作業フォルダを作り、Vagrant ファイルを書く&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;もしWindowsでうまく動かない時は、Hyper-Vが有効になっていないか確認しましょう。Virtualboxと共存できません。&lt;/p&gt;

&lt;h2 id=&#34;サンプル解説&#34;&gt;サンプル解説&lt;/h2&gt;

&lt;p&gt;OSSなAzurerである、わたしのVagrantfileです。日々環境に合わせて変えてますが、以下は現時点でのスナップショット。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# -*- mode: ruby -*-
# vi: set ft=ruby :

# Vagrantfile API/syntax version. Don&#39;t touch unless you know what you&#39;re doing!
VAGRANTFILE_API_VERSION = &amp;quot;2&amp;quot;

$bootstrap=&amp;lt;&amp;lt;SCRIPT

#Common tools
sudo apt-get update
sudo apt-get -y install wget unzip jq

#Docker Engine
sudo apt-get -y install apt-transport-https ca-certificates
sudo apt-get -y install linux-image-extra-$(uname -r)
sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
sudo sh -c &amp;quot;echo deb https://apt.dockerproject.org/repo ubuntu-trusty main &amp;gt; /etc/apt/sources.list.d/docker.list&amp;quot;
sudo apt-get update
sudo apt-get -y purge lxc-docker
sudo apt-cache policy docker-engine
sudo apt-get -y install docker-engine=1.11.1-0~trusty
sudo gpasswd -a vagrant docker
sudo service docker restart

#Docker Machine
sudo sh -c &amp;quot;curl -L https://github.com/docker/machine/releases/download/v0.7.0/docker-machine-`uname -s`-`uname -m` &amp;gt;/usr/local/bin/docker-machine &amp;amp;&amp;amp; chmod +x /usr/local/bin/docker-machine&amp;quot;

#Azure CLI
echo &amp;quot;alias azure=&#39;docker run -it --rm -v \\\$HOME/.azure:/root/.azure -v \\\$PWD:/data -w /data microsoft/azure-cli:latest azure&#39;&amp;quot; &amp;gt;&amp;gt; $HOME/.bashrc

#Terraform
echo &amp;quot;alias terraform=&#39;docker run -it --rm -v \\\$PWD:/data -w /data hashicorp/terraform:0.6.14&#39;&amp;quot; &amp;gt;&amp;gt; $HOME/.bashrc

#Packer
echo &amp;quot;alias packer=&#39;docker run -it --rm -v \\\$PWD:/data -w /data hashicorp/packer:latest&#39;&amp;quot; &amp;gt;&amp;gt; $HOME/.bashrc

#nodebrew
curl -L git.io/nodebrew | perl - setup
echo &#39;export PATH=$HOME/.nodebrew/current/bin:$PATH&#39; &amp;gt;&amp;gt; $HOME/.bashrc
$HOME/.nodebrew/current/bin/nodebrew install-binary 5.9.1
$HOME/.nodebrew/current/bin/nodebrew use 5.9.1

#Python3
wget -qO- https://bootstrap.pypa.io/get-pip.py | sudo -H python3.4

SCRIPT

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
  # Every Vagrant virtual environment requires a box to build off of.

  config.vm.box = &amp;quot;ubuntu/trusty64&amp;quot;

  # Create a private network, which allows host-only access to the machine
  # using a specific IP.

  config.vm.network &amp;quot;private_network&amp;quot;, ip: &amp;quot;192.168.33.10&amp;quot;

  config.vm.provider &amp;quot;virtualbox&amp;quot; do |vb|
     vb.customize [&amp;quot;modifyvm&amp;quot;, :id, &amp;quot;--memory&amp;quot;, &amp;quot;2048&amp;quot;]
  end

  config.vm.provision :shell, inline: $bootstrap, privileged: false

end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;$bootstrap=&amp;lt;&amp;lt;SCRIPT から SCRIPT が、プロビジョニングシェルです。初回のvagrant up時とvagrant provision時に実行されます。&lt;/p&gt;

&lt;h3 id=&#34;common-tools&#34;&gt;Common tools&lt;/h3&gt;

&lt;p&gt;一般的なツールをaptでインストールします。wgetとかjqとか。&lt;/p&gt;

&lt;h3 id=&#34;docker-engine-machine&#34;&gt;Docker Engine &amp;amp; Machine&lt;/h3&gt;

&lt;p&gt;この後前提となるDockerをインストール。Dockerのバージョンは1.11.1を明示しています。Dockerは他への影響が大きいので、バージョンアップは慎重めの方針です。&lt;/p&gt;

&lt;h3 id=&#34;azure-cli&#34;&gt;Azure CLI&lt;/h3&gt;

&lt;p&gt;インストールせずに&lt;a href=&#34;https://hub.docker.com/r/microsoft/azure-cli/&#34;&gt;MS公式のDockerイメージ&lt;/a&gt;を引っ張ります。なのでalias設定だけ。
-v オプションで、ホストLinuxとコンテナ間でデータを共有します。CLIが使う認証トークン($HOME/.azure下)やCLI実行時に渡すjsonファイル(作業ディレクトリ)など。詳細は後ほど。
また、azureコマンド発行ごとにコンテナが溜まっていくのがつらいので、&amp;ndash;rmで消します。&lt;/p&gt;

&lt;h3 id=&#34;terraform-packer&#34;&gt;Terraform &amp;amp; Packer&lt;/h3&gt;

&lt;p&gt;Azure CLIと同様です。Hashicorpが&lt;a href=&#34;https://hub.docker.com/u/hashicorp/&#34;&gt;公式イメージ&lt;/a&gt;を提供しているので、それを活用します。
方針はlatest追いですが、不具合があればバージョンを指定します。たとえば、現状Terraformのlatestイメージに不具合があるので、0.6.14を指定しています。
-v オプションもAzure CLIと同じ。ホストとコンテナ間のファイルマッピングに使います。&lt;/p&gt;

&lt;p&gt;なお、公式とはいえ他人のイメージを使う時には、Dockerfileの作りやビルド状況は確認しましょう。危険がデンジャラスですし、ENTRYPOINTとか知らずにうっかり使うと途方に暮れます。&lt;/p&gt;

&lt;h3 id=&#34;nodebrew&#34;&gt;nodebrew&lt;/h3&gt;

&lt;p&gt;nodeのバージョンを使い分けるため。セットアップ時にv5.9.1を入れています。Azure Functions開発向け。&lt;/p&gt;

&lt;h3 id=&#34;python3&#34;&gt;Python3&lt;/h3&gt;

&lt;p&gt;Ubuntu 14.04では標準がPython2なので別途入れてます。Azure Batch向け開発でPython3使いたいので。&lt;/p&gt;

&lt;p&gt;みなさん他にもいろいろあるでしょう。シェルなのでお好みで。&lt;/p&gt;

&lt;p&gt;さて、ここまでがプロビジョニング時の処理です。以降の&amp;rdquo;Vagrant.configure～&amp;rdquo;は仮想マシンの定義で、難しくありません。ubuntu/trusty64(14.04)をboxイメージとし、IPやメモリを指定し、先ほど定義したプロビジョニング処理を指しているだけです。&lt;/p&gt;

&lt;h2 id=&#34;どれだけ楽か&#34;&gt;どれだけ楽か&lt;/h2&gt;

&lt;p&gt;では、環境を作ってみましょう。Vagrantfileがあるフォルダで&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vagrant up
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;仮想マシンが作成されます。初回はプロビジョニング処理も走ります。&lt;/p&gt;

&lt;p&gt;できましたか。できたら、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vagrant putty
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;はい。Puttyが起動し、ID/Passを入れなくてもsshログインします。破壊力抜群。わたしはこの魅力だけでTeraterm(Terraformではない)からPuttyに乗り換えました。ちなみにMacでは、vagrant sshで済みます。&lt;/p&gt;

&lt;p&gt;あとはプロビジョニングされたLinuxを使って楽しんでください。そして、必要なくなったら or 作り直したくなったら&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vagrant destroy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;綺麗さっぱりです。仮想マシンごと消します。消さずにまた使う時は、vagrant haltを。&lt;/p&gt;

&lt;p&gt;なお、vagrant upしたフォルダにあるファイルは、Virtualboxの共有フォルダ機能で仮想マシンと共有されます。shareとかいう名のフォルダを作って、必要なファイルを放り込んでおきましょう。その場合、仮想マシンのUbuntuからは/vagrant/shareと見えます。双方向で同期されます。&lt;/p&gt;

&lt;p&gt;わたしは長いコードを書くときは、Windows/Mac側のIDEなりエディタを使って、実行は仮想マシンのLinux側、という流れで作業しています。&lt;/p&gt;

&lt;p&gt;ちなみに、改行コードの違いやパーミッションには気を付けてください。改行コードはLFにする癖をつけておくと幸せになれます。パーミッションは全開、かつ共有領域では変えられないので、問題になるときは仮想マシン側で/vagrant外にコピーして使ってください。パーミッション全開だと怒られる認証鍵など置かないよう、注意。&lt;/p&gt;

&lt;p&gt;また、Dockerコンテナを引っ張るAzure CLI、Terraform、Packerの注意点。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;初回実行時にイメージのPullを行うので、帯域の十分なところでやりましょう&lt;/li&gt;
&lt;li&gt;サンプルでは -v $PWD:/data オプションにて、ホストのカレントディレクトリをコンテナの/dataにひもづけています。そして、-w /data にて、コンテナ内ワーキングディレクトリを指定しています。コマンドの引数でファイル名を指定したい場合は、実行したいファイルがあるディレクトリに移動して実行してください

&lt;ul&gt;
&lt;li&gt;(例) azure group deployment create RG01 DEP01 -f ./azuredeploy.json -e ./azuredeploy.parameters.json&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;bash-on-windowsまで待つとか言わない&#34;&gt;Bash on Windowsまで待つとか言わない&lt;/h2&gt;

&lt;p&gt;「WindowsではOSSの開発や管理がしにくい。Bash on Windowsが出てくるまで待ち」という人は、待たないで今すぐGoです。思い立ったが吉日です。繰り返しますがVagrantとDocker、超絶便利です。&lt;/p&gt;

&lt;p&gt;インフラのコード化なんか信用ならん！という人も、まず今回紹介したように端末からはじめてみたらいかがでしょう。激しく生産性上がると思います。&lt;/p&gt;

&lt;p&gt;夏近し、楽して早く帰ってビール呑みましょう。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azure FunctionsとFacebook Messenger APIで好みなんて聞いてないBotを作る</title>
      <link>https://ToruMakabe.github.io/post/azure_functions_fbmsgapi/</link>
      <pubDate>Sun, 08 May 2016 14:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_functions_fbmsgapi/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;まだ好みなんて聞いてないぜ&#34;&gt;まだ好みなんて聞いてないぜ&lt;/h2&gt;

&lt;p&gt;Build 2016で、&lt;a href=&#34;https://azure.microsoft.com/ja-jp/services/functions/&#34;&gt;Azure Functions&lt;/a&gt;が発表されました。&lt;/p&gt;

&lt;p&gt;Azure Functionsは、&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;アプリを放り込めば動く。サーバの管理が要らない。サーバレス。  #でもこれは従来のPaaSもそう&lt;/li&gt;
&lt;li&gt;利用メモリ単位での、粒度の細かい課金。  #現在プレビュー中にて、詳細は今後発表&lt;/li&gt;
&lt;li&gt;Azure内外機能との、容易なイベント連動。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;が特徴です。AWSのLambdaと似てるっちゃ似ています。&lt;/p&gt;

&lt;p&gt;何が新しいかというと、特に3つ目の特徴、イベント連動です。触ってみなければわからん、ということで、流行りのBotでも作ってみたいと思います。&lt;/p&gt;

&lt;h3 id=&#34;基本方針&#34;&gt;基本方針&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;FunctionsはAzure内の様々な機能と&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/functions-reference/#bindings&#34;&gt;イベント連動&lt;/a&gt;できるが、あえてサンプルの少ないAzure外とつないでみる&lt;/li&gt;
&lt;li&gt;Facebook Messenger APIを使って、webhook連動する&lt;/li&gt;
&lt;li&gt;Facebook Messenger向けに書き込みがあると、ランダムでビールの種類と参考URLを返す&lt;/li&gt;
&lt;li&gt;ビールは&lt;a href=&#34;http://beertaster.org/beerstyle/web/beerstyle_main_j.html&#34;&gt;Craft Beer Association&lt;/a&gt;の分類に従い、協会のビアスタイル・ガイドライン参考ページの該当URLを返す&lt;/li&gt;
&lt;li&gt;Botらしく、それらしい文末表現をランダムで返す&lt;/li&gt;
&lt;li&gt;好みとか文脈は全く聞かないぜSorry&lt;/li&gt;
&lt;li&gt;アプリはNodeで書く。C#のサンプルは増えてきたので&lt;/li&gt;
&lt;li&gt;静的データをランダムに返す、かつ少量なのでメモリ上に広げてもいいが、せっかくなのでNodeと相性のいいDocumentDBを使う&lt;/li&gt;
&lt;li&gt;DocumentDBではSQLでいうORDER BY RAND()のようなランダムな問い合わせを書けないため、ストアドプロシージャで実装する  #&lt;a href=&#34;https://gist.github.com/murdockcrc/12266f9d844be416a6a0&#34;&gt;サンプル&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FunctionsとGithubを連携し、GithubへのPush -&amp;gt; Functionsへのデプロイというフローを作る&lt;/li&gt;
&lt;li&gt;拡張性はひとまず目をつぶる  #&lt;a href=&#34;http://qiita.com/yoichiro@github/items/6d4c7309210af20a5c8f&#34;&gt;この辺の話&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ひとまずFunctionsとBotの枠組みの理解をゴールとします。ロジックをたくさん書けばそれなりに文脈を意識した返事はできるのですが、書かずに済む仕組みがこれからいろいろ出てきそうなので、書いたら負けの精神でぐっと堪えます。&lt;/p&gt;

&lt;h2 id=&#34;必要な作業&#34;&gt;必要な作業&lt;/h2&gt;

&lt;p&gt;以下が必要な作業の流れです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Azureで

&lt;ul&gt;
&lt;li&gt;Function Appの作成  #1&lt;/li&gt;
&lt;li&gt;Bot用Functionの作成 #2&lt;/li&gt;
&lt;li&gt;Facebook Messenger APIとの接続検証  #6&lt;/li&gt;
&lt;li&gt;Facebook Messenger API接続用Tokenの設定  #8&lt;/li&gt;
&lt;li&gt;DocumentDBのデータベース、コレクション作成、ドキュメント投入  #9&lt;/li&gt;
&lt;li&gt;DocumentDBのストアドプロシージャ作成  #10&lt;/li&gt;
&lt;li&gt;Function Appを書く  #11&lt;/li&gt;
&lt;li&gt;FunctionsのサイトにDocumentDB Node SDKを導入 #12&lt;/li&gt;
&lt;li&gt;Function AppのGithub連携設定  #13&lt;/li&gt;
&lt;li&gt;Function Appのデプロイ (GithubへのPush)  #14&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Facebookで

&lt;ul&gt;
&lt;li&gt;Facebook for Developersへの登録  #3&lt;/li&gt;
&lt;li&gt;Botをひも付けるFacebook Pageの作成  #4&lt;/li&gt;
&lt;li&gt;Bot用マイアプリの作成  #5&lt;/li&gt;
&lt;li&gt;Azure Functionsからのcallback URLを登録、接続検証  #6&lt;/li&gt;
&lt;li&gt;Azure Functions向けTokenを生成 #7&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;アプリのコード書きの他はそれほど重くない作業ですが、すべての手順を書くと本ができそうです。Function Appの作りにポイントを絞りたいので、以下、参考になるサイトをご紹介します。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Function Appを書くまで、#1〜2、#5〜8は、&lt;a href=&#34;http://oauth.jp/blog/2016/04/19/fb-message-callback-with-azure-function/&#34;&gt;こちらのブログエントリ&lt;/a&gt;がとても参考になります。&lt;/li&gt;
&lt;li&gt;Facebook for Developersへの登録、#3は、&lt;a href=&#34;https://developers.facebook.com/&#34;&gt;https://developers.facebook.com/&lt;/a&gt; から。いきなり迷子の人は、&lt;a href=&#34;http://qiita.com/k_kuni/items/3d7176ee4e3009b45dd8&#34;&gt;こちら&lt;/a&gt;も参考に。&lt;/li&gt;
&lt;li&gt;Facebook Pageの作成は、&lt;a href=&#34;http://allabout.co.jp/gm/gc/387840/&#34;&gt;ここ&lt;/a&gt;を。Botで楽しむだけなら細かい設定は後回しでいいです。&lt;/li&gt;
&lt;li&gt;DocumentDBについては、&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/documentdb-introduction/&#34;&gt;公式&lt;/a&gt;を。

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/documentdb-create-account/&#34;&gt;DBアカウント〜コレクション作成&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/documentdb-import-data/&#34;&gt;ドキュメントインポート&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/documentdb-programming/&#34;&gt;ストアドプロシージャ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;FunctionsのサイトにDocumentDB Node SDKを導入する#12は、&lt;a href=&#34;http://tech.guitarrapc.com/entry/2016/04/05/043723&#34;&gt;こちら&lt;/a&gt;を。コンソールからnpm installできます。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Github連携設定、#13〜14は、&lt;a href=&#34;http://tech.guitarrapc.com/entry/2016/04/03/051552&#34;&gt;こちら&lt;/a&gt;がとても参考になります。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;function-appのサンプル&#34;&gt;Function Appのサンプル&lt;/h2&gt;

&lt;p&gt;Githubにソースを&lt;a href=&#34;https://github.com/ToruMakabe/MakabeerBot&#34;&gt;置いておきます&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;ちなみにこのディレクトリ階層はGithub連携を考慮し、Function Appサイトのそれと合わせています。以下がデプロイ後のサイト階層です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;D:\home\site\wwwroot
├── fb-message-callback
│   ├── TestOutput.json
│   ├── function.json
│   └── index.js  #これが今回のアプリ
├── node_modules  #DocumentDB Node SDKが入っている
├── host.json
├── README.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;なお、DocumentDBのSDKパッケージは、なぜかfb-message-callbackローカルに置くと読み込まれないため、暫定的にルートへ配置しています。&lt;/p&gt;

&lt;p&gt;ではFunction Appの実体、index.jsを見てみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var https = require(&#39;https&#39;);
var documentClient = require(&amp;quot;documentdb&amp;quot;).DocumentClient;
const databaseUrl = &amp;quot;dbs/&amp;quot; + process.env.APPSETTING_DOCDB_DB_ID;

var client = new documentClient(process.env.APPSETTING_DOCDB_ENDPOINT, { &amp;quot;masterKey&amp;quot;: process.env.APPSETTING_DOCDB_AUTHKEY });

function sendTextMessage(sender, text, context) {
  getDataFromDocDB().then(function (value) {
    var msgAll = value[0].randomDocument.beer + &amp;quot; &amp;quot; + value[1].randomDocument.msg;
    var postData = JSON.stringify({
      recipient: sender,
      message: {
        &amp;quot;attachment&amp;quot;:{
          &amp;quot;type&amp;quot;:&amp;quot;template&amp;quot;,
          &amp;quot;payload&amp;quot;:{
            &amp;quot;template_type&amp;quot;:&amp;quot;button&amp;quot;,
            &amp;quot;text&amp;quot;:msgAll,
            &amp;quot;buttons&amp;quot;:[
              {
                &amp;quot;type&amp;quot;:&amp;quot;web_url&amp;quot;,
                &amp;quot;url&amp;quot;:value[0].randomDocument.url,
                &amp;quot;title&amp;quot;:&amp;quot;詳しく&amp;quot;
              }
            ]
          }
        }
      }
    });
    var req = https.request({
      hostname: &#39;graph.facebook.com&#39;,
      port: 443,
      path: &#39;/v2.6/me/messages&#39;,
      method: &#39;POST&#39;,
      headers: {
        &#39;Content-Type&#39;: &#39;application/json&#39;,
        &#39;Authorization&#39;: &#39;Bearer &#39; + process.env.APPSETTING_FB_PAGE_TOKEN
      }
    });
    req.write(postData);
    req.end();
  }).catch(function(err){
    context.log(err);
  });  
}

function getRandomDoc(sprocUrl){
  return new Promise(function (resolve, reject) {
    const sprocParams = {};
    client.executeStoredProcedure(sprocUrl, sprocParams, function(err, result, responseHeaders) {
      if (err) {
        reject(err);
      }
      if (result) {
        resolve(result);
      }
    });
  });
}

var results = {
  beer: function getBeer() {
    var collectionUrl = databaseUrl + &amp;quot;/colls/beer&amp;quot;;
    var sprocUrl = collectionUrl + &amp;quot;/sprocs/GetRandomDoc&amp;quot;;
    return getRandomDoc(sprocUrl).then(function (result) {
      return result;
    });
  },
  eom: function getEom() {
    var collectionUrl = databaseUrl + &amp;quot;/colls/eom&amp;quot;;
    var sprocUrl = collectionUrl + &amp;quot;/sprocs/GetRandomDoc&amp;quot;;
    return getRandomDoc(sprocUrl).then(function (result) {
      return result;
    });
  }
}

function getDataFromDocDB() {
  return Promise.all([results.beer(), results.eom()]);
}

module.exports = function (context, req) {
  messaging_evts = req.body.entry[0].messaging;
  for (i = 0; i &amp;lt; messaging_evts.length; i++) {
    evt = req.body.entry[0].messaging[i];
    sender = evt.sender;
    if (evt.message &amp;amp;&amp;amp; evt.message.text, context) {
      sendTextMessage(sender, evt.message.text, context);
    }
  }
  context.done();
};
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;最下部のmodule.export以降のブロックで、webhookイベントを受け取ります&lt;/li&gt;
&lt;li&gt;それがmessageイベントで、テキストが入っていれば、sendTextMessage関数を呼びます

&lt;ul&gt;
&lt;li&gt;好みは聞いてないので、以降、受け取ったテキストが読まれることはありませんが&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;sendTextMessage関数内、getDataFromDocDB関数呼び出しでDocumentDBへ問い合わせてビールと文末表現をランダムに取り出します

&lt;ul&gt;
&lt;li&gt;コレクション&amp;rdquo;beer&amp;rdquo;、&amp;rdquo;eom(end of message)&amp;ldquo;の構造はそれぞれこんな感じ
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;url&amp;quot;: &amp;quot;http://beertaster.org/beerstyle/web/001A.html#japanese&amp;quot;,
  &amp;quot;beer&amp;quot;: &amp;quot;酵母なし、ライトアメリカン・ウィートビール&amp;quot;,
  &amp;quot;id&amp;quot;: &amp;quot;bf3636c5-4284-4e7a-b587-9002a771f214&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;msg&amp;quot;: &amp;quot;はウマい&amp;quot;,
  &amp;quot;id&amp;quot;: &amp;quot;acd63222-2138-4e19-894e-dc85a950be64&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;DocumentDBの2つのコレクションへの問い合わせが終わった後、Facebookへメッセージを返すため、逐次処理目的でJavaScriptの&lt;a href=&#34;http://azu.github.io/promises-book/&#34;&gt;Promise&lt;/a&gt;を使っています&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;いかがでしょう。好みを聞かない気まぐれBotとはいえ、気軽に作れることがわかりました。ゼロからこの手のイベント処理を作るの、面倒ですものね。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&amp;ldquo;なお、Facebook Messenger API連動アプリの外部公開には、審査が必要とのことです&amp;rdquo;&lt;/em&gt;&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azure BatchとDockerで管理サーバレスバッチ環境を作る</title>
      <link>https://ToruMakabe.github.io/post/azure_batch_docker/</link>
      <pubDate>Fri, 29 Apr 2016 17:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_batch_docker/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;サーバレスって言いたいだけじゃないです&#34;&gt;サーバレスって言いたいだけじゃないです&lt;/h2&gt;

&lt;p&gt;Linux向けAzure BatchのPreviewが&lt;a href=&#34;https://azure.microsoft.com/ja-jp/blog/announcing-support-of-linux-vm-on-azure-batch-service/&#34;&gt;はじまり&lt;/a&gt;ました。地味ですが、なかなかのポテンシャルです。&lt;/p&gt;

&lt;p&gt;クラウドでバッチを走らせる時にチャレンジしたいことの筆頭は「ジョブを走らせる時だけサーバ使う。待機時間は消しておいて、
節約」でしょう。&lt;/p&gt;

&lt;p&gt;ですが、仕組み作りが意外に面倒なんですよね。管理サーバを作って、ジョブ管理ソフト入れて、Azure SDK/CLI入れて。クレデンシャルを安全に管理して。可用性確保して。バックアップして。で、管理サーバは消せずに常時起動。なんか中途半端です。&lt;/p&gt;

&lt;p&gt;その課題、Azure Batchを使って解決しましょう。レッツ管理サーバレスバッチ処理。&lt;/p&gt;

&lt;h2 id=&#34;コンセプト&#34;&gt;コンセプト&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;管理サーバを作らない&lt;/li&gt;
&lt;li&gt;Azure Batchコマンドでジョブを投入したら、あとはスケジュール通りに定期実行される&lt;/li&gt;
&lt;li&gt;ジョブ実行サーバ群(Pool)は必要な時に作成され、処理が終わったら削除される&lt;/li&gt;
&lt;li&gt;サーバの迅速な作成とアプリ可搬性担保のため、dockerを使う&lt;/li&gt;
&lt;li&gt;セットアップスクリプト、タスク実行ファイル、アプリ向け入力/出力ファイルはオブジェクトストレージに格納&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;サンプル&#34;&gt;サンプル&lt;/h2&gt;

&lt;p&gt;Githubにソースを&lt;a href=&#34;https://github.com/ToruMakabe/Azure_Batch_Sample&#34;&gt;置いておきます&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;バッチアカウントとストレージアカウント-コンテナの作成とアプリ-データの配置&#34;&gt;バッチアカウントとストレージアカウント、コンテナの作成とアプリ、データの配置&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/batch-technical-overview/&#34;&gt;公式ドキュメント&lt;/a&gt;で概要を確認しましょう。うっすら理解できたら、バッチアカウントとストレージアカウントを作成します。&lt;/p&gt;

&lt;p&gt;ストレージアカウントに、Blobコンテナを作ります。サンプルの構成は以下の通り。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.
├── blob
│   ├── application
│   │   ├── starttask.sh
│   │   └── task.sh
│   ├── input
│   │   └── the_star_spangled_banner.txt
│   └── output
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;applicationコンテナに、ジョブ実行サーバ作成時のスクリプト(starttask.sh)と、タスク実行時のスクリプト(task.sh)を配置します。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ToruMakabe/Azure_Batch_Sample/blob/master/blob/application/starttask.sh&#34;&gt;starttask.sh&lt;/a&gt; - docker engineをインストールします&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ToruMakabe/Azure_Batch_Sample/blob/master/blob/application/task.sh&#34;&gt;task.sh&lt;/a&gt; - docker hubからサンプルアプリが入ったコンテナを持ってきて実行します。&lt;a href=&#34;https://github.com/ToruMakabe/Azure_Batch_Sample/tree/master/docker&#34;&gt;サンプル&lt;/a&gt;はPythonで書いたシンプルなWord Countアプリです&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;また、アプリにデータをわたすinputコンテナと、実行結果を書き込むoutputコンテナも作ります。サンプルのinputデータはアメリカ国歌です。&lt;/p&gt;

&lt;p&gt;コンテナ、ファイルには、適宜SASを生成しておいてください。inputではreadとlist、outputでは加えてwrite権限を。&lt;/p&gt;

&lt;p&gt;さて、いよいよジョブをJSONで定義します。詳細は&lt;a href=&#34;https://msdn.microsoft.com/en-us/library/azure/dn820158.aspx?f=255&amp;amp;MSPPError=-2147217396&#34;&gt;公式ドキュメント&lt;/a&gt;を確認してください。ポイントだけまとめます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;2016/04/29 05:30(UTC)から開始する - schedule/doNotRunUntil&lt;/li&gt;
&lt;li&gt;4時間ごとに実行する - schedule/recurrenceInterval&lt;/li&gt;
&lt;li&gt;ジョブ実行後にサーバプールを削除する - jobSpecification/poolInfo/autoPoolSpecification/poolLifetimeOption&lt;/li&gt;
&lt;li&gt;ジョブ実行時にtask.shを呼び出す  - jobSpecification/jobManagerTask/commandLine&lt;/li&gt;
&lt;li&gt;サーバはUbuntu 14.04とする - jobSpecification/poolInfo/autoPoolSpecification/virtualMachineConfiguration&lt;/li&gt;
&lt;li&gt;サーバ数は1台とする - jobSpecification/poolInfo/autoPoolSpecification/pool/targetDedicated&lt;/li&gt;
&lt;li&gt;サーバプール作成時にstarttask.shを呼び出す - jobSpecification/poolInfo/autoPoolSpecification/pool/startTask&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;  {
  &amp;quot;odata.metadata&amp;quot;:&amp;quot;https://myaccount.myregion.batch.azure.com/$metadata#jobschedules/@Element&amp;quot;,
  &amp;quot;id&amp;quot;:&amp;quot;myjobschedule1&amp;quot;,
  &amp;quot;schedule&amp;quot;: {
    &amp;quot;doNotRunUntil&amp;quot;:&amp;quot;2016-04-29T05:30:00.000Z&amp;quot;,
    &amp;quot;recurrenceInterval&amp;quot;:&amp;quot;PT4H&amp;quot;
  },
  &amp;quot;jobSpecification&amp;quot;: {
    &amp;quot;priority&amp;quot;:100,
    &amp;quot;constraints&amp;quot;: {
      &amp;quot;maxWallClockTime&amp;quot;:&amp;quot;PT1H&amp;quot;,
      &amp;quot;maxTaskRetryCount&amp;quot;:-1
    },
    &amp;quot;jobManagerTask&amp;quot;: {
      &amp;quot;id&amp;quot;:&amp;quot;mytask1&amp;quot;,
      &amp;quot;commandLine&amp;quot;:&amp;quot;/bin/bash -c &#39;export LC_ALL=en_US.UTF-8; ./task.sh&#39;&amp;quot;,
      &amp;quot;resourceFiles&amp;quot;: [ {
        &amp;quot;blobSource&amp;quot;:&amp;quot;yourbloburi&amp;amp;sas&amp;quot;,
        &amp;quot;filePath&amp;quot;:&amp;quot;task.sh&amp;quot;
      }], 
      &amp;quot;environmentSettings&amp;quot;: [ {
        &amp;quot;name&amp;quot;:&amp;quot;VAR1&amp;quot;,
        &amp;quot;value&amp;quot;:&amp;quot;hello&amp;quot;
      } ],
      &amp;quot;constraints&amp;quot;: {
        &amp;quot;maxWallClockTime&amp;quot;:&amp;quot;PT1H&amp;quot;,
        &amp;quot;maxTaskRetryCount&amp;quot;:0,
        &amp;quot;retentionTime&amp;quot;:&amp;quot;PT1H&amp;quot;
      },
      &amp;quot;killJobOnCompletion&amp;quot;:false,
      &amp;quot;runElevated&amp;quot;:true,
      &amp;quot;runExclusive&amp;quot;:true
      },
      &amp;quot;poolInfo&amp;quot;: {
        &amp;quot;autoPoolSpecification&amp;quot;: {
          &amp;quot;autoPoolIdPrefix&amp;quot;:&amp;quot;mypool&amp;quot;,
          &amp;quot;poolLifetimeOption&amp;quot;:&amp;quot;job&amp;quot;,
          &amp;quot;pool&amp;quot;: {
            &amp;quot;vmSize&amp;quot;:&amp;quot;STANDARD_D1&amp;quot;,
            &amp;quot;virtualMachineConfiguration&amp;quot;: {
              &amp;quot;imageReference&amp;quot;: {
                &amp;quot;publisher&amp;quot;:&amp;quot;Canonical&amp;quot;,
                &amp;quot;offer&amp;quot;:&amp;quot;UbuntuServer&amp;quot;,
                &amp;quot;sku&amp;quot;:&amp;quot;14.04.4-LTS&amp;quot;,
                &amp;quot;version&amp;quot;:&amp;quot;latest&amp;quot;
              },
              &amp;quot;nodeAgentSKUId&amp;quot;:&amp;quot;batch.node.ubuntu 14.04&amp;quot;
            },
            &amp;quot;resizeTimeout&amp;quot;:&amp;quot;PT15M&amp;quot;,
            &amp;quot;targetDedicated&amp;quot;:1,
            &amp;quot;maxTasksPerNode&amp;quot;:1,
            &amp;quot;taskSchedulingPolicy&amp;quot;: {
              &amp;quot;nodeFillType&amp;quot;:&amp;quot;Spread&amp;quot;
            },
            &amp;quot;enableAutoScale&amp;quot;:false,
            &amp;quot;enableInterNodeCommunication&amp;quot;:false,
            &amp;quot;startTask&amp;quot;: {
              &amp;quot;commandLine&amp;quot;:&amp;quot;/bin/bash -c &#39;export LC_ALL=en_US.UTF-8; ./starttask.sh&#39;&amp;quot;,
              &amp;quot;resourceFiles&amp;quot;: [ {
                &amp;quot;blobSource&amp;quot;:&amp;quot;yourbloburi&amp;amp;sas&amp;quot;,
                &amp;quot;filePath&amp;quot;:&amp;quot;starttask.sh&amp;quot;
              } ],
              &amp;quot;environmentSettings&amp;quot;: [ {
                &amp;quot;name&amp;quot;:&amp;quot;VAR2&amp;quot;,
                &amp;quot;value&amp;quot;:&amp;quot;Chao&amp;quot;
              } ],
              &amp;quot;runElevated&amp;quot;:true,
              &amp;quot;waitForSuccess&amp;quot;:true
            },
            &amp;quot;metadata&amp;quot;: [ {
              &amp;quot;name&amp;quot;:&amp;quot;myproperty&amp;quot;,
              &amp;quot;value&amp;quot;:&amp;quot;myvalue&amp;quot;
            } ]
          }
        }
      }
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;そろそろ人類はJSONに変わるやり口を発明すべきですが、XMLよりはいいですね。&lt;/p&gt;

&lt;p&gt;それはさておき、面白そうなパラメータたち。並列バッチやジョブリリース時のタスクなど、今回使っていないものもまだまだあります。応用版はまたの機会に。&lt;/p&gt;

&lt;p&gt;ではスケジュールジョブをAzure BatchにCLIで送り込みます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;azure batch job-schedule create -f ./create_jobsched.json -u https://yourendpoint.location.batch.azure.com -a yourbatchaccount -k yourbatchaccountkey
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上です。あとはAzureにお任せです。4時間に1回、アメリカ国歌の単語を数える刺身タンポポなジョブですが、コツコツいきましょう。&lt;/p&gt;

&lt;h2 id=&#34;azure-automationとの使い分け&#34;&gt;Azure Automationとの使い分け&lt;/h2&gt;

&lt;p&gt;Azure Automationを使っても、ジョブの定期実行はできます。大きな違いは、PowerShellの要否と並列実行フレームワークの有無です。Azure AutomationはPowerShell前提ですが、Azure BatchはPowerShellに馴染みのない人でも使うことができます。また、今回は触れませんでしたが、Azure Batchは並列バッチ、オートスケールなど、バッチ処理に特化した機能を提供していることが特長です。うまく使い分けましょう。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azure Linux VMのディスク利用料節約Tips</title>
      <link>https://ToruMakabe.github.io/post/azure_pageblob_billable_linux/</link>
      <pubDate>Thu, 21 Apr 2016 21:30:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_pageblob_billable_linux/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;定義領域全てが課金されるわけではありません&#34;&gt;定義領域全てが課金されるわけではありません&lt;/h2&gt;

&lt;p&gt;AzureのIaaSでは、VMに接続するディスクとしてAzure StorageのPage Blobを使います。Page Blobは作成時に容量を定義しますが、課金対象となるのは、実際に書き込んだ領域分のみです。たとえば10GBytesのVHD Page Blobを作ったとしても、1GBytesしか書き込んでいなければ、課金対象は1GBytesです。&lt;/p&gt;

&lt;p&gt;なお、Premium Storageは例外です。&lt;a href=&#34;https://azure.microsoft.com/ja-jp/pricing/details/storage/&#34;&gt;FAQ&lt;/a&gt;を確認してみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;仮想マシンに空の 100 GB ディスクを接続した場合、100 GB 全体に対する料金が請求されますか? それとも使用したストレージ領域の分だけが請求されますか?

空の 100 GB ディスクが Premium Storage アカウントによって保持されている場合、P10 (128 GB) ディスクの料金が課金されます。その他の種類の Storage アカウントが使用されている場合、割り当てられたディスク サイズに関わらず、ディスクに書き込まれたデータを保存するために使用しているストレージ領域分のみ請求されます。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;詳細な定義は、以下で。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blogs.msdn.microsoft.com/windowsazurestorage/2010/07/08/understanding-windows-azure-storage-billing-bandwidth-transactions-and-capacity/&#34;&gt;Understanding Windows Azure Storage Billing – Bandwidth, Transactions, and Capacity&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;書き込み方はosやファイルシステム次第&#34;&gt;書き込み方はOSやファイルシステム次第&lt;/h2&gt;

&lt;p&gt;じゃあ、OSなりファイルシステムが、実際にどのタイミングでディスクに書き込むのか、気になりますね。実データの他に管理情報、メタデータがあるので、特徴があるはずです。Linuxで検証してみましょう。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RHEL 7.2 on Azure&lt;/li&gt;
&lt;li&gt;XFS &amp;amp; Ext4&lt;/li&gt;
&lt;li&gt;10GBytesのPage Blobの上にファイルシステムを作成&lt;/li&gt;
&lt;li&gt;mkfsはデフォルト&lt;/li&gt;
&lt;li&gt;mountはデフォルトとdiscardオプションありの2パターン&lt;/li&gt;
&lt;li&gt;MD、LVM構成にしない&lt;/li&gt;
&lt;li&gt;以下のタイミングで課金対象容量を確認

&lt;ul&gt;
&lt;li&gt;Page BlobのVMアタッチ時&lt;/li&gt;
&lt;li&gt;ファイルシステム作成時&lt;/li&gt;
&lt;li&gt;マウント時&lt;/li&gt;
&lt;li&gt;約5GBytesのデータ書き込み時 (ddで/dev/zeroをbs=1M、count=5000で書き込み)&lt;/li&gt;
&lt;li&gt;5GBytesのファイル削除時&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;課金対象容量は、以下のPowerShellで取得します。リファレンスは&lt;a href=&#34;https://gallery.technet.microsoft.com/scriptcenter/Get-Billable-Size-of-32175802&#34;&gt;ここ&lt;/a&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$Blob = Get-AzureStorageBlob yourDataDisk.vhd -Container vhds -Context $Ctx

$blobSizeInBytes = 124 + $Blob.Name.Length * 2

$metadataEnumerator = $Blob.ICloudBlob.Metadata.GetEnumerator()
while ($metadataEnumerator.MoveNext())
{
    $blobSizeInBytes += 3 + $metadataEnumerator.Current.Key.Length + $metadataEnumerator.Current.Value.Length
}

$Blob.ICloudBlob.GetPageRanges() | 
    ForEach-Object { $blobSizeInBytes += 12 + $_.EndOffset - $_.StartOffset }

return $blobSizeInBytes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ストレージコンテキストの作り方は&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/storage-powershell-guide-full/&#34;&gt;ここ&lt;/a&gt;を参考にしてください。&lt;/p&gt;

&lt;h2 id=&#34;結果&#34;&gt;結果&lt;/h2&gt;

&lt;h3 id=&#34;xfs&#34;&gt;XFS&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;　確認タイミング　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　課金対象容量(Bytes)　&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Page BlobのVMアタッチ時&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;960&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイルシステム作成時&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10,791,949&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;マウント時&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10,791,949&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;5GBytesのデータ書き込み時&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5,253,590,051&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;5Gbytesのファイル削除時&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5,253,590,051&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;5Gbytesのファイル削除時 (discard)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10,710,029&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;ext4&#34;&gt;Ext4&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;　確認タイミング　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　課金対象容量(Bytes)　&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Page BlobのVMアタッチ時&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;960&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイルシステム作成時&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;138,683,592&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;マウント時&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;306,451,689&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;5GBytesのデータ書き込み時&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5,549,470,887&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;5Gbytesのファイル削除時&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5,549,470,887&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;5Gbytesのファイル削除時 (discard)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;306,586,780&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;この結果から、以下のことがわかります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;10GBytesのBlobを作成しても、全てが課金対象ではない&lt;/li&gt;
&lt;li&gt;当然だが、ファイルシステムによってメタデータの書き方が違う、よって書き込み容量も異なる&lt;/li&gt;
&lt;li&gt;discardオプションなしでマウントすると、ファイルを消しても課金対象容量は減らない

&lt;ul&gt;
&lt;li&gt;OSがPage Blobに&amp;rdquo;消した&amp;rdquo;と伝えないから&lt;/li&gt;
&lt;li&gt;discardオプションにてSCSI UNMAPがPage Blobに伝えられ、領域は解放される(課金対象容量も減る)&lt;/li&gt;
&lt;li&gt;discardオプションはリアルタイムであるため便利。でも性能影響があるため、実運用ではバッチ適用(fstrim)が&lt;a href=&#34;https://access.redhat.com/documentation/ja-JP/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/ch02s05.html&#34;&gt;おすすめ&lt;/a&gt;

&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;知っているとコスト削減に役立つTipsでした。ぜひ運用前には、利用予定のファイルシステムやオプションで、事前に検証してみてください。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>AzureとDockerでDeep Learning(CNTK)環境をサク作する</title>
      <link>https://ToruMakabe.github.io/post/azure_docker_cntk/</link>
      <pubDate>Sun, 17 Apr 2016 10:30:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_docker_cntk/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;気軽に作って壊せる環境を作る&#34;&gt;気軽に作って壊せる環境を作る&lt;/h2&gt;

&lt;p&gt;Deep Learning環境設計のお手伝いをする機会に恵まれまして。インフラおじさんはDeep Learningであれこれする主役ではないのですが、ちょっとは中身を理解しておきたいなと思い、環境作ってます。&lt;/p&gt;

&lt;p&gt;試行錯誤するでしょうから、萎えないようにデプロイは自動化します。&lt;/p&gt;

&lt;h2 id=&#34;方針&#34;&gt;方針&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;インフラはAzure Resource Manager Templateでデプロイする

&lt;ul&gt;
&lt;li&gt;Linux (Ubuntu 14.04) VM, 仮想ネットワーク/ストレージ関連リソース&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;CNTKをビルド済みのdockerリポジトリをDocker Hubに置いておく

&lt;ul&gt;
&lt;li&gt;Dockerfileの元ネタは&lt;a href=&#34;https://github.com/Microsoft/CNTK/tree/master/Tools/docker&#34;&gt;ここ&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;GPUむけもあるけどグッと我慢、今回はCPUで&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Docker Hub上のリポジトリは &lt;a href=&#34;https://hub.docker.com/r/torumakabe/cntk-cpu/&#34;&gt;torumakabe/cntk-cpu&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ARM TemplateデプロイでVM Extensionを仕込んで、上物のセットアップもやっつける

&lt;ul&gt;
&lt;li&gt;docker extensionでdocker engineを導入&lt;/li&gt;
&lt;li&gt;custom script extensionでdockerリポジトリ(torumakabe/cntk-cpu)をpull&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;VMにログインしたら即CNTKを使える、幸せ&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;使い方&#34;&gt;使い方&lt;/h2&gt;

&lt;p&gt;Azure CLIでARM Templateデプロイします。WindowsでもMacでもLinuxでもOK。&lt;/p&gt;

&lt;p&gt;リソースグループを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;C:\Work&amp;gt; azure group create CNTK -l &amp;quot;Japan West&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ARMテンプレートの準備をします。テンプレートはGithubに置いておきました。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ToruMakabe/CNTK/blob/master/deploy_singlenode/azuredeploy.json&#34;&gt;azuredeploy.json&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;編集不要です&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ToruMakabe/CNTK/blob/master/deploy_singlenode/azuredeploy.parameters.sample.json&#34;&gt;azuredeploy.parameters.json&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;テンプレートに直で書かきたくないパラメータです&lt;/li&gt;
&lt;li&gt;fileUris、commandToExecute以外は、各々で&lt;/li&gt;
&lt;li&gt;fileUris、commandToExecuteもGist読んでdocker pullしているだけなので、お好みで変えてください&lt;/li&gt;
&lt;li&gt;ファイル名がazuredeploy.parameters.&amp;ldquo;sample&amp;rdquo;.jsonなので、以降の手順では&amp;rdquo;sample&amp;rdquo;を外して読み替えてください
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;うし、デプロイ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;C:\Work&amp;gt; azure group deployment create CNTK dep01 -f .\azuredeploy.json -e .\azuredeploy.parameters.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;10分くらい待つと、できあがります。VMのパブリックIPを確認し、sshしましょう。&lt;/p&gt;

&lt;p&gt;docker engine入ってますかね。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yourname@yournamecntkr0:~$ docker -v
Docker version 1.11.0, build 4dc5990
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CNTKビルド済みのdockerイメージ、pullできてますかね。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yourname@yournamecntkr0:~$ docker images
REPOSITORY            TAG                 IMAGE ID            CREATED             SIZE
yournamebe/cntk-cpu   latest              9abab8a76543        9 hours ago         2.049 GB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;問題なし。ではエンジョイ Deep Learning。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yourname@yournamecntkr0:~$ docker run -it torumakabe/cntk-cpu
root@a1234bc5d67d:/cntk#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CNTKの利用例は、&lt;a href=&#34;https://github.com/Microsoft/CNTK/tree/master/Examples&#34;&gt;Github&lt;/a&gt;にあります。&lt;/p&gt;

&lt;h2 id=&#34;今後の展開&#34;&gt;今後の展開&lt;/h2&gt;

&lt;p&gt;インフラおじさんは、最近Linuxむけに&lt;a href=&#34;https://azure.microsoft.com/ja-jp/blog/announcing-support-of-linux-vm-on-azure-batch-service/&#34;&gt;Previewがはじまった&lt;/a&gt;Azure Batchと、このエントリで使った仕掛けを組み合わせて、大規模並列Deep Learning環境の自動化と使い捨て化を企んでいます。&lt;/p&gt;

&lt;p&gt;これだけ簡単に再現性ある環境を作れるなら、常時インフラ起動しておく必要ないですものね。使い捨てでいいです。&lt;/p&gt;

&lt;p&gt;もちろんdockerやGPUまわりの性能など別の課題にぶつかりそうですが、人間がどれだけ楽できるかとのトレードオフかと。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azureの監査ログアラートからWebhookの流れで楽をする</title>
      <link>https://ToruMakabe.github.io/post/azure_auditlog_alert/</link>
      <pubDate>Wed, 06 Apr 2016 17:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_auditlog_alert/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;監査ログからアラートを上げられるようになります&#34;&gt;監査ログからアラートを上げられるようになります&lt;/h2&gt;

&lt;p&gt;Azureの監査ログからアラートを上げる機能のプレビューが&lt;a href=&#34;https://azure.microsoft.com/ja-jp/blog/new-features-for-azure-alerts-and-autoscale/&#34;&gt;はじまりました&lt;/a&gt;。これ、地味ですが便利な機能です。日々の運用に効きます。&lt;/p&gt;

&lt;h2 id=&#34;どんな風に使えるか&#34;&gt;どんな風に使えるか&lt;/h2&gt;

&lt;p&gt;ルールに合致した監査ログが生成された場合、メール通知とWebhookによる自動アクションができます。可能性無限大です。&lt;/p&gt;

&lt;p&gt;たとえば、「特定のリソースグループにVMが生成された場合、そのVMに対し強制的にログ収集エージェントをインストールし、ログを集める」なんてことができます。&lt;/p&gt;

&lt;p&gt;これは「生産性を上げるため、アプリ開発チームにVMの生成は委任したい。でもセキュリティなどの観点から、ログは集めておきたい」なんてインフラ担当/Opsの課題に効きます。開発チームに「VM生成時には必ず入れてね」とお願いするのも手ですが、やはり人間は忘れる生き物ですので、自動で適用できる仕組みがあるとうれしい。&lt;/p&gt;

&lt;p&gt;これまでは監視用のVMを立てて、「新しいVMがあるかどうか定期的にチェックして、あったらエージェントを叩き込む」なんてことをしていたわけですが、もうそのVMは不要です。定期的なチェックも要りません。アラートからアクションを実現する仕組みを、Azureがマネージドサービスとして提供します。&lt;/p&gt;

&lt;h2 id=&#34;実装例&#34;&gt;実装例&lt;/h2&gt;

&lt;p&gt;例としてこんな仕組みを作ってみましょう。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;西日本リージョンのリソースグループ&amp;rdquo;dev&amp;rdquo;にVMが作成されたら、自動的にメール通知とWebhookを実行&lt;/li&gt;
&lt;li&gt;WebhookでAzure AutomationのRunbook Jobを呼び出し、OMS(Operations Management Suite)エージェントを該当のVMにインストール、接続先OMSを設定する&lt;/li&gt;
&lt;li&gt;OMSでログ分析&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;準備&#34;&gt;準備&lt;/h2&gt;

&lt;p&gt;以下の準備ができているか確認します。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Azure Automation向けADアプリ、サービスプリンシパル作成&lt;/li&gt;
&lt;li&gt;サービスプリンシパルへのロール割り当て&lt;/li&gt;
&lt;li&gt;Azure Automationのアカウント作成&lt;/li&gt;
&lt;li&gt;Azure Automation Runbook実行時ログインに必要な証明書や資格情報などの資産登録&lt;/li&gt;
&lt;li&gt;Azure Automation Runbookで使う変数資産登録 (Runbook内でGet-AutomationVariableで取得できます。暗号化もできますし、コードに含めるべきでない情報は、登録しましょう。後述のサンプルではログイン関連情報、OMS関連情報を登録しています)&lt;/li&gt;
&lt;li&gt;OMSワークスペースの作成&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;もしAutomationまわりの作業がはじめてであれば、下記記事を参考にしてください。とてもわかりやすい。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://qiita.com/sengoku/items/1c3994ac8a2f0f0e88c5&#34;&gt;勤務時間中だけ仮想マシンを動かす（スケジュールによる自動起動・停止）&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;azure-automation側の仕掛け&#34;&gt;Azure Automation側の仕掛け&lt;/h2&gt;

&lt;p&gt;先にAutomationのRunbookを作ります。アラート設定をする際、RunbookのWebhook URLが必要になるので。&lt;/p&gt;

&lt;p&gt;ちなみにわたしは証明書を使ってログインしています。資格情報を使う場合はログインまわりのコードを読み替えてください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;param ( 
    [object]$WebhookData          
)

if ($WebhookData -ne $null) {  
    $WebhookName    =   $WebhookData.WebhookName
    $WebhookBody    =   $WebhookData.RequestBody  
    $WebhookBody = (ConvertFrom-Json -InputObject $WebhookBody)

    $AlertContext = [object]$WebhookBody.context

    $SPAppID = Get-AutomationVariable -Name &#39;SPAppID&#39;
    $Tenant = Get-AutomationVariable -Name &#39;TenantID&#39;
    $OMSWorkspaceId = Get-AutomationVariable -Name &#39;OMSWorkspaceId&#39;
    $OMSWorkspaceKey = Get-AutomationVariable -Name &#39;OMSWorkspaceKey&#39;
    $CertificationName = Get-AutomationVariable -Name &#39;CertificationName&#39;
    $Certificate = Get-AutomationCertificate -Name $CertificationName
    $CertThumbprint = ($Certificate.Thumbprint).ToString()    

    $null = Login-AzureRmAccount -ServicePrincipal -TenantId $Tenant -CertificateThumbprint $CertThumbprint -ApplicationId $SPAppID   

    $resourceObj = Get-AzureRmResource -ResourceId $AlertContext.resourceId
    $VM = Get-AzureRmVM -Name $resourceObj.Name -ResourceGroupName $resourceObj.ResourceGroupName

    $Settings = @{&amp;quot;workspaceId&amp;quot; = &amp;quot;$OMSWorkspaceId&amp;quot;}
    $ProtectedSettings = @{&amp;quot;workspaceKey&amp;quot; = &amp;quot;$OMSWorkspaceKey&amp;quot;}

    if ($VM.StorageProfile.OsDisk.OsType -eq &amp;quot;Linux&amp;quot;) {  
        Set-AzureRmVMExtension -ResourceGroupName $AlertContext.resourceGroupName -Location $VM.Location -VMName $VM.Name -Name &amp;quot;OmsAgentForLinux&amp;quot; -Publisher &amp;quot;Microsoft.EnterpriseCloud.Monitoring&amp;quot; -ExtensionType &amp;quot;OmsAgentForLinux&amp;quot; -TypeHandlerVersion &amp;quot;1.0&amp;quot; -Settings $Settings -ProtectedSettings $ProtectedSettings;
    }
    elseif ($VM.StorageProfile.OsDisk.OsType -eq &amp;quot;Windows&amp;quot;)
    {
        Set-AzureRmVMExtension -ResourceGroupName $AlertContext.resourceGroupName -Location $VM.Location -VMName $VM.Name -Name &amp;quot;MicrosoftMonitoringAgent&amp;quot; -Publisher &amp;quot;Microsoft.EnterpriseCloud.Monitoring&amp;quot; -ExtensionType &amp;quot;MicrosoftMonitoringAgent&amp;quot; -TypeHandlerVersion &amp;quot;1.0&amp;quot; -Settings $Settings -ProtectedSettings $ProtectedSettings;
    }
    else
    {
        Write-Error &amp;quot;Unknown OS Type.&amp;quot;
    }
}
else 
{
    Write-Error &amp;quot;This runbook is meant to only be started from a webhook.&amp;quot; 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Runbookができたら、Webhookを作ります。詳しくは&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/automation-webhooks/&#34;&gt;こちら&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;WebhookのURLを控えておいてください。&lt;/p&gt;

&lt;h2 id=&#34;azure-監査ログアラート側の仕掛け&#34;&gt;Azure 監査ログアラート側の仕掛け&lt;/h2&gt;

&lt;p&gt;Powershellでアラートルールを作ります。実行アカウントの権限に気をつけてください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\work&amp;gt; $actionEmail = New-AzureRmAlertRuleEmail -CustomEmail yourname@example.com

PS C:\work&amp;gt; $actionWebhook = New-AzureRmAlertRuleWebhook -ServiceUri https://abcdefgh.azure-automation.net/webhooks?token=your_token

PS C:\work&amp;gt; Add-AzureRmLogAlertRule -Name createdVM -Location &amp;quot;Japan West&amp;quot; -ResourceGroup dev -OperationName Microsoft.Compute/virtualMachines/write -Status Succeeded  -SubStatus Created -TargetResourceGroup dev -Actions $actionEmail,$actionWebhook
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上。これで&amp;rdquo;dev&amp;rdquo;リソースグループにVMが作られた場合、自動でOMSエージェントがインストールされ、ログ収集がはじまります。&lt;/p&gt;

&lt;p&gt;なお、メールも飛んできますので、うっとおしくなったらメール通知はアクションから外すか、ルールでさばいてくださいね。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>書評: Site Reliability Engineering</title>
      <link>https://ToruMakabe.github.io/post/bookreview_site_reliability_engineering/</link>
      <pubDate>Sun, 27 Mar 2016 20:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/bookreview_site_reliability_engineering/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;英語だけどぜひ読んでほしい&#34;&gt;英語だけどぜひ読んでほしい&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://www.amazon.co.jp/Site-Reliability-Engineering-Production-Systems-ebook/dp/B01DCPXKZ6/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;amp;qid=1459069692&amp;amp;sr=8-1&#34;&gt;Site Reliability Engineering: How Google Runs Production Systems&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;参考になったのでご紹介。Googleのインフラ/Ops系技術チームの働き方や考え方を題材にした本です。GoogleのSREについては断片的に知っていたのですが、まとめて読むと違いますね。背景やストーリーがあって、理解しやすいです。&lt;/p&gt;

&lt;p&gt;共感できるネタがどんどん繰り出されるので、一気読みしました。読み込みが浅いところもあったので、改めて読む予定。&lt;/p&gt;

&lt;p&gt;以下、印象に残ったこと。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Site Reliability Engineering teamは、インフラ/Ops担当であるが、Unix内部やネットワークなどインフラの知見を持つソフトウェアエンジニアの集団。自分たちのオペレーションを効率的に、迅速に、確実にするために、コードを書く。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;インシデント対応、問い合わせ対応、手作業は仕事の50%に収まるように調整する。残りの時間は自分たちの仕事をより良く、楽にするために、コードを書く。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;日々のリアクティブな活動に忙殺されるインフラ/Ops担当はどうしても減点評価になりがちだが、仕事の半分がプロアクティブな活動であり、成果を加点評価できる。昇格、昇給の根拠になりやすい。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;アプリ/製品チームとSREチームは&amp;rdquo;Error Budget&amp;rdquo;を定義、共有する。これは四半期ごとに定義される、サービスレベル目標である。ユーザがサービスを使えなくなると、その時間が、このError Budgetから取り崩されていく。Budgetが残り少なくなると、リスクを伴うデプロイなどは控える。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;インフラ/Ops担当は「サービスを少しでもダウンさせたら悪」となりがちだが、サービスごとにアプリ/製品チームとSREチームがError Budgetを共有することで、利害関係を一致できる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Error Budgetの大きさはサービスごとに異なり、定義は製品チームの責任。当然Error Budgetが少ない = サービスレベルが高い = コストがかかる ので、製品チームはいたずらに高いサービスレベルを定義しない。Google Apps for WorkとYoutubeのError Budgetは異なる。Appsはサービスレベル重視であり、Youtubeは迅速で頻繁な機能追加を重視する。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;SLA違反など、重大な障害では&amp;rdquo;Postmortem(過激だが死体解剖の意)&amp;ldquo;を作成し、失敗から学ぶ。客観的に、建設的に。誰かや何かを責めるためにやるわけではない。マサカリ投げない。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;他の産業から学ぶ。製造業のビジネス継続プラン、国防のシミュレーションや演習、通信業の輻輳対策など。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;もう一回読んだら、また違う発見があるんじゃないかと。&lt;/p&gt;

&lt;h2 id=&#34;自分ごととして読みたい&#34;&gt;自分ごととして読みたい&lt;/h2&gt;

&lt;p&gt;今後の働き方や所属組織に行き詰まりを感じているインフラ/Ops技術者に、参考になるネタが多いと思います。&lt;/p&gt;

&lt;p&gt;DevOpsムーブメントが来るか来ないかという今、Opsとしてのスタンスを考え直すのにも、いいかもしれません。&lt;/p&gt;

&lt;p&gt;もちろん、Googleの圧倒的物量、成長スピードゆえのミッションと働き方である事は否定しません。でも、自分とは無関係、と無視するにはもったいないです。&lt;/p&gt;

&lt;p&gt;なお、このSREチーム、できてから10年以上たっているそうです。それだけ持続できるということは、そこに何か本質的な価値があるのではないでしょうか。&lt;/p&gt;

&lt;p&gt;オススメです。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azure &amp; Terraform Tips (ARM対応 2016春版)</title>
      <link>https://ToruMakabe.github.io/post/azure_terraform_earlyphase_tips/</link>
      <pubDate>Fri, 25 Mar 2016 22:50:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_terraform_earlyphase_tips/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;俺の屍を越えていけ&#34;&gt;俺の屍を越えていけ&lt;/h2&gt;

&lt;p&gt;今週リリースされたTerraform v0.6.14で、Azure Resource Manager ProviderのリソースにVMとテンプレートデプロイが&lt;a href=&#34;https://github.com/hashicorp/terraform/blob/v0.6.14/CHANGELOG.md&#34;&gt;追加&lt;/a&gt;されました。この週末お楽しみ、という人も多いかもしれません。&lt;/p&gt;

&lt;p&gt;小生、v0.6.14以前から触っていたこともあり、土地勘があります。そこで現時点でのTipsをいくつかご紹介します。&lt;/p&gt;

&lt;h2 id=&#34;この3つは触る前から意識しよう&#34;&gt;この3つは触る前から意識しよう&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;ARMテンプレートリソースは分離して使う&lt;/li&gt;
&lt;li&gt;リソース競合したら依存関係を定義する&lt;/li&gt;
&lt;li&gt;公開鍵認証SSH指定でエラーが出ても驚かない&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;1-armテンプレートリソースは分離して使う&#34;&gt;1. ARMテンプレートリソースは分離して使う&lt;/h2&gt;

&lt;p&gt;v0.6.14で、リソース&lt;a href=&#34;https://www.terraform.io/docs/providers/azurerm/r/template_deployment.html&#34;&gt;&amp;ldquo;azurerm_template_deployment&amp;rdquo;&lt;/a&gt;が追加されました。なんとARMテンプレートを、Terraformの定義ファイル内にインラインで書けます。&lt;/p&gt;

&lt;p&gt;でも、現時点の実装では、おすすめしません。&lt;/p&gt;

&lt;h3 id=&#34;armテンプレートのデプロイ機能とterraformで作ったリソースが不整合を起こす&#34;&gt;ARMテンプレートのデプロイ機能とTerraformで作ったリソースが不整合を起こす&lt;/h3&gt;

&lt;p&gt;避けるべきなのは&amp;rdquo;Complete(完全)&amp;ldquo;モードでのARMテンプレートデプロイです。なぜなら完全モードでは、ARM リソースマネージャーは次の動きをするからです。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/resource-group-template-deploy/&#34;&gt;リソース グループに存在するが、テンプレートに指定されていないリソースを削除します&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;つまり、ARMテンプレートで作ったリソース以外、Terraform担当部分を消しにいきます。恐怖! デプロイ vs デプロイ!!。リソースグループを分ければ回避できますが、リスク高めです。&lt;/p&gt;

&lt;h3 id=&#34;タイムアウトしがち&#34;&gt;タイムアウトしがち&lt;/h3&gt;

&lt;p&gt;それでもTerraformの外でARMテンプレートデプロイは継続します。成功すれば結果オーライですが&amp;hellip;Terraform上はエラーが残ります。「ああそれ無視していいよ」ではあるのですが、&lt;a href=&#34;https://ja.wikipedia.org/wiki/%E5%89%B2%E3%82%8C%E7%AA%93%E7%90%86%E8%AB%96&#34;&gt;割れ窓理論&lt;/a&gt;的によろしくないです。&lt;/p&gt;

&lt;h3 id=&#34;せっかくのリソースグラフを活用できない&#34;&gt;せっかくのリソースグラフを活用できない&lt;/h3&gt;

&lt;p&gt;Terraformはグラフ構造で賢くリソース間の依存関係を管理し、整合性を維持しています。サクサク apply &amp;amp; destroyできるのもそれのおかげです。ARMテンプレートでデプロイしたリソースはそれに入れられないので、もったいないです。&lt;/p&gt;

&lt;h3 id=&#34;読みづらい&#34;&gt;読みづらい&lt;/h3&gt;

&lt;p&gt;Terraform DSLにJSONが混ざって読みにくいです。Terraform DSLを使わない手もありますが、それでいいのかという話です。&lt;/p&gt;

&lt;p&gt;それでも&amp;rdquo;terraformコマンドに操作を統一したい&amp;rdquo;など、どうしても使いたい人は、ARMテンプレート実行部は管理も実行も分離した方がいいと思います。&lt;/p&gt;

&lt;h2 id=&#34;2-リソース競合したら依存関係を定義する&#34;&gt;2. リソース競合したら依存関係を定義する&lt;/h2&gt;

&lt;p&gt;Terraformはリソース間の依存関係を明示する必要がありません。ですが、行き届かないこともあります。その場合は&lt;a href=&#34;https://www.terraform.io/intro/getting-started/dependencies.html&#34;&gt;&amp;ldquo;depends_on&amp;rdquo;&lt;/a&gt;で明示してあげましょう。&lt;/p&gt;

&lt;p&gt;例えば、&lt;a href=&#34;http://torumakabe.github.io/post/azure_terraform_429_workaround/&#34;&gt;以前のエントリ&lt;/a&gt;で紹介した下記の問題。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error applying plan:

1 error(s) occurred:
azurerm_virtual_network.vnet1: autorest:DoErrorUnlessStatusCode 429 PUT https://management.azure.com/subscriptions/my_subscription_id/resourceGroups/mygroup/providers/Microsoft.Network/virtualnetworks/vnet1?api-version=2015-06-15 failed with 429


Cannot proceed with operation since resource /subscriptions/GUID/resourceGroups/xxxx/providers/Microsoft.Network/networkSecurityGroups/yyy allocated to resource /subscriptions/GUID/resourceGroups/***/providers/Microsoft.Network/virtualNetworks/yyy is not in Succeeded state. Resource is in Updating state and the last operation that updated/is updating the resource is PutSecurityRuleOperation. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;HTTPステータスコード429(Too many requests)が返ってきているのでわかりにくいですが、実態はセキュリティーグループリソースの取り合いです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;サブネットリソース作成側: サブネットを新規作成し、セキュリティーグループを紐付けたい&lt;/li&gt;
&lt;li&gt;セキュリティーグループルール作成側: ルールをセキュリティーグループに登録したい(更新処理)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;この2つが並行してセキュリティーグループを取り合うので、高確率でエラーになります。セキュリティーグループルールはリソースの新規作成でなく、セキュリティーグループの更新処理であるため「リソースを&lt;strong&gt;作成したら/存在したら&lt;/strong&gt;次にすすむ」というTerraformのグラフでうまく表現できないようです。&lt;/p&gt;

&lt;p&gt;そのような場合、明示的に依存関係を&amp;rdquo;depends_on&amp;rdquo;で定義します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Create a frontend subnet
# &amp;quot;depends_on&amp;quot; arg is a workaround to avoid conflict with updating NSG rules 
resource &amp;quot;azurerm_subnet&amp;quot; &amp;quot;frontend&amp;quot; {
    name = &amp;quot;frontend&amp;quot;
    resource_group_name = &amp;quot;${var.resource_group_name}&amp;quot;
    virtual_network_name = &amp;quot;${azurerm_virtual_network.vnet1.name}&amp;quot;
    address_prefix = &amp;quot;${var.vnet1_frontend_address_prefix}&amp;quot;
    network_security_group_id = &amp;quot;${azurerm_network_security_group.frontend.id}&amp;quot;
    depends_on = [
        &amp;quot;azurerm_network_security_rule.fe_web80&amp;quot;,
        &amp;quot;azurerm_network_security_rule.fe_web443&amp;quot;,
        &amp;quot;azurerm_network_security_rule.fe_ssh&amp;quot;
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これでサブネット作成処理は、セキュリティーグループルール登録完了まで、作成処理開始を待ちます。美しくないですが、当面の回避策です。&lt;/p&gt;

&lt;h2 id=&#34;3-公開鍵認証ssh指定でエラーが出ても驚かない&#34;&gt;3. 公開鍵認証SSH指定でエラーが出ても驚かない&lt;/h2&gt;

&lt;p&gt;TerraformはLinux VMの定義で、公開鍵認証SSHを指定できます。こんな感じで。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;os_profile_linux_config {
    disable_password_authentication = true
    ssh_keys {
        path = &amp;quot;/home/${var.adminuser}/.ssh/authorized_keys&amp;quot;
        key_data = &amp;quot;${file(&amp;quot;/Users/you/.ssh/yourkey.pem&amp;quot;)}&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;が、エラーが返ってきます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[DEBUG] Error setting Virtual Machine Storage OS Profile Linux Configuration: &amp;amp;errors.errorString{s:&amp;quot;Invalid address to set: []string{\&amp;quot;os_profile_linux_config\&amp;quot;, \&amp;quot;12345678\&amp;quot;, \&amp;quot;ssh_keys\&amp;quot;}&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;残念ながら、Terraformが使っているAzure SDK(Golang)のバグです。&lt;/p&gt;

&lt;p&gt;妥当性チェックのエラーで、実際にはキーの登録はできているようです。私は何度か試行してすべて公開鍵SSHログインに成功しています。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/hashicorp/terraform/issues/5793&#34;&gt;Issueとして認識&lt;/a&gt;されていますので、修正を待ちましょう。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azure &amp; Terraform エラーコード429の対処法</title>
      <link>https://ToruMakabe.github.io/post/azure_terraform_429_workaround/</link>
      <pubDate>Wed, 23 Mar 2016 13:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_terraform_429_workaround/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;terraformer増加に備えて&#34;&gt;Terraformer増加に備えて&lt;/h2&gt;

&lt;p&gt;2016/3/21にリリースされたTerraform v0.6.14で、Azure Resource Manager ProviderのリソースにVMとテンプレートデプロイが&lt;a href=&#34;https://github.com/hashicorp/terraform/blob/v0.6.14/CHANGELOG.md&#34;&gt;追加&lt;/a&gt;されました。待っていた人も多いのではないでしょうか。&lt;/p&gt;

&lt;p&gt;追って&lt;a href=&#34;https://www.hashicorp.com/partners.html#sipart&#34;&gt;Hashicorp認定パートナー&lt;/a&gt;のクリエーションラインさんから導入・サポートサービスが&lt;a href=&#34;http://www.creationline.com/lab/13268&#34;&gt;アナウンス&lt;/a&gt;されましたし、今後AzureをTerraformでコントロールしようという需要は増えそうです。&lt;/p&gt;

&lt;h2 id=&#34;エラーコード429&#34;&gt;エラーコード429&lt;/h2&gt;

&lt;p&gt;さて、TerraformでAzureをいじっていると、下記のようなエラーに出くわすことがあります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error applying plan:

1 error(s) occurred:
azurerm_virtual_network.vnet1: autorest:DoErrorUnlessStatusCode 429 PUT https://management.azure.com/subscriptions/my_subscription_id/resourceGroups/mygroup/providers/Microsoft.Network/virtualnetworks/vnet1?api-version=2015-06-15 failed with 429
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;autorestがステータスコード429をキャッチしました。&lt;a href=&#34;https://tools.ietf.org/html/rfc6585#section-4&#34;&gt;RFC上で429は&lt;/a&gt;&amp;ldquo;Too many requests&amp;rdquo;です。何かが多すぎたようです。&lt;/p&gt;

&lt;h2 id=&#34;対処法&#34;&gt;対処法&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;もう一度applyしてください&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;冪等性最高。冪等性なんていらない、という人もいますが、こういうときはありがたい。Terraformが作成に失敗したリソースのみ再作成します。&lt;/p&gt;

&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;

&lt;p&gt;エラーになった背景ですが、2つの可能性があります。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;APIリクエスト数上限に達した&lt;/li&gt;
&lt;li&gt;リソースの作成や更新に時間がかかっており、Azure側で処理を中断した&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;1-apiリクエスト数上限に達した&#34;&gt;1. APIリクエスト数上限に達した&lt;/h3&gt;

&lt;p&gt;Azure Resource Manager APIには時間当たりのリクエスト数制限があります。読み取り 15,000/時、書き込み1,200/時です。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/azure-subscription-service-limits/&#34;&gt;Azure サブスクリプションとサービスの制限、クォータ、制約&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Terraformは扱うリソースごとにAPIをコールするので、数が多い環境で作って壊してをやると、この上限にひっかかる可能性があります。&lt;/p&gt;

&lt;p&gt;長期的な対処として、Terraformにリトライ/Exponential Backoffロジックなどを実装してもらうのがいいのか、このままユーザ側でシンプルにリトライすべきか、悩ましいところです。&lt;/p&gt;

&lt;p&gt;ひとまずプロダクトの方針は確認したいので、Issueに質問を&lt;a href=&#34;https://github.com/hashicorp/terraform/issues/5704&#34;&gt;あげておきました&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;2-リソースの作成や更新に時間がかかっており-azure側で処理を中断した&#34;&gt;2. リソースの作成や更新に時間がかかっており、Azure側で処理を中断した&lt;/h3&gt;

&lt;p&gt;Terraform側ではエラーコードで判断するしかありませんが、Azureの監査ログで詳細が確認できます。&lt;/p&gt;

&lt;p&gt;わたしが経験したエラーの中に、こんなものがありました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Cannot proceed with operation since resource /subscriptions/GUID/resourceGroups/xxxx/providers/Microsoft.Network/networkSecurityGroups/yyy allocated to resource /subscriptions/GUID/resourceGroups/***/providers/Microsoft.Network/virtualNetworks/yyy is not in Succeeded state. Resource is in Updating state and the last operation that updated/is updating the resource is PutSecurityRuleOperation. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Too many requestsというよりは、リソースのアップデートが終わってないので先に進めない、という内容です。&lt;/p&gt;

&lt;p&gt;Too many requestsをどう解釈するかにもよりますが、ちょっと混乱しますね。この問題はFeedbackとして&lt;a href=&#34;https://feedback.azure.com/forums/34192--general-feedback/suggestions/13069563-better-http-status-code-instead-of-429&#34;&gt;あがっています&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;でも安心してください。&lt;strong&gt;もう一度applyしてください&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(2016/3/25 追記) 回避策を&lt;a href=&#34;http://torumakabe.github.io/post/azure_terraform_earlyphase_tips/&#34;&gt;別エントリ&lt;/a&gt;に書きました&lt;/strong&gt;&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>PackerとAnsibleでAzureのGolden Imageを作る(ARM対応)</title>
      <link>https://ToruMakabe.github.io/post/azure_packer_ansible_arm_sp/</link>
      <pubDate>Thu, 17 Mar 2016 23:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_packer_ansible_arm_sp/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;いつの間に&#34;&gt;いつの間に&lt;/h2&gt;

&lt;p&gt;ナイスな感じにイメージを作ってくれるPackerですが、いつの間にか&lt;a href=&#34;https://www.packer.io/docs/builders/azure.html&#34;&gt;Azure ARM対応のBuilder&lt;/a&gt;が出ておりました。0.10からかな。早く言ってください。&lt;/p&gt;

&lt;h2 id=&#34;ansible-localと組み合わせたサンプル&#34;&gt;ansible_localと組み合わせたサンプル&lt;/h2&gt;

&lt;p&gt;さっそく試してそつなく動くことを確認しました。サンプルを&lt;a href=&#34;https://github.com/ToruMakabe/Packer_Azure_Sample&#34;&gt;Githubにあげておきます&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;手の込んだ設定もできるように、Provisonerにansible_localを使うサンプルで。&lt;/p&gt;

&lt;h3 id=&#34;前準備&#34;&gt;前準備&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;リソースグループとストレージアカウントを作っておいてください。そこにイメージが格納されます。&lt;/li&gt;
&lt;li&gt;認証情報の類は外だしします。builder/variables.sample.jsonを参考にしてください。&lt;/li&gt;
&lt;li&gt;Packerの構成ファイルはOSに合わせて書きます。サンプルのbuilder/ubuntu.jsonはubuntuの例です。

&lt;ul&gt;
&lt;li&gt;Azure ARM BuilderはまだWindowsに対応していません。開発中とのこと。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ansibleはapache2をインストール、サービスEnableするサンプルにしました。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;サンプル&#34;&gt;サンプル&lt;/h3&gt;

&lt;p&gt;ubuntu.jsonはこんな感じです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;variables&amp;quot;: {
    &amp;quot;client_id&amp;quot;: &amp;quot;&amp;quot;,
    &amp;quot;client_secret&amp;quot;: &amp;quot;&amp;quot;,
    &amp;quot;resource_group&amp;quot;: &amp;quot;&amp;quot;,
    &amp;quot;storage_account&amp;quot;: &amp;quot;&amp;quot;,
    &amp;quot;subscription_id&amp;quot;: &amp;quot;&amp;quot;,
    &amp;quot;tenant_id&amp;quot;: &amp;quot;&amp;quot;
  },
  &amp;quot;builders&amp;quot;: [{
    &amp;quot;type&amp;quot;: &amp;quot;azure-arm&amp;quot;,

    &amp;quot;client_id&amp;quot;: &amp;quot;{{user `client_id`}}&amp;quot;,
    &amp;quot;client_secret&amp;quot;: &amp;quot;{{user `client_secret`}}&amp;quot;,
    &amp;quot;resource_group_name&amp;quot;: &amp;quot;{{user `resource_group`}}&amp;quot;,
    &amp;quot;storage_account&amp;quot;: &amp;quot;{{user `storage_account`}}&amp;quot;,
    &amp;quot;subscription_id&amp;quot;: &amp;quot;{{user `subscription_id`}}&amp;quot;,
    &amp;quot;tenant_id&amp;quot;: &amp;quot;{{user `tenant_id`}}&amp;quot;,

    &amp;quot;capture_container_name&amp;quot;: &amp;quot;images&amp;quot;,
    &amp;quot;capture_name_prefix&amp;quot;: &amp;quot;packer&amp;quot;,

    &amp;quot;image_publisher&amp;quot;: &amp;quot;Canonical&amp;quot;,
    &amp;quot;image_offer&amp;quot;: &amp;quot;UbuntuServer&amp;quot;,
    &amp;quot;image_sku&amp;quot;: &amp;quot;14.04.3-LTS&amp;quot;,

    &amp;quot;location&amp;quot;: &amp;quot;Japan West&amp;quot;,
    &amp;quot;vm_size&amp;quot;: &amp;quot;Standard_D1&amp;quot;
  }],
  &amp;quot;provisioners&amp;quot;: [{
    &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;,
      &amp;quot;scripts&amp;quot;: [
        &amp;quot;../script/ubuntu/provision.sh&amp;quot;
    ]
  },
  {
    &amp;quot;type&amp;quot;: &amp;quot;ansible-local&amp;quot;,
    &amp;quot;playbook_file&amp;quot;: &amp;quot;../ansible/baseimage.yml&amp;quot;,
    &amp;quot;inventory_file&amp;quot;: &amp;quot;../ansible/hosts&amp;quot;,
    &amp;quot;role_paths&amp;quot;: [
      &amp;quot;../ansible/roles/baseimage&amp;quot;
    ]
  },
  {
    &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;,
      &amp;quot;scripts&amp;quot;: [
        &amp;quot;../script/ubuntu/deprovision.sh&amp;quot;
    ]
  }]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;waagentによるde-provisionはansibleでもできるのですが、他OS対応も考えて、最後に追いshellしてます。他ファイルは&lt;a href=&#34;https://github.com/ToruMakabe/Packer_Azure_Sample&#34;&gt;Github&lt;/a&gt;でご確認を。&lt;/p&gt;

&lt;p&gt;これで手順書&amp;amp;目視&amp;amp;指差し確認でイメージ作るのを、やめられそうですね。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Terraform &amp; Azure デプロイ設計4原則</title>
      <link>https://ToruMakabe.github.io/post/azure_tf_fundamental_rules/</link>
      <pubDate>Wed, 09 Mar 2016 16:30:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_tf_fundamental_rules/</guid>
      <description></description>
      
      <content>

&lt;p&gt;注: 2018/1/8にサンプルを更新しました。更新エントリは&lt;a href=&#34;http://torumakabe.github.io/post/terraform_azure_sample_201801/&#34;&gt;こちら&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;情報がありそうでない&#34;&gt;情報がありそうでない&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://torumakabe.github.io/post/azure_tf_arm_sp/&#34;&gt;以前のエントリ&lt;/a&gt;で書いたとおり、TerraformでAzureへデプロイする方式をClassicからResource Managerへ移行しているところです。&lt;/p&gt;

&lt;p&gt;今後も継続して試行錯誤するとは思うのですが、ふらふらしないように原則を作りました。この手の情報はありそうでないので、参考になればと思いこのエントリを書いています。&lt;/p&gt;

&lt;p&gt;なお、考え方は他のクラウドやデプロイツールでも応用できるかと。&lt;/p&gt;

&lt;h2 id=&#34;4原則&#34;&gt;4原則&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;セキュリティファースト&lt;/li&gt;
&lt;li&gt;手順書をなくそう&lt;/li&gt;
&lt;li&gt;分割境界にこだわりすぎない&lt;/li&gt;
&lt;li&gt;早すぎる最適化は悪&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;なお、サンプルのTerraformファイル群を、&lt;a href=&#34;https://github.com/ToruMakabe/Terraform_Azure_Sample&#34;&gt;Githubに置いて&lt;/a&gt;おきました。&lt;/p&gt;

&lt;p&gt;今後ガラガラポンする可能性は大いにありますが、現時点ではこんな構造です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.
├── .gitignore
├── main.tf
├── availability_set
│   ├── avset_web.tf
│   ├── avset_db.tf
│   └── variables.tf
├── network
│   ├── sg_backend.tf
│   ├── sg_frontend.tf
│   ├── variables.tf
│   └── vnets.tf
├── storage
│   ├── storage_backend.tf
│   ├── storage_frontend.tf
│   └── variables.tf
└── terraform.tfvars
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Availability Setに対するVMのデプロイはTerraformの外でやっています。まだTerraformのAzure RM Providerにない、ということもありますが、VMの増減はアドホックだったり、別ツールを使いたいケースが多いので。&lt;/p&gt;

&lt;h2 id=&#34;1-セキュリティファースト&#34;&gt;1. セキュリティファースト&lt;/h2&gt;

&lt;p&gt;セキュリティはデザイン時に考慮すべき時代です。機密情報が漏れないように、また、身内がうっかりリソースを壊して泣かないようにしましょう。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;認証情報は変数指定し、設定ファイルから読み込む&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;サブスクリプションIDやOAuth Client ID/Secretなどを、リソースを作るtfファイルに書かない&lt;/li&gt;
&lt;li&gt;terraform.tfvarsなどにまとめて書く&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;認証情報や現物情報が入ったファイルはバージョン管理ツールから除外する&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Gitなら.gitignoreに指定する&lt;/li&gt;
&lt;li&gt;.tfstateなど現物情報(Azure上のIDなど)が入る結果ファイルも除外

&lt;ul&gt;
&lt;li&gt;チームで使う場合はファイルではなく、Consulなどのリモートバックエンドを使うと思いますが、念のため&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;RBACで必要最小限の権限を付与する&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Terraformの外の話ですが、サービスプリンシパルを作る時には意識しましょう&lt;/li&gt;
&lt;li&gt;身内がリソースをうっかり壊したら、それは管理者の責任です
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ネットワークセキュリティグループはサブネットに指定しておく&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;個々のVMの管理者に任せず、サブネットで絞っておきましょう

&lt;ul&gt;
&lt;li&gt;VMはアドホックに作られるケースが多く、ルーズになりがちです&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;サンプルではフロントエンドとバックエンドサブネットそれぞれにセキュリティグループを指定しています

&lt;ul&gt;
&lt;li&gt;フロントの受信はPort 80、443、22を許可 (できれば22はソースIP指定)&lt;/li&gt;
&lt;li&gt;バックの受信はフロントサブネットからのみ許可 (Internetからの通信を deny all)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2-手順書をなくそう&#34;&gt;2. 手順書をなくそう&lt;/h2&gt;

&lt;p&gt;どうせなら手順書を無くす心意気でやりましょう。Infrastructure as Codeのメリットのひとつです。コードで手順を語りましょう。わかりやすさ重視です。&lt;/p&gt;

&lt;p&gt;ドキュメントを否定する訳ではなく、コード化に至った背景、ポリシーや使い方、前提条件はドキュメント化し、あとはコードで語る。という世界観です。&lt;/p&gt;

&lt;h2 id=&#34;3-分割境界にこだわりすぎない&#34;&gt;3. 分割境界にこだわりすぎない&lt;/h2&gt;

&lt;p&gt;TerraformのModuleをはじめ、最近のデプロイツールはリソースや処理単位をグルーピングできます。ここがアーキテクトの腕の見せ所です。安易に「ベストプラクティス教えろや」という人は残念ながら残念です。大事なことなので2回言いました。&lt;/p&gt;

&lt;p&gt;グルーピング、分割する目的は、&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;main.tfの肥大化を防止し、コードの見通しを良くする&lt;/li&gt;
&lt;li&gt;再利用しやすくする&lt;/li&gt;
&lt;li&gt;責任範囲を明確化し、オーナー意識を醸成する&lt;/li&gt;
&lt;li&gt;権限とコードを一致させる&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;などが挙げられます。規模が小さく関わる人が少ないうちは無理して分割する必要はないですが、大きくなってくるとメリットがあります。&lt;/p&gt;

&lt;p&gt;以下が分割単位、境界ポリシーの例です。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;リソースタイプで分割する&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;サンプルはその例

&lt;ul&gt;
&lt;li&gt;ネットワーク、ストレージ、VM Availability Setで分割&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;直観的&lt;/li&gt;
&lt;li&gt;デプロイに関わる人数が少ない間はこれがおすすめ
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;組織単位で分割する&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ja.wikipedia.org/wiki/%E3%83%A1%E3%83%AB%E3%83%B4%E3%82%A3%E3%83%B3%E3%83%BB%E3%82%B3%E3%83%B3%E3%82%A6%E3%82%A7%E3%82%A4&#34;&gt;コンウェイの法則&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;リソースタイプ = 組織 という場合もある

&lt;ul&gt;
&lt;li&gt;ネットワーク管理者が別グループ、など&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;地理的に分割する&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;リージョンやロケーションで分割&lt;/li&gt;
&lt;li&gt;リソースタイプと組み合わせる手もある

&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Network_JapanEast&amp;rdquo;など
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;静的なリソースと動的なリソースを分ける&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;変化の頻度で分ける

&lt;ul&gt;
&lt;li&gt;ネットワークが頻繁に変わることはまれ&lt;/li&gt;
&lt;li&gt;VMは増減が激しい&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;動的なリソースは対象から外す、別手段とする手も
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;スカッとしませんが、ひとつのポリシーにこだわらず、複数組み合わせてもいいと思います。そんな世界に僕らは生きています。&lt;/p&gt;

&lt;h2 id=&#34;4-早すぎる最適化は悪&#34;&gt;4. 早すぎる最適化は悪&lt;/h2&gt;

&lt;p&gt;最適化できる人 = その道のエキスパート です。使いはじめたばかりの段階では、最適化とか無理。また、システムの外部環境や制約がはじめから決まっていることは、まれです。&lt;/p&gt;

&lt;p&gt;なので、はじめから「最強の構成」を目指さないほうがいいでしょう。特に分割方針。きっとすぐに変えたくなります。&lt;/p&gt;

&lt;p&gt;ひとつのmain.tfで動かしながら、まずTerraformやAzureの仕様や挙動を理解しましょう。そして、慣れてきて、システムの外部環境や制約が見えてきた時点で分割方針を決めてもいいのではないか、と思います。&lt;/p&gt;

&lt;p&gt;そして、&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;リファクタリングできるなら、する&lt;/li&gt;
&lt;li&gt;リファクタリングできなくても、理解の上で維持し機会を待つ、または、次の機会に活かす&lt;/li&gt;
&lt;li&gt;はじめに作った人へマサカリを投げない&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;完璧を求めずにいきましょう。&lt;/p&gt;

&lt;p&gt;でも、しつこいですが、セキュリティだけは、はじめから意識してくださいね。Security by design。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>TerraformをAzure ARMで使う時の認証</title>
      <link>https://ToruMakabe.github.io/post/azure_tf_arm_sp/</link>
      <pubDate>Sat, 27 Feb 2016 12:30:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_tf_arm_sp/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;高まってまいりました&#34;&gt;高まってまいりました&lt;/h2&gt;

&lt;p&gt;全国10,000人のTerraformファンのみなさま、こんにちは。applyしてますか。&lt;/p&gt;

&lt;p&gt;Terraformのマイナーバージョンアップのたびに、&lt;a href=&#34;https://www.terraform.io/docs/providers/azurerm/index.html&#34;&gt;Azure Resource Manager Providerのリソース&lt;/a&gt;が追加されているので、ぼちぼちClassic(Service Management)からの移行を考えよう、という人もいるのでは。VMリソースが追加されたら、いよいよ、ですかね。&lt;/p&gt;

&lt;p&gt;そこで、Classicとは認証方式が変わっているので、ご注意を、という話です。&lt;/p&gt;

&lt;h2 id=&#34;client-id-client-secret-って何よ&#34;&gt;client_id/client_secret って何よ&lt;/h2&gt;

&lt;p&gt;以下がARM向けのProvider設定です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Configure the Azure Resource Manager Provider
provider &amp;quot;azurerm&amp;quot; {
  subscription_id = &amp;quot;...&amp;quot;
  client_id       = &amp;quot;...&amp;quot;
  client_secret   = &amp;quot;...&amp;quot;
  tenant_id       = &amp;quot;...&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;subscription_idは、いつものあれ。tenant_idは普段使わないけどどこかで見た気がする。でも、&lt;strong&gt;client_id/client_secret って何よ&lt;/strong&gt;。ためしにポータルログインで使うID/パスワード指定したら、盛大にコケた。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;The provider needs to be configured with the credentials needed to generate OAuth tokens for the ARM API.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;おっとそういうことか。OAuth。&lt;/p&gt;

&lt;h2 id=&#34;サービスプリンシパルを使おう&#34;&gt;サービスプリンシパルを使おう&lt;/h2&gt;

&lt;p&gt;Terraformをアプリケーションとして登録し、そのサービスプリンシパルを作成し権限を付与すると、使えるようになります。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/active-directory-application-objects/&#34;&gt;&amp;ldquo;アプリケーション オブジェクトおよびサービス プリンシパル オブジェクト&amp;rdquo;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/resource-group-authenticate-service-principal/&#34;&gt;&amp;ldquo;Azure リソース マネージャーでのサービス プリンシパルの認証&amp;rdquo;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;以下、Azure CLIでの実行結果をのせておきます。WindowsでもMacでもLinuxでも手順は同じです。&lt;/p&gt;

&lt;p&gt;まずは、Terraformをアプリとして登録します。&amp;ndash;identifier-urisの存在チェックはないですが、ユニークにしなければいけません。また、&amp;ndash;passwordはclient_secretになるので、おぼえておきましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ azure ad app create --name &amp;quot;My Terraform&amp;quot; --home-page &amp;quot;http://tftest.makabe.info&amp;quot; --identifier-uris &amp;quot;http://tftest.makabe.info&amp;quot; --password pAssw0rd%
info:    Executing command ad app create
+ Creating application My Terraform
data:    AppId:                   AppId-AppId-AppId-AppId-AppId
data:    ObjectId:                AppObjId-AppObjId-AppObjId-AppObjId
data:    DisplayName:             My Terraform
data:    IdentifierUris:          0=http://tftest.makabe.info
data:    ReplyUrls:
data:    AvailableToOtherTenants:  False
data:    AppPermissions:
data:                             claimValue:  user_impersonation
data:                             description:  Allow the application to access My Terraform on behalf of the signed-in user.
data:                             directAccessGrantTypes:
data:                             displayName:  Access My Terraform
data:                             impersonationAccessGrantTypes:  impersonated=User, impersonator=Application
data:                             isDisabled:
data:                             origin:  Application
data:                             permissionId:  AppPermID-AppPermID-AppPermID-AppPermID
data:                             resourceScopeType:  Personal
data:                             userConsentDescription:  Allow the application to access My Terraform on your behalf.
data:                             userConsentDisplayName:  Access My Terraform
data:                             lang:
info:    ad app create command OK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次にサービスプリンシパルを作ります。AppIdは先ほどアプリを登録した際に生成されたものです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ azure ad sp create AppId-AppId-AppId-AppId-AppId
info:    Executing command ad sp create
+ Creating service principal for application AppId-AppId-AppId-AppId-AppId
data:    Object Id:               SpObjId-SpObjId-SpObjId-SpObjId
data:    Display Name:            My Terraform
data:    Service Principal Names:
data:                             AppId-AppId-AppId-AppId-AppId
data:                             http://tftest.makabe.info
info:    ad sp create command OK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サービスプリンシパルの役割を設定します。&amp;ndash;objectIdは、サービスプリンシパルのObject Idなのでご注意を。アプリのObject Idではありません。&lt;/p&gt;

&lt;p&gt;この例では、サブスクリプションのContributorとして位置づけました。権限設定は慎重に。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ azure role assignment create --objectId SpObjId-SpObjId-SpObjId-SpObjId-SpObjId -o Contributor -c /subscriptions/SubId-SubId-SubId-SubId-SubId/
info:    Executing command role assignment create
+ Finding role with specified name
/data:    RoleAssignmentId     : /subscriptions/SubId-SubId-SubId-SubId-SubId/providers/Microsoft.Authorization/roleAssignments/RoleAsId-RoleAsId-RoleAsId-RoleAsId
data:    RoleDefinitionName   : Contributor
data:    RoleDefinitionId     : RoleDefId-RoleDefId-RoleDefId-RoleDefId-RoleDefId
data:    Scope                : /subscriptions/SubId-SubId-SubId-SubId-SubId
data:    Display Name         : My Terraform
data:    SignInName           :
data:    ObjectId             : SpObjId-SpObjId-SpObjId-SpObjId-SpObjId
data:    ObjectType           : ServicePrincipal
data:
+
info:    role assignment create command OK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サービスプリンシパルまわりの設定は以上です。&lt;/p&gt;

&lt;p&gt;テナントIDを確認しておきましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ azure account list --json
[
  {
    &amp;quot;id&amp;quot;: &amp;quot;SubId-SubId-SubId-SubId-SubId&amp;quot;,
    &amp;quot;name&amp;quot;: &amp;quot;Your Subscription Name&amp;quot;,
    &amp;quot;user&amp;quot;: {
      &amp;quot;name&amp;quot;: &amp;quot;abc@microsoft.com&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;user&amp;quot;
    },
    &amp;quot;tenantId&amp;quot;: &amp;quot;TenantId-TenantId-TenantId-TenantId-TenantId&amp;quot;,
    &amp;quot;state&amp;quot;: &amp;quot;Enabled&amp;quot;,
    &amp;quot;isDefault&amp;quot;: true,
    &amp;quot;registeredProviders&amp;quot;: [],
    &amp;quot;environmentName&amp;quot;: &amp;quot;AzureCloud&amp;quot;
  }
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これでようやく.tfファイルが書けます。さくっとリソースグループでも作ってみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Configure the Azure Resource Manager Provider
provider &amp;quot;azurerm&amp;quot; {
  subscription_id = &amp;quot;SubId-SubId-SubId-SubId-SubId&amp;quot;
  client_id       = &amp;quot;AppId-AppId-AppId-AppId-AppId&amp;quot;
  client_secret   = &amp;quot;pAssw0rd%&amp;quot;
  tenant_id       = &amp;quot;TenantId-TenantId-TenantId-TenantId-TenantId&amp;quot;
}

# Create a resource group
resource &amp;quot;azurerm_resource_group&amp;quot; &amp;quot;test&amp;quot; {
    name     = &amp;quot;test&amp;quot;
    location = &amp;quot;Japan West&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;apply。もちろんplanしましたよ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ terraform apply
azurerm_resource_group.test: Creating...
  location: &amp;quot;&amp;quot; =&amp;gt; &amp;quot;japanwest&amp;quot;
  name:     &amp;quot;&amp;quot; =&amp;gt; &amp;quot;test&amp;quot;
azurerm_resource_group.test: Creation complete

Apply complete! Resources: 1 added, 0 changed, 0 destroyed.  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これで、ARM認証難民がうまれなくなりますように。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azure DDoS対策ことはじめ</title>
      <link>https://ToruMakabe.github.io/post/azure_ddosprotection/</link>
      <pubDate>Mon, 15 Feb 2016 17:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_ddosprotection/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;すこぶるfaq&#34;&gt;すこぶるFAQ&lt;/h2&gt;

&lt;p&gt;攻撃者の荒ぶり具合が高まっており、ご相談いただく機会が増えました。「どうすればいいか見当がつかない」というケースも少なくないので、DDoSに絞り、現時点で検討していただきたいことをシンプルにまとめます。&lt;/p&gt;

&lt;h2 id=&#34;公式ホワイトペーパー&#34;&gt;公式ホワイトペーパー&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://download.microsoft.com/download/C/A/3/CA3FC5C0-ECE0-4F87-BF4B-D74064A00846/AzureNetworkSecurity_v3_Feb2015.pdf&#34;&gt;Microsoft Azure Network Security Whitepaper V3&lt;/a&gt;が、現時点でのMicrosoft公式見解です。DDoS以外にもセキュリティ関連で考慮すべきことがまとまっています。おすすめです。&lt;/p&gt;

&lt;p&gt;今回はここから、DDoSに言及している部分を抜き出し意訳します。必要に応じて補足も入れます。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2016//3/4 追記 &lt;a href=&#34;http://download.microsoft.com/download/8/0/A/80ABD45E-BF1B-4235-A1C4-C8C43113CE70/AzureNetworkSecurity_v3_Mar2015.pdf&#34;&gt;日本語訳&lt;/a&gt;がありました&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;2-2-security-management-and-threat-defense-protecting-against-ddos&#34;&gt;2.2 Security Management and Threat Defense - Protecting against DDoS&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;To protect Azure platform services, Microsoft provides a distributed denial-of-service (DDoS) defense system that is part of Azure’s continuous monitoring process, and is continually improved through penetration-testing. Azure’s DDoS defense system is designed to not only withstand attacks from the outside, but also from other Azure tenants:&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;MicrosoftはDDoSを防ぐ仕組みを提供しています。Azure外部からの攻撃はもちろんのこと、Azure内部で別テナントから攻撃されることも考慮しています。&lt;/p&gt;

&lt;h2 id=&#34;azureがやってくれること&#34;&gt;Azureがやってくれること&lt;/h2&gt;

&lt;p&gt;では、具体的に。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;1. Network-layer high volume attacks. These attacks choke network pipes and packet processing capabilities by flooding the network with packets. The Azure DDoS defense technology provides detection and mitigation techniques such as SYN cookies, rate limiting, and connection limits to help ensure that such attacks do not impact customer environments.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ネットワークレイヤで検知できる力押しは、AzureのDDoS防御システムが検知、緩和します。このホワイトペーパーのAppendixで図解されていますが、それはファイヤウォールの前段に配置され、SYN Cookieやレート制限、コネクション制限などのテクニックを使っています。&lt;/p&gt;

&lt;h2 id=&#34;お客様対応が必要なこと&#34;&gt;お客様対応が必要なこと&lt;/h2&gt;

&lt;p&gt;ですが、アプリケーションレイヤの攻撃は、AzureのDDoS防御システムだけでは防ぎきれません。お客様のアプリや通信の内容、要件まで踏み込めないからです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;2. Application-layer attacks. These attacks can be launched against a customer VM. Azure does not provide mitigation or actively block network traffic affecting individual customer deployments, because the infrastructure does not interpret the expected behavior of customer applications. In this case, similar to on-premises deployments, mitigations include:&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のような対処が有効です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;Running multiple VM instances behind a load-balanced Public IP address.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;攻撃されるポイントを負荷分散装置のパブリックIPに限定し、複数のVMへ負荷を散らします。 攻撃されても、できる限り踏ん張るアプローチです。AzureのDDoS防御システムで緩和しきれなかったトラフィックを受け止め、ダウンしないようにします。攻撃規模は事前に判断できないので、どれだけスケールさせるかは、ダウンした場合のビジネスインパクトとコストの兼ね合いで決める必要があります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;Using firewall proxy devices such as Web Application Firewalls (WAFs) that terminate and forward traffic to endpoints running in a VM. This provides some protection against a broad range of DoS and other attacks, such as low-rate, HTTP, and other application-layer threats. Some virtualized solutions, such as Barracuda Networks, are available that perform both intrusion detection and prevention.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WAFを入れて、通信の中身を見ないとわからない攻撃を検知、緩和します。一見ノーマルなトラフィックでも「ゆっくりと攻撃」するようなケースもあります。たとえば、ゆっくりWebサーバのコネクションを枯渇させるような攻撃などです。Azureでは仮想アプライアンスとして、Barracuda NetworksのWAFなどが使えます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot; Web Server add-ons that protect against certain DoS attacks.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Webサーバへアドインを入れましょう。パッチも適用しましょう。構成も見直しましょう。ちょっと古いですが&lt;a href=&#34;http://blogs.msdn.com/b/friis/archive/2014/12/30/security-guidelines-to-detect-and-prevent-dos-attacks-targeting-iis-azure-web-role-paas.aspx&#34;&gt;ここ&lt;/a&gt;が参考になります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;Network ACLs, which can prevent packets from certain IP addresses from reaching VMs.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;もしブロックしたいアクセス元IPアドレスがわかるなら、ACLで遮断しましょう。逆に通信可能な範囲のみ指定することもできます。&lt;/p&gt;

&lt;h2 id=&#34;ホワイトペーパーに加えて&#34;&gt;ホワイトペーパーに加えて&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/services/cdn/&#34;&gt;CDN&lt;/a&gt;も有効ですので検討ください。2段構えでの負荷分散、防御ができます。Akamaiとの統合ソリューションも今後&lt;a href=&#34;https://azure.microsoft.com/ja-jp/blog/microsoft-and-akamai-bring-cdn-to-azure-customers/&#34;&gt;提供される予定&lt;/a&gt;です。&lt;/p&gt;

&lt;p&gt;CDNは常に世界中からのトラフィックで揉まれているだけあって、DDoS防御四天王で最強の漢が最初に出てくるくらい強力です。&lt;/p&gt;

&lt;p&gt;最後に。攻撃されている感があれば、カスタマーサポートまで。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azure Blob Upload ツール別ベンチマーク</title>
      <link>https://ToruMakabe.github.io/post/azureblobupload_perf/</link>
      <pubDate>Thu, 11 Feb 2016 12:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azureblobupload_perf/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;同じ目的を達成できるツールがたくさん&#34;&gt;同じ目的を達成できるツールがたくさん&lt;/h2&gt;

&lt;p&gt;やりたいことがあり、それを達成する手段がたくさん。どう選ぼう。じゃあ特徴を知りましょう。という話です。&lt;/p&gt;

&lt;p&gt;端末からAzureへファイルをアップロードする手段は多くあります。CLIツール、GUIツール、SDKで自作する、etc。&lt;/p&gt;

&lt;p&gt;そして、端末と、そのおかれている環境も多様です。Windows、Mac。有線、無線。&lt;/p&gt;

&lt;p&gt;で、大事なのは平行度。ブロックBlobはブロックを平行に転送する方式がとれるため、ツールが平行転送をサポートしているか? どのくらい効くのか? は重要な評価ポイントです。&lt;/p&gt;

&lt;p&gt;なので、どのツールがおすすめ?と聞かれても、条件抜きでズバっとは答えにくい。そしてこの質問は頻出。なのでこんな記事を書いています。&lt;/p&gt;

&lt;h2 id=&#34;環境と測定方式&#34;&gt;環境と測定方式&lt;/h2&gt;

&lt;p&gt;おそらくファイルを送る、という用途でもっとも重視すべき特徴は転送時間でしょう。ではツール、環境別に転送時間を測定してみます。&lt;/p&gt;

&lt;p&gt;環境は以下の通り。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Windows端末

&lt;ul&gt;
&lt;li&gt;Surface Pro 4 Core i7/16GB Memory/802.11ac&lt;/li&gt;
&lt;li&gt;1Gbps Ethernet (USB経由)&lt;/li&gt;
&lt;li&gt;Windows 10 (1511)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Mac端末

&lt;ul&gt;
&lt;li&gt;Macbook 12inch Core M/8GB Memory/802.11ac&lt;/li&gt;
&lt;li&gt;USB-C&amp;hellip; 有線テストは省きます&lt;/li&gt;
&lt;li&gt;El Capitan&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Wi-Fiアクセスポイント/端末間帯域

&lt;ul&gt;
&lt;li&gt;100~200Mbpsでつながっています&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Azureデータセンタまでの接続

&lt;ul&gt;
&lt;li&gt;日本マイクロソフトの品川オフィスから、首都圏にあるAzure Japan Eastリージョンに接続&lt;/li&gt;
&lt;li&gt;よってWAN側の遅延、帯域ともに条件がいい&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;対象ツール

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/storage-use-azcopy/&#34;&gt;AzCopy v5.0.0.27&lt;/a&gt; (Windowsのみ)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/xplat-cli-install/&#34;&gt;Azure CLI v0.9.15&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://storageexplorer.com/&#34;&gt;Azure Storage Explorer - Cross Platform GUI v0.7&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;転送ファイル

&lt;ul&gt;
&lt;li&gt;Ubuntu 15.10 ISOイメージ (647MBytes)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;そして測定方式。&lt;/p&gt;

&lt;p&gt;AzCopyはPowerShellのMeasure-Commandにて実行時間をとります。NCが平行度指定です。デフォルトの平行度はCPUコア数の8倍です。わしのSurface、OSから4コア見えていますので、32。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Measure-Command {AzCopy /Source:C:\Users\myaccount\work /Dest:https://myaccount.blob.core.windows.net/mycontainer /DestKey:mykey /Pattern:ubuntu-15.10-server-amd64.iso /Y /NC:count}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Azure CLIも同様にMeasure-Commandで。&amp;ndash;concurrenttaskcountで平行度を指定できますが、&lt;a href=&#34;https://github.com/Azure/azure-xplat-cli/blob/dev/lib/util/storage.util._js&#34;&gt;ソース&lt;/a&gt;を確認したところ、平行度のデフォルトは5です。&amp;rdquo;StorageUtil.threadsInOperation = 5;&amp;ldquo;ですね。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Measure-Command {azure storage blob upload ./ubuntu-15.10-server-amd64.iso -a myaccount -k mykey mycontainer ubuntu1510 --concurrenttaskcount count}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;残念ながらMacむけAzCopyはありませんので、Azure CLIのみ実行します。timeコマンドで時間をとります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;time azure storage blob upload ./ubuntu-15.10-server-amd64.iso -a myaccount -k mykey mycontainer ubuntu1510 --concurrenttaskcount count
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Azure Storage Explorer Cross Platform GUIは、目視+iPhoneのストップウォッチで。&lt;/p&gt;

&lt;h2 id=&#34;結果&#34;&gt;結果&lt;/h2&gt;

&lt;p&gt;平行度上げても伸びないな、というタイミングまで上げます。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;right&#34;&gt;　実行No　&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;　クライアントOS　&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;　ネットワーク接続　&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;　クライアント　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　並行数　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　転送時間(秒)　&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Windows 10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1Gbps Ethernet&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;AzCopy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;(default:32)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9.62&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Windows 10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1Gbps Ethernet&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;AzCopy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.28&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Windows 10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1Gbps Ethernet&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;AzCopy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.83&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Windows 10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1Gbps Ethernet&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;AzCopy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.43&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Windows 10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1Gbps Ethernet&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Azure CLI&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;(default:5)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;49.92&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Windows 10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1Gbps Ethernet&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Azure CLI&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;29.47&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Windows 10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1Gbps Ethernet&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Azure CLI&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21.05&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Windows 10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1Gbps Ethernet&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Azure CLI&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;40&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20.12&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Windows 10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1Gbps Ethernet&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Storage Explorer&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;N/A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50.10&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Windows 10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;802.11ac&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;AzCopy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;(default:32)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;74.87&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Windows 10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;802.11ac&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;AzCopy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;53.32&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Windows 10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;802.11ac&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;AzCopy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;58.85&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Windows 10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;802.11ac&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Azure CLI&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;(default:5)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;57.23&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Windows 10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;802.11ac&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Azure CLI&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50.71&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Windows 10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;802.11ac&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Azure CLI&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;54.37&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Windows 10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;802.11ac&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Storage Explorer&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;N/A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;54.63&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Mac OS X&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;802.11ac&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Azure CLI&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;(default:5)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;40.86&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Mac OS X&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;802.11ac&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Azure CLI&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33.97&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Mac OS X&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;802.11ac&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Azure CLI&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;58.57&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Mac OS X&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;802.11ac&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Storage Explorer&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;N/A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;58.20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;考察&#34;&gt;考察&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;有線AzCopy早い。単純計算で67MByte/s出ています。それぞれの計測点の解釈の違いでBlobサービス制限の60MBytes/sを超えてしまっていますがw。データセンタまでのボトルネックがなければ、ポテンシャルを引き出せることがわかります。&lt;/li&gt;
&lt;li&gt;平行度は大きく性能に影響します。

&lt;ul&gt;
&lt;li&gt;平行度が高すぎてもだめ

&lt;ul&gt;
&lt;li&gt;無線AzCopyのデフォルト(平行度32)が平行度10、20より時間がかかっていることからわかる&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;デフォルトで遅いからといってあきらめず、平行度変えて試してみましょう&lt;/li&gt;
&lt;li&gt;SDK使って自分で作る時も同じ。平行度パラメータを意識してください

&lt;ul&gt;
&lt;li&gt;.NET: BlobRequestOptions&lt;/li&gt;
&lt;li&gt;Java/Android: BlobRequestOptions.setConcurrentRequestCount()&lt;/li&gt;
&lt;li&gt;Node.js: parallelOperationThreadCount&lt;/li&gt;
&lt;li&gt;C++: blob_request_options::set_parallelism_factor&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Azure CLIよりAzCopyが早い。

&lt;ul&gt;
&lt;li&gt;.NETで最適化できているから合点&lt;/li&gt;
&lt;li&gt;Node.jsベースでマルチOS対応のAzure CLIは比べられると分が悪い&lt;/li&gt;
&lt;li&gt;でも、802.11acでも無線がボトルネックになっているので、いまどきのWi-Fi環境では似たような性能になる&lt;/li&gt;
&lt;li&gt;No.18の結果は無線状態がよかったと想定&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Azure Storage Explorer Cross Platform GUIは、現時点で平行度変えられないので性能面では不利。でも直観的なので、使い分け。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;WAN条件がいいベンチマークでなので、ぜひみなさんの条件でも試してみてください。遅延の大きなリージョンや途中に帯域ボトルネックがある条件でやると、最適な平行度が変わってくるはずです。&lt;/p&gt;

&lt;p&gt;でも一番言いたかったのは、Macbookの有線アダプタ欲しいということです。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Linux on Azureでファイル共有する方法</title>
      <link>https://ToruMakabe.github.io/post/fileshare_linuxonazure/</link>
      <pubDate>Sun, 07 Feb 2016 17:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/fileshare_linuxonazure/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;ファイル共有-あまりおすすめしないです&#34;&gt;ファイル共有、あまりおすすめしないです&lt;/h2&gt;

&lt;p&gt;いきなりタイトルを否定しました。ロック。&lt;/p&gt;

&lt;p&gt;さて、これからクラウド、というお客様に、よく聞かれる質問があります。それは「NFSとかの、ファイル共有使える?」です。頻出です。クラウド頻出質問選手権では、西東京予選で毎年ベスト8入りするレベルの強豪校です。&lt;/p&gt;

&lt;p&gt;ですが&lt;strong&gt;個人的には&lt;/strong&gt;あまりおすすめしません。クラウドはなるべく共有部分を減らして、スケーラブルに、かつ障害の影響範囲を局所化するべき、と考えるからです。特にストレージはボトルネックや広範囲な障害の要因になりやすい。障害事例が物語ってます。その代わりにオブジェクトストレージなど、クラウド向きの機能がおすすめです。&lt;/p&gt;

&lt;p&gt;でも、否定はしません。アプリの作りを変えられないケースもあるかと思います。&lt;/p&gt;

&lt;p&gt;そこで、もしAzureでファイル共有が必要であれば、&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/storage-introduction/&#34;&gt;Azure File Storage&lt;/a&gt;を検討してみてください。Azureのマネージドサービスなので、わざわざ自分でサーバたてて運用する必要がありません。楽。&lt;/p&gt;

&lt;p&gt;対応プロトコルは、SMB2.1 or 3.0。LinuxからはNFSじゃなくSMBでつついてください。&lt;/p&gt;

&lt;p&gt;使い方は公式ドキュメントを。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/storage-azure-cli/#create-and-manage-file-shares&#34;&gt;&amp;ldquo;Azure Storage での Azure CLI の使用&amp;rdquo;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/storage-how-to-use-files-linux/&#34;&gt;&amp;ldquo;Linux で Azure File Storage を使用する方法&amp;rdquo;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;もうちょっと情報欲しいですね。補足のためにわたしも流します。&lt;/p&gt;

&lt;h2 id=&#34;azure-cliでストレージアカウントを作成し-ファイル共有を設定&#34;&gt;Azure CLIでストレージアカウントを作成し、ファイル共有を設定&lt;/h2&gt;

&lt;p&gt;ストレージアカウントを作ります。fspocは事前に作っておいたリソースグループです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;local$ azure storage account create tomakabefspoc -l &amp;quot;Japan East&amp;quot; --type LRS -g fspoc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ストレージアカウントの接続情報を確認します。必要なのはdata: connectionstring:の行にあるAccountKey=以降の文字列です。このキーを使ってshareの作成、VMからのマウントを行うので、控えておいてください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;local$ azure storage account connectionstring show tomakabefspoc -g fspoc
info:    Executing command storage account connectionstring show
+ Getting storage account keys
data:    connectionstring: DefaultEndpointsProtocol=https;AccountName=tomakabefspoc;AccountKey=qwertyuiopasdfghjklzxcvbnm==
info:    storage account connectionstring show command OK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;shareを作成します。share名はfspocshareとしました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;local$ azure storage share create -a tomakabefspoc -k qwertyuiopasdfghjklzxcvbnm== fspocshare
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;エンドポイントを確認しておきましょう。VMからのマウントの際に必要です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;local$ azure storage account show tomakabefspoc -g fspoc
[snip]
data:    Primary Endpoints: file https://tomakabefspoc.file.core.windows.net/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;linux-2vmで共有&#34;&gt;Linux * 2VMで共有&lt;/h2&gt;

&lt;p&gt;Ubuntuでやりますよ。SMBクライアントとしてcifs-utilsパッケージをインストールします。&lt;a href=&#34;https://azure.microsoft.com/ja-jp/marketplace/partners/canonical/ubuntuserver1404lts/&#34;&gt;Marketplace提供の14.04 LTS&lt;/a&gt;であれば、すでに入ってるはずです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fspocvm01:~$ sudo apt-get install cifs-utils
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;マウントポイントを作り、マウントします。接続先の指定はエンドポイント+share名で。usernameはストレージアカウント名。パスワードはストレージアカウントのキーです。
パーミッションは要件に合わせてください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fspocvm01:~$ sudo mkdir -p /mnt/fspoc
fspocvm01:~$ sudo mount -t cifs //tomakabefspoc.file.core.windows.net/fspocshare /mnt/fspoc -o vers=3.0,username=tomakabefspoc,password=qwertyuiopasdfghjklzxcvbnm==,dir_mode=0777,file_mode=0777
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;マウント完了。確認用のファイルを作っておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fspocvm01:~$ echo &amp;quot;test&amp;quot; &amp;gt; /mnt/fspoc/test.txt
fspocvm01:~$ cat /mnt/fspoc/test.txt
test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2台目のVMでも同様のマウント作業を。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fspocvm02:~$ sudo apt-get install cifs-utils
fspocvm02:~$ sudo mkdir -p /mnt/fspoc
fspocvm02:~$ sudo mount -t cifs //tomakabefspoc.file.core.windows.net/fspocshare /mnt/fspoc -o vers=3.0,username=tomakabefspoc,password=qwertyuiopasdfghjklzxcvbnm==,dir_mode=0777,file_mode=0777
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1台目で作ったファイルが見えますね。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fspocvm02:~$ ls /mnt/fspoc
test.txt
fspocvm02:~$ cat /mnt/fspoc/test.txt
test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ファイルをいじりましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fspocvm02:~$ echo &amp;quot;onemoretest&amp;quot; &amp;gt;&amp;gt; /mnt/fspoc/test.txt
fspocvm02:~$ cat /mnt/fspoc/test.txt
test
onemoretest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1台目から確認。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fspocvm01:~$ cat /mnt/fspoc/test.txt
test
onemoretest
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ご利用は計画的に&#34;&gt;ご利用は計画的に&lt;/h2&gt;

&lt;p&gt;2016年2月時点で、Azure File Storageには最大容量:5TB/share、1TB/file、ストレージアカウントあたりの帯域:60MBytes/sという制約があります。これを超えるガチ共有案件では、&lt;a href=&#34;https://azure.microsoft.com/en-us/marketplace/partners/intel/lustre-cloud-edition-evaleval-lustre-2-7/&#34;&gt;Lustre&lt;/a&gt;など別の共有方法を検討してください。&lt;/p&gt;

&lt;p&gt;なおファイルサーバ用途であれば、Azure File Storageではなく、OneDriveなどオンラインストレージSaaSに移行した方が幸せになれると思います。企業向けが使いやすくなってきましたし。運用から解放されるだけじゃなく、便利ですよ。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Linux on AzureでDisk IO性能を確保する方法</title>
      <link>https://ToruMakabe.github.io/post/striping_linuxonazure/</link>
      <pubDate>Wed, 27 Jan 2016 00:19:30 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/striping_linuxonazure/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;俺の鉄板-ができるまで&#34;&gt;&amp;ldquo;俺の鉄板&amp;rdquo;ができるまで&lt;/h2&gt;

&lt;p&gt;前半はポエムです。おそらくこのエントリにたどり着く人の期待はLinux on AzureのDisk IO性能についてと思いますが、それは後半に書きます。&lt;/p&gt;

&lt;p&gt;クラウド、Azureに関わらず、技術や製品の組み合わせは頭の痛い問題です。「これとこれ、組み合わせて動くの？サポートされるの？性能出るの？」という、あれです。技術や製品はどんどん進化しますので、同じ組み合わせが使えることは珍しくなってきています。&lt;/p&gt;

&lt;p&gt;ちなみにお客様のシステムを設計する機会が多いわたしは、こんな流れで検討します。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;構成要素全体を俯瞰したうえで、調査が必要な技術や製品、ポイントを整理する

&lt;ul&gt;
&lt;li&gt;やみくもに調べものしないように&lt;/li&gt;
&lt;li&gt;経験あるアーキテクトは実績ある組み合わせや落とし穴を多くストックしているので、ここが早い&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ベンダの公式資料を確認する

&lt;ul&gt;
&lt;li&gt;「この使い方を推奨/サポートしています」と明記されていれば安心&lt;/li&gt;
&lt;li&gt;でも星の数ほどある技術や製品との組み合わせがすべて網羅されているわけではない&lt;/li&gt;
&lt;li&gt;不明確なら早めに問い合わせる&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ベンダが運営しているコミュニティ上の情報を確認する

&lt;ul&gt;
&lt;li&gt;ベンダの正式見解ではない場合もあるが、その製品を担当する社員が書いている情報には信ぴょう性がある&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;コミュニティや有識者の情報を確認する

&lt;ul&gt;
&lt;li&gt;OSSでは特に&lt;/li&gt;
&lt;li&gt;専門性を感じるサイト、人はリストしておく&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;動かす

&lt;ul&gt;
&lt;li&gt;やっぱり動かしてみないと&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;提案する

&lt;ul&gt;
&lt;li&gt;リスクがあれば明示します&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;問題なければ実績になる、問題があればリカバリする

&lt;ul&gt;
&lt;li&gt;提案しっぱなしにせずフォローすることで、自信とパターンが増える&lt;/li&gt;
&lt;li&gt;次の案件で活きる
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;いまのわたしの課題は4、5です。特にOSS案件。AzureはOSSとの組み合わせを推進していて、ここ半年でぐっと情報増えたのですが、まだ物足りません。断片的な情報を集め、仮説を立て、動かす機会が多い。なので、5を増やして、4の提供者側にならんとなぁ、と。&lt;/p&gt;

&lt;h2 id=&#34;linux-on-azureでdisk-io性能を確保する方法&#34;&gt;Linux on AzureでDisk IO性能を確保する方法&lt;/h2&gt;

&lt;p&gt;さて今回の主題です。&lt;/p&gt;

&lt;p&gt;結論: Linux on AzureでDisk IOを最大化するには、MDによるストライピングがおすすめ。いくつかパラメータを意識する。&lt;/p&gt;

&lt;p&gt;Linux on AzureでDisk IO性能を必要とする案件がありました。検討したアイデアは、SSDを採用したPremium Storageを複数束ねてのストライピングです。Premium Storageはディスクあたり5,000IOPSを期待できます。でも、それで足りない恐れがありました。なので複数並べて平行アクセスし、性能を稼ぐ作戦です。&lt;/p&gt;

&lt;p&gt;サーバ側でのソフトウェアストライピングは古くからあるテクニックで、ハードの能力でブン殴れそうなハイエンドUnixサーバとハイエンドディスクアレイを組み合わせた案件でも、匠の技として使われています。キャッシュやアレイコントローラ頼りではなく、明示的にアクセスを分散することで性能を確保することができます。&lt;/p&gt;

&lt;p&gt;Linuxで使える代表的なストライプ実装は、LVMとMD。&lt;/p&gt;

&lt;p&gt;ではAzure上でどちらがを選択すべきでしょう。この案件では性能が優先事項です。わたしはその時点で判断材料を持っていませんでした。要調査。この絞り込みまでが前半ポエムの1です。&lt;/p&gt;

&lt;p&gt;前半ポエムの2、3はググ、もといBing力が試される段階です。わたしは以下の情報にたどり着きました。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-linux-configure-raid/&#34;&gt;&amp;ldquo;Configure Software RAID on Linux&amp;rdquo;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/storage-premium-storage-preview-portal/&#34;&gt;&amp;ldquo;Premium Storage: Azure 仮想マシン ワークロード向けの高パフォーマンス ストレージ&amp;rdquo;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blogs.msdn.com/b/igorpag/archive/2014/10/23/azure-storage-secrets-and-linux-i-o-optimizations.aspx&#34;&gt;&amp;ldquo;Azure Storage secrets and Linux I/O optimizations&amp;rdquo;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;得られた情報の中で大事なのは、&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;公式ドキュメントで

&lt;ul&gt;
&lt;li&gt;LVMではなくMDを使った構成例が紹介されている&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;マイクロソフトがホストするブログ(MSDN)で、エキスパートが

&lt;ul&gt;
&lt;li&gt;LVMと比較したうえで、MDをすすめている&lt;/li&gt;
&lt;li&gt;MDのChunkサイズについて推奨値を紹介している&lt;/li&gt;
&lt;li&gt;そのほか、ファイルシステムやスケジューラに関する有益な情報あり&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;なるほど。わたしのこの時点での方針はこうです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;LVMを使う必然性はないため、MDに絞る

&lt;ul&gt;
&lt;li&gt;LVMのほうが機能豊富だが、目的はストライピングだけであるため、シンプルなほうを&lt;/li&gt;
&lt;li&gt;物理障害対策はAzureに任せる (3コピー)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;MDのChunkをデフォルトの512KBから64KBに変更する (ここは結果によって調整)&lt;/li&gt;
&lt;li&gt;Premium StorageのキャッシュはReadOnly or Noneにする予定であるため、ファイルシステムのバリアを無効にする&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上記シナリオで、ディスク当たり5,000IOPS、ストライプ数に比例した性能が実際出れば提案価値あり、ということになります。
ですが、ズバリな実績値が見つからない。ダラダラ探すのは時間の無駄。これは自分でやるしかない。&lt;/p&gt;

&lt;p&gt;構成手順は前述のリンク先にありますが、ポイントを抜き出します。OS=Ubuntu、ファイルシステム=ext4の場合です。&lt;/p&gt;

&lt;p&gt;MDでストライプを作る際、チャンクを64KBに変更します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo mdadm --create /dev/md127 --level 0 --raid-devices 2  /dev/sdc1 /dev/sdd1 -c 64k
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;マウント時にバリアを無効にします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo mount /dev/md127 /mnt -o barrier=0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;では、Premium Storage(P30)をMDで2つ束ねたストライプにfioを実行してみましょう。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;100% Random Read&lt;/li&gt;
&lt;li&gt;キャッシュ効果のないデータをとるため、Premium StorageのキャッシュはNone、fio側もdirect=1&lt;/li&gt;
&lt;li&gt;ブロックサイズは小さめの値が欲しかったので、1K&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;結果。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;randread: (g=0): rw=randread, bs=1K-1K/1K-1K/1K-1K, ioengine=libaio, iodepth=32
fio-2.1.3
Starting 1 process

randread: (groupid=0, jobs=1): err= 0: pid=9193: Tue Jan 26 05:48:09 2016
  read : io=102400KB, bw=9912.9KB/s, iops=9912, runt= 10330msec
[snip]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2本束ねて9,912IOPS。1本あたりほぼ5,000IOPS。ほぼ期待値。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>クラウドは本当に性能不足なのか</title>
      <link>https://ToruMakabe.github.io/post/doubt_lackofperf_oncloud/</link>
      <pubDate>Sun, 24 Jan 2016 00:19:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/doubt_lackofperf_oncloud/</guid>
      <description></description>
      
      <content>

&lt;p&gt;&lt;strong&gt;このエントリは2016/1/24に書きました。使えるリソースはどんどん増えていくので、適宜その時点で情報をとってください。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;具体的な数値で-正しい理解を&#34;&gt;具体的な数値で、正しい理解を&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://itpro.nikkeibp.co.jp/atcl/watcher/14/334361/011800463/&#34;&gt;&amp;ldquo;クラウドは性能不足、企業システムが重すぎる&amp;rdquo;&lt;/a&gt;という記事が身の回りで話題になりました。公開から4日たっても「いま読まれている記事」の上位にあり、注目されているようです。&lt;/p&gt;

&lt;p&gt;記事で訴えたかったことは、クラウドを過信しないように、そして、クラウドはクラウドらしい使い方をしよう、ということでしょう。ユーザの声は貴重ですし、同意できるところも多い。でも、「企業システム」とひとくくりにしてしまったこと。タイトルのバイアスが強いこと。そして、具体的な根拠に欠けることから、誤解を招いている印象です。&lt;/p&gt;

&lt;p&gt;どんな技術、製品、サービスにも限界や制約はあります。具体的な数値や仕様で語らないと、そこから都市伝説が生まれます。&lt;/p&gt;

&lt;p&gt;いい機会なので、わたしの主戦場であるAzureを例に、クラウドでどのくらいの性能を期待できるか、まとめてみようと思います。&lt;/p&gt;

&lt;h2 id=&#34;シングルvmでどれだけ&#34;&gt;シングルVMでどれだけ&lt;/h2&gt;

&lt;p&gt;話題となった記事でも触れられているように、クラウドはその生まれから、分散、スケールアウトな作りのアプリに向いています。ですが世の中には「そうできない」「そうするのが妥当ではない」システムもあります。記事ではそれを「企業システム」とくくっているようです。&lt;/p&gt;

&lt;p&gt;わたしは原理主義者ではないので「クラウドに載せたかったら、そのシステムを作り直せ」とは思いません。作りを大きく変えなくても載せられる、それでクラウドの特徴を活かして幸せになれるのであれば、それでいいです。もちろん最適化するにこしたことはありませんが。&lt;/p&gt;

&lt;p&gt;となると、クラウド活用の検討を進めるか、あきらめるか、判断材料のひとつは「スケールアウトできなくても、性能足りるか?」です。&lt;/p&gt;

&lt;p&gt;この場合、1サーバ、VMあたりの性能上限が制約です。なので、AzureのシングルVM性能が鍵になります。&lt;/p&gt;

&lt;p&gt;では、Azureの仮想マシンの提供リソースを確認しましょう。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/virtual-machines-size-specs/&#34;&gt;&amp;ldquo;仮想マシンのサイズ&amp;rdquo;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ざっくりA、D、Gシリーズに分けられます。Aは初期からあるタイプ。ＤはSSDを採用した現行の主力。Gは昨年後半からUSリージョンで導入がはじまった、大物です。ガンダムだと後半、宇宙に出てから登場するモビルアーマー的な存在。現在、GシリーズがもっともVMあたり多くのリソースを提供できます。&lt;/p&gt;

&lt;p&gt;企業システムではOLTPやIOバウンドなバッチ処理が多いと仮定します。では、Gシリーズ最大サイズ、Standard_GS5の主な仕様から、OLTPやバッチ処理性能の支配要素となるCPU、メモリ、IOPSを見てみましょう。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Standard_GS5の主な仕様

&lt;ul&gt;
&lt;li&gt;32仮想CPUコア&lt;/li&gt;
&lt;li&gt;448GBメモリ&lt;/li&gt;
&lt;li&gt;80,000IOPS&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;メモリはクラウドだからといって特記事項はありません。クラウドの特徴が出るCPUとIOPSについて深掘りしていきます。&lt;/p&gt;

&lt;p&gt;なお、&lt;strong&gt;現時点で&lt;/strong&gt;まだ日本リージョンにはGシリーズが投入されていません。必要に応じ、公開スペックと後述のACUなどを使ってA、Dシリーズと相対評価してください。&lt;/p&gt;

&lt;h2 id=&#34;32仮想cpuコアの規模感&#34;&gt;32仮想CPUコアの規模感&lt;/h2&gt;

&lt;p&gt;クラウドのCPU性能表記は、なかなか悩ましいです。仮想化していますし、CPUは世代交代していきます。ちなみにAzureでは、ACU(Azure Compute Unit)という単位を使っています。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/virtual-machines-size-specs/#-3&#34;&gt;&amp;ldquo;パフォーマンスに関する考慮事項&amp;rdquo;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ACUはAzure内で相対評価をする場合にはいいのですが、「じゃあAzureの外からシステムもってきたとき、実際どのくらいさばけるのよ。いま持ってる/買えるサーバ製品でいうと、どのくらいよ」という問いには向きません。&lt;/p&gt;

&lt;p&gt;クラウドや仮想化に関わらず、アプリの作りと処理するデータ、ハードの組み合わせで性能は変わります。動かしてみるのが一番です。せっかくイニシャルコストのかからないクラウドです。試しましょう。でもその前に、試す価値があるか判断しなければいけない。なにかしらの参考値が欲しい。予算と組織で動いてますから。わかります。&lt;/p&gt;

&lt;p&gt;では例をあげましょう。&lt;strong&gt;俺のベンチマーク&lt;/strong&gt;を出したいところですが、「それじゃない」と突っ込まれそうです。ここはぐっと我慢して、企業でよく使われているERP、SAPのSAP SDベンチマークにしましょう。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://global.sap.com/campaigns/benchmark/appbm_cloud.epx&#34;&gt;&amp;ldquo;SAP Standard Application Benchmarks in Cloud Environments&amp;rdquo;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://global.sap.com/campaigns/benchmark/index.epx&#34;&gt;&amp;ldquo;SAP Standard Application Benchmarks&amp;rdquo;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;SAPSという値が出てきます。販売管理アプリケーションがその基盤上でどれだけ仕事ができるかという指標です。&lt;/p&gt;

&lt;p&gt;比較のため、3年ほど前の2ソケットマシン、現行2ソケットマシン、現行4ソケットマシンを選びました。単体サーバ性能をみるため、APとDBを1台のサーバにまとめた、2-Tierの値をとります。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;&lt;a href=&#34;http://download.sap.com/download.epd?context=40E2D9D5E00EEF7C91D3C5AFFF9A4689C82EA97027CDF4A42858AD1610A3F732&#34;&gt;DELL R720&lt;/a&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;&lt;a href=&#34;http://global.sap.com/campaigns/benchmark/assets/Cert15038.pdf&#34;&gt;Azure VM GS5&lt;/a&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;&lt;a href=&#34;http://download.sap.com/download.epd?context=40E2D9D5E00EEF7CFDB9CAEA540B6F601993E4359AB45BEF7ED0949D1BFF155D&#34;&gt;NEC R120f-2M&lt;/a&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;&lt;a href=&#34;http://download.sap.com/download.epd?context=40E2D9D5E00EEF7C14B03FD143D20C6C90E8F6DEAA4E15F8090BA77A6249E1D0&#34;&gt;FUJITSU RX4770 M2&lt;/a&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Date&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;sup&gt;2012&lt;/sup&gt;&amp;frasl;&lt;sub&gt;4&lt;/sub&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;sup&gt;2015&lt;/sup&gt;&amp;frasl;&lt;sub&gt;9&lt;/sub&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;sup&gt;2015&lt;/sup&gt;&amp;frasl;&lt;sub&gt;7&lt;/sub&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;sup&gt;2015&lt;/sup&gt;&amp;frasl;&lt;sub&gt;7&lt;/sub&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;CPU Type&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Intel Xeon Processor E5-2690&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Intel Xeon Processor E5-2698B v3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Intel Xeon Processor E5-2699 v3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Intel Xeon Processor E7-8890 v3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;CPU Sockets&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;CPU Cores&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;32 (Virtual)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;36&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;72&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;SD Benchmark Users&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;6,500&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;7,600&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;14,440&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;29,750&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;SAPS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;35,970&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;41,670&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;79,880&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;162,500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;3年前の2ソケットマシンより性能はいい。現行2ソケットマシンの半分程度が期待値でしょうか。ざっくりE5-2699 v3の物理18コアくらい。4ソケットは無理め。&lt;/p&gt;

&lt;p&gt;なお補足ですが、もちろんSAPはAPサーバをスケールアウトする構成もとれます。その性能は&lt;a href=&#34;http://global.sap.com/campaigns/benchmark/appbm_cloud.epx&#34;&gt;3-Tierベンチマーク&lt;/a&gt;で確認できます。&lt;a href=&#34;http://blogs.msdn.com/b/saponsqlserver/archive/2015/10/05/world-record-sap-sales-and-distribution-standard-application-benchmark-for-sap-cloud-deployments-released-using-azure-iaas-vms.aspx&#34;&gt;Azure上で247,880SAPS&lt;/a&gt;出たそうです。&lt;/p&gt;

&lt;h2 id=&#34;80-000iopsの規模感&#34;&gt;80,000IOPSの規模感&lt;/h2&gt;

&lt;p&gt;IOPS = IO Per Second、秒あたりどれだけIOできるかという指標です。Azure VM GS5では&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/storage-premium-storage-preview-portal/&#34;&gt;Premium Storage&lt;/a&gt;を接続し、VMあたり最大80,000IOPSを提供します。&lt;/p&gt;

&lt;p&gt;一般的に企業で使われているディスクアレイに載っているHDDのIOPSは、1本あたりおおよそ200です。IOPSに影響する要素は回転数で、よく回る15,000rpm FC/SAS HDDでだいたいこのくらい。&lt;/p&gt;

&lt;p&gt;なので80,000 / 200 = 400。よって80,000IOPSを達成しようとすると、HDDを400本並べないといけません。小さくないです。&lt;/p&gt;

&lt;p&gt;もちろんディスクアレイにはキャッシュがあるので、キャッシュヒット次第でIOPSは変わります。ベンダが胸を張って公開している値も、キャッシュに当てまくった数字であることが多いです。ですが誠実な技術者は「水物」なキャッシュヒットを前提にサイジングしません。アプリがアレイを占有できて、扱うデータの量や中身に変化がない場合は別ですが、それはまれでしょう。ヒットしない最悪の場合を考慮するはずです。&lt;/p&gt;

&lt;p&gt;なお、数十万IOPSをこえるディスクアレイがあるのは事実です。でも「桁が違う。クラウドしょぼい」と思わないでください。ディスクアレイ全体の性能と、VMあたりどのくらい提供するかは、別の問題です。ひとつのVMがディスクアレイを占有するのでない限り、VMあたりのIOコントロールは必要です。そうでないと、暴れん坊VMの割を食うVMがでてきます。見えていないだけで、クラウドのバックエンドにはスケーラブルなストレージが鎮座しています。&lt;/p&gt;

&lt;h2 id=&#34;結論&#34;&gt;結論&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Intel x86 2ソケットモデルサーバで動いているようなシステムの移行であれば検討価値あり&lt;/li&gt;
&lt;li&gt;メモリが448GB以上必要であれば難しい&lt;/li&gt;
&lt;li&gt;サーバあたり80,000IOPS以上必要であれば難しい、でも本当にサーバあたりそれだけ必要か精査すべき&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ちょっと前までオンプレ案件も担当していましたが、ここ数年は2ソケットサーバ案件中心、ときどき、4ソケット以上で興奮。という感覚です。みなさんはいかがでしょう。データはないのでご参考まで。&lt;/p&gt;

&lt;p&gt;なにはともあれ、プロのみなさんは噂に流されず、制約を数値で把握して判断、設計しましょう。Azureではそのほかの制約条件も公開されていますので、ぜひご一読を。上限を緩和できるパラメータも、あります。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/azure-subscription-service-limits/&#34;&gt;&amp;ldquo;Azure サブスクリプションとサービスの制限、クォータ、制約&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azureでインフラデプロイツールを選ぶ時に考えていること</title>
      <link>https://ToruMakabe.github.io/post/azure_infradeployment_selection/</link>
      <pubDate>Mon, 11 Jan 2016 00:20:30 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_infradeployment_selection/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;ケースバイケースだけど&#34;&gt;ケースバイケースだけど&lt;/h2&gt;

&lt;p&gt;Azureを生業にして、3か月たちます。ここまで、もっとも質問や議論が多いのが、デプロイメントの自動化についてです。進化が早いですし、選択肢も豊富。クラウド採用に合わせて自動化に挑戦するケースも増えてますので、自然なことと思います。&lt;/p&gt;

&lt;p&gt;特に話題になるのが「どのツールを選べばいいか」。ツールというのは課題を解決する手段なので、まず課題を掘るべきです。ですが、まだ成熟していない領域で変化が激しいですし、ツールひとつで課題を解決できるとも限らない。複数のツールを組み合わせることも多く、依存関係もありそう。となると、考えるきっかけが欲しいのは、ごもっとも。&lt;/p&gt;

&lt;p&gt;なので「ケースバイケース。以上」とは、言いにくい。&lt;/p&gt;

&lt;p&gt;私見であっても、たたき台となる考え方なりパターンがWebに転がっていれば、参考になるかもしれない。それがこのエントリを書く動機です。わたしは他のプラットフォームからAzureに主戦場を移していますので、新鮮な意見が書けるかも、という背景も、あります。&lt;/p&gt;

&lt;h2 id=&#34;書く前に前提など&#34;&gt;書く前に前提など&lt;/h2&gt;

&lt;p&gt;対象はインフラレイヤのデプロイメントに絞ります。そして、インフラ = 物理/仮想ハードウェア(サーバ、ストレージ、ネットワーク) + OS + プラットフォームソフト(アプリじゃないもの、Webサーバ、ユーティリティ、etc）と定義します。&lt;/p&gt;

&lt;p&gt;レイヤリングや用語は、 @gosukenator さんの&lt;a href=&#34;http://mizzy.org/blog/2013/10/29/1/&#34;&gt;&amp;ldquo;インフラ系技術の流れ&amp;rdquo;&lt;/a&gt;が参考になるので、合わせて読むと幸せになれるでしょう。このエントリで言うBootstrapping/Configurationレイヤが今回の焦点です。&lt;/p&gt;

&lt;p&gt;では、わたしがツールを選ぶ時にどんなことを考えているのか、脳内をダンプしていきましょう。&lt;/p&gt;

&lt;h2 id=&#34;そもそもツールで自動化すべきかを考える&#34;&gt;そもそもツールで自動化すべきかを考える&lt;/h2&gt;

&lt;p&gt;いきなり萎えるそもそも論で恐縮ですが、重要です。たとえばあるソフトの試用目的で、同じ構成のサーバのデプロイは今後しなさそう、台数は1台、使うのは自分だけ、なんていう環境のデプロイまで、自動化する必要はないはずです。時短、工数削減、オペレーションミスリスクの軽減、そもそも自動化しないと運用がまわらない、など自動化によって得られる利益がその手間を上回るかを判断します。&lt;/p&gt;

&lt;p&gt;なお「知っている/できる」人でないとその価値、利益はわかりません。やらないという判断は、腕があってはじめてできることです。&lt;/p&gt;

&lt;h2 id=&#34;使い捨てられないかを考える&#34;&gt;使い捨てられないかを考える&lt;/h2&gt;

&lt;p&gt;次は、ツールによって作った環境がどのように変化するか、変えられるかを検討します。ストレートに言うと、変化のタイミングで捨てられないか？新しいものに置き換えられないか？を考えます。もしこれができるのであれば、方式はとてもシンプルにできます。Immutable Infrastructure、Blue/Green Deploymentなどのやり口が注目されていますが、これらの根っこには「ちまちま変化を加えて複雑化するくらいなら、使い捨て/入れ替えてしまえ」という意識があります。&lt;/p&gt;

&lt;p&gt;ですが、とは言ってもそんな大胆にできない事情もあると思います。Blue/Green Deploymentでは、入れ替えのタイミングでBlue、Green分のリソースが必要になりますし、切り替えにともなうリスクもあります。それを許容できない場合、同じインフラに変化を積んでいくことになります。ChefなどConfigurationレイヤで冪等なオペーレーションができるツールが注目されたのは、この変化を維持しやすいからです。&lt;/p&gt;

&lt;p&gt;変化を積む場合にやるべきでないのは、中途半端に職人が真心こめて手作業してしまうことです。ツールでやると決めたら、少なくともそのカバー範囲はツールに任せましょう。でないといわゆる「手作業汚れ」「スノーフレークサーバ（雪の結晶のように、全部同じように見えて実はそれぞれ違う）」のダークサイドに堕ちます。&lt;/p&gt;

&lt;p&gt;変化を積まないのであれば、インフラデプロイメント用途ではConfigurationレイヤのツールを導入しないという割り切りもできるでしょう。&lt;/p&gt;

&lt;h2 id=&#34;優先事項や制約条件を洗い出す&#34;&gt;優先事項や制約条件を洗い出す&lt;/h2&gt;

&lt;p&gt;アーキテクトが真っ白なキャンバスに画を描けることはほぼありません。きっと、先になんらかの優先事項や制約条件があるはずです。そして、ほとんどのシステムにおいて、インフラのデプロイは主役ではありません。ツールに合わせてもらえることはまれでしょう。様々な条件を選定にあたって洗い出す必要があります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;社内/プロジェクト標準&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;　周知されていないだけで、推奨ツールが決まってたりします。あるある。そのツールの良し悪しは置いておいて、社内ノウハウの蓄積など、大きな目的がある場合には従うべきでしょう。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;他レイヤでの優先ツール&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;　インフラのデプロイに影響がありそうなツールがアプリ開発側で決まっていたりします。最近華やかなのがDockerです。Docker社が出してるツール群は上から下までカバー範囲も広く、デプロイツールと重複しがちです。組み合わせを検討しなければいけません。また、Apache Mesosもインフラとアプリのグレーゾーンに鎮座します。なかなか悩ましいですが、優先せざるをえません。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;規模&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;　いきなり1000台とか10000台規模を扱うユーザは多くないと思いますが、その規模になるとツールの性能限界にぶち当たったりします。念のため、意識はしましょう。ちなみに、1000台をひとつのツールの傘に入れずとも、たとえば10*100台にする設計ができないか、事前に考えておくと打ち手が増えます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;チーム or ひとり&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;　本番環境のデプロイ自動化はチームプレイになるので、ツールの導入はサーバ上になるでしょうし、構成ファイルの共有、バージョンコントロールなど考慮点は多いです。一方で、開発者が開発、検証用途で端末に導入し実行する使い方では、手軽さが求められます。誤解を恐れず例をあげると、前者にはChefが、後者にはAnsibleやTerraformがフィットしやすいです。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Windows or Linux&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;　Azure ARM Templateなど、はじめからマルチOS環境を前提に作られているツールはありますが、ほとんどのツールはその生まれがWindows、Linuxに寄っています。マルチOS対応が進んではいますが、活用にあたって、参考となる情報量には大きな差があります。たとえばマルチOS対応のツールであっても、DSCはWindowsの、ChefやAnsibleはLinuxの情報が圧倒的に多いです。これは意識せざるを得ません。使うOSでの十分な情報があるか確認します。&lt;/p&gt;

&lt;h2 id=&#34;マネージドサービス-機能を活用する&#34;&gt;マネージドサービス、機能を活用する&lt;/h2&gt;

&lt;p&gt;マネージドサービス = プラットフォームが提供している機能です。Azureであれば、今回対象としているレイヤではARMがそれにあたります。デプロイツールは有用ですが、その導入や維持運用には本質的価値はありません。プラットフォームに任せられるのであれば、そうしたほうが楽です。&lt;/p&gt;

&lt;p&gt;また、Azureのインフラは進化が早いため、それに対応するスピードも、本家ツールのほうが期待できます。&lt;/p&gt;

&lt;p&gt;ですが、&lt;a href=&#34;http://torumakabe.github.io/post/arm_idempotent/&#34;&gt;以前のエントリ&lt;/a&gt;で触れたように、本家のツールであっても、すべてのレイヤをカバーできるほど万能ではありません。たとえばARM TemplateはインフラのBootstrappingには向いていますが冪等性が限定的であるため、ソフトウェアパッケージを足す/消す/入れ替えるを頻繁に繰り返す環境のConfiguration用途では、苦しいです。&lt;/p&gt;

&lt;p&gt;よってARM Templateは、Immutableな環境で使う、もしくは、ChefなどのConfigurationツールと組み合わせて使うことを念頭に設計をします。&lt;/p&gt;

&lt;p&gt;ARM Templateでは、ハード(VM、ストレージ、ネットワーク)の割り当て、OSの導入と設定、各種エージェントの導入が基本。それに加え、Immutableな環境ではプラットフォームソフトを導入してしまっていいでしょう。ARM TemplateにはDSCやシェルを実行するエクステンションが使えるので、活用します。&lt;/p&gt;

&lt;p&gt;また、Bootstrapping時点で、Configurationツールを導入できてしまうのであれば、せっかくなので入れてしまいましょう。たとえばChefサーバのインストールは、ここで。&lt;/p&gt;

&lt;p&gt;以上、ちょっとまとまりに欠けますが、ざっとわたしが意識していることを、挙げてみました。&lt;/p&gt;

&lt;h2 id=&#34;汎用的-リファレンスアーキテクチャ&#34;&gt;汎用的 リファレンスアーキテクチャ&lt;/h2&gt;

&lt;p&gt;具体例があったほうが分かりやすいので、最後に汎用的な組み合わせを紹介します。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://gallery.technet.microsoft.com/Automating-Deployment-with-84c1549f&#34;&gt;&amp;ldquo;Automating Deployment with Azure &amp;amp; Chef&amp;rdquo;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ARM TemplateでBootstrapping&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;VMを4つ作成、1つはLinux、他はWindows&lt;/li&gt;
&lt;li&gt;ストレージ、ネットワークの作成&lt;/li&gt;
&lt;li&gt;VMのストレージ、ネットワーク設定&lt;/li&gt;
&lt;li&gt;OSの導入&lt;/li&gt;
&lt;li&gt;ドメインコントローラサーバへのソフト導入、各種設定 (DSC/PowerShell Extension)&lt;/li&gt;
&lt;li&gt;他Windowsサーバへのソフト導入、各種設定、ドメイン参加 (PowerShell Extension)&lt;/li&gt;
&lt;li&gt;LinuxへChefサーバを導入、各種設定 (Shell Extension)
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ChefでConfiguration&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;各ノードのChef bootstrap(言葉が混同しやすいので注意)&lt;/li&gt;
&lt;li&gt;Chef Clientサービスの起動設定&lt;/li&gt;
&lt;li&gt;DBサーバのDB領域ディスク作成、フォーマット&lt;/li&gt;
&lt;li&gt;DBサーバへSQL Server 2014のインストール&lt;/li&gt;
&lt;li&gt;ChefがDBサーバが設定通りになるよう維持し続ける
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;どうでしょう、役割分担がイメージできたでしょうか。いいドキュメントがあったので、ChefのLinux/Windows混在例を紹介しましたが、Windowsとの親和性や情報量を重視するなら、ChefをAzure Automation DSCに置き換えて挑戦してもいいでしょう。そのまた逆もありで、ChefならLinux染めな環境で、とこだわってもいいと思います。&lt;/p&gt;

&lt;p&gt;書くことが意外に多かったので、また機会があれば、参考例を交えて紹介します。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azure ARM Templateによるデプロイと冪等性</title>
      <link>https://ToruMakabe.github.io/post/arm_idempotent/</link>
      <pubDate>Wed, 06 Jan 2016 00:16:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/arm_idempotent/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;宣言的に-冪等に&#34;&gt;宣言的に、冪等に&lt;/h2&gt;

&lt;p&gt;ここ数年で生まれたデプロイメント手法、ツールは数多くありますが、似たような特徴があります。それは「より宣言的に、冪等に」です。これまで可読性や再利用性を犠牲にしたシェル芸になりがちだったデプロイの世界。それがいま、あるべき姿を定義しその状態に収束させるように、また、何度ツールを実行しても同じ結果が得られるように変わってきています。&lt;/p&gt;

&lt;p&gt;さて、そんな時流に飛び込んできたデプロイ手法があります。AzureのARM(Azure Resource Manager) Templateによるデプロイです。ARMはAzureのリソース管理の仕組みですが、そのARMに対し、構成を宣言的に書いたJSONを食わせて環境を構築する手法です。Azureの標準機能として、提供されています。&lt;/p&gt;

&lt;h3 id=&#34;azure-リソース-マネージャーの概要-https-azure-microsoft-com-ja-jp-documentation-articles-resource-group-overview&#34;&gt;&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/resource-group-overview/&#34;&gt;Azure リソース マネージャーの概要&lt;/a&gt;&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;ソリューションを開発のライフサイクル全体で繰り返しデプロイできます。また、常にリソースが一貫した状態でデプロイされます&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&amp;ldquo;宣言型のテンプレートを利用し、デプロイメントを定義できます&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;冪等と言い切ってはいませんが、目的は似ています。&lt;/p&gt;

&lt;p&gt;なるほど、期待十分。ではあるのですが、冪等性の実現は簡単ではありません。たとえばChefやAnsibleも、冪等性はリソースやモジュール側で考慮する必要があります。多様なリソースの違いを吸収しなければいけないので、仕方ありません。魔法じゃないです。その辺を理解して使わないと、ハマります。&lt;/p&gt;

&lt;p&gt;残念ながらARMは成長が著しく、情報が多くありません。そこで、今回は実行結果を元に、冪等さ加減を理解していきましょう。&lt;/p&gt;

&lt;h2 id=&#34;増分デプロイと完全デプロイ&#34;&gt;増分デプロイと完全デプロイ&lt;/h2&gt;

&lt;p&gt;まず、デプロイのコマンド例を見ていきましょう。今回はPowerShellを使いますが、Mac/Linux/Winで使える&lt;a href=&#34;https://github.com/Azure/azure-xplat-cli&#34;&gt;クロスプラットフォームCLI&lt;/a&gt;もあります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\&amp;gt; New-AzureRmResourceGroupDeployment -ResourceGroupName YourRGName -TemplateFile .\azuredeploy.json -TemplateParameterFile .\azuredeploy.parameters.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ワンライナーです。これだけで環境ができあがります。-TemplateFileでリソース定義を記述したJSONファイルを指定します。また、-TemplateParameterFileにパラメータを外だしできます。&lt;/p&gt;

&lt;p&gt;今回は冪等さがテーマであるため詳細は省きます。関心のあるかたは、別途&lt;a href=&#34;https://azure.microsoft.com/ja-jp/documentation/articles/resource-group-template-deploy/&#34;&gt;ドキュメント&lt;/a&gt;で確認してください。&lt;/p&gt;

&lt;p&gt;さて、ワンライナーで環境ができあがるわけですが、その後が重要です。環境変更の際にJSONで定義を変更し、同じコマンドを再投入したとしても、破たんなく使えなければ冪等とは言えません。&lt;/p&gt;

&lt;p&gt;コマンド投入には2つのモードがあります。増分(Incremental)と完全(Complete)です。まずは増分から見ていきましょう。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;・リソース グループに存在するが、テンプレートに指定されていないリソースを変更せず、そのまま残します&lt;/p&gt;

&lt;p&gt;・テンプレートに指定されているが、リソース グループに存在しないリソースを追加します&lt;/p&gt;

&lt;p&gt;・テンプレートに定義されている同じ条件でリソース グループに存在するリソースを再プロビジョニングしません&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;すでに存在するリソースには手を入れず、JSONへ新たに追加されたリソースのみを追加します。&lt;/p&gt;

&lt;p&gt;いっぽうで、完全モードです。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;・リソース グループに存在するが、テンプレートに指定されていないリソースを削除します&lt;/p&gt;

&lt;p&gt;・テンプレートに指定されているが、リソース グループに存在しないリソースを追加します&lt;/p&gt;

&lt;p&gt;・テンプレートに定義されている同じ条件でリソース グループに存在するリソースを再プロビジョニングしません&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2、3番目は増分と同じです。1番目が違います。JSONから定義を消されたリソースを削除するかどうかが、ポイントです。完全モードはスッキリするけどリスクも高そう、そんな印象を受けるのはわたしだけではないでしょう。&lt;/p&gt;

&lt;h2 id=&#34;動きをつかむ&#34;&gt;動きをつかむ&lt;/h2&gt;

&lt;p&gt;では動きを見ていきましょう。テンプレートはGithubに公開されている&lt;a href=&#34;https://github.com/Azure/azure-quickstart-templates/tree/master/101-vm-simple-linux&#34;&gt;Very simple deployment of an Linux VM&lt;/a&gt;を使います。詳細は説明しませんので、読み進める前にリソース定義テンプレートファイル(azuredeploy.json)を&lt;a href=&#34;https://github.com/Azure/azure-quickstart-templates/blob/master/101-vm-simple-linux/azuredeploy.json&#34;&gt;リンク先&lt;/a&gt;でざっと確認してください。&lt;/p&gt;

&lt;p&gt;パラメータファイル(azuredeploy.parameters.json)は以下とします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;$schema&amp;quot;: &amp;quot;http://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#&amp;quot;,
  &amp;quot;contentVersion&amp;quot;: &amp;quot;1.0.0.0&amp;quot;,
  &amp;quot;parameters&amp;quot;: {
    &amp;quot;adminUsername&amp;quot;: {
      &amp;quot;value&amp;quot;: &amp;quot;azureUser&amp;quot;
    },
    &amp;quot;adminPassword&amp;quot;: {
      &amp;quot;value&amp;quot;: &amp;quot;password1234!&amp;quot;
    },
    &amp;quot;dnsLabelPrefix&amp;quot;: {
      &amp;quot;value&amp;quot;: &amp;quot;armpocps&amp;quot;
    },
    &amp;quot;ubuntuOSVersion&amp;quot;: {
      &amp;quot;value&amp;quot;: &amp;quot;14.04.2-LTS&amp;quot;
    }    
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;まず、1回目の実行です。リソースグループ &amp;ldquo;ARMEval&amp;rdquo;に対しデプロイします。このリソースグループは前もって作っておいた空の箱です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Workspace&amp;gt; New-AzureRmResourceGroupDeployment -ResourceGroupName ARMEval -TemplateFile .\azuredeploy.json -TemplateParameterFile .\azuredeploy.parameters.json 

DeploymentName    : azuredeploy
ResourceGroupName : ARMEval
ProvisioningState : Succeeded
Timestamp         : 2016/01/04 11:46:41
Mode              : Incremental
TemplateLink      :
Parameters        :
                Name             Type                       Value
                ===============  =========================  ==========
                adminUsername    String                     azureUser
                adminPassword    SecureString
                dnsLabelPrefix   String                     armpocps
                ubuntuOSVersion  String                     14.04.2-LTS

Outputs           :
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;できあがりです。空のリソースグループ にLinux VM、ストレージ、仮想ネットワーク、パブリックIPなどがデプロイされました。Modeを指定しない場合は増分(Incremental)となります。&lt;/p&gt;

&lt;p&gt;この環境にじわじわと変更を入れていきましょう。まずはazuredeploy.parameter.json上のパラメータ、DNS名のPrefix(dnsLabelPrefix)をarmpocps -&amp;gt; armpocps2と変えます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;dnsLabelPrefix&amp;quot;: {
  &amp;quot;value&amp;quot;: &amp;quot;armpocps2&amp;quot;
},
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;では再投入です。パラメータファイルの内容は変えましたが、コマンドは同じです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Workspace&amp;gt; New-AzureRmResourceGroupDeployment -ResourceGroupName ARMEval -TemplateFile .\azuredeploy.json -TemplateParameterFile .\azuredeploy.parameters.json 
[snip]
Parameters        :
                Name             Type                       Value
                ===============  =========================  ==========
                adminUsername    String                     azureUser
                adminPassword    SecureString
                dnsLabelPrefix   String                     armpocps2
                ubuntuOSVersion  String                     14.04.2-LTS
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;変更内容の確認です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Workspace&amp;gt; Get-AzureRmPublicIpAddress
[snip]
DnsSettings              : {
                             &amp;quot;DomainNameLabel&amp;quot;: &amp;quot;armpocps2&amp;quot;,
                             &amp;quot;Fqdn&amp;quot;: &amp;quot;armpocps2.japanwest.cloudapp.azure.com&amp;quot;
                           }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;問題なく変わっていますね。冪等チックです。この例ではシンプルにDNS名のPrefixを変えましたが、VMインスタンス数やsubnet名を変えたりもできます。関心のある方は&lt;a href=&#34;https://gallery.technet.microsoft.com/Cloud-Consistency-with-0b79b775&#34;&gt;ドキュメント&lt;/a&gt;を。&lt;/p&gt;

&lt;p&gt;増分モードによる変更は期待できそうです。が、さて、ここからが探検です。リソース削除が可能な完全モードを試してみましょう。
リソース定義ファイル(azuredeploy.json)から、大胆にVMの定義を削ってみます。下記リソースをファイルからごっそり消します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;apiVersion&amp;quot;: &amp;quot;[variables(&#39;apiVersion&#39;)]&amp;quot;,
  &amp;quot;type&amp;quot;: &amp;quot;Microsoft.Compute/virtualMachines&amp;quot;,
  &amp;quot;name&amp;quot;: &amp;quot;[variables(&#39;vmName&#39;)]&amp;quot;,
[snip]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;では、完全モード &amp;ldquo;-Mode complete&amp;rdquo;付きでコマンドを再投入します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Workspace&amp;gt; New-AzureRmResourceGroupDeployment -ResourceGroupName ARMEval -TemplateFile .\azuredeploy.json -TemplateParameterFile .\azuredeploy.parameters.json  -Mode complete

確認
Are you sure you want to use the complete deployment mode? Resources in the resource group &#39;ARMEval&#39; which are not included in the template will be deleted.
[Y] はい(Y)  [N] いいえ(N)  [S] 中断(S)  [?] ヘルプ (既定値は &amp;quot;Y&amp;quot;): Y

DeploymentName    : azuredeploy
ResourceGroupName : ARMEval
ProvisioningState : Succeeded
Timestamp         : 2016/01/04 12:01:00
Mode              : Complete
TemplateLink      :
Parameters        :
                Name             Type                       Value
                ===============  =========================  ==========
                adminUsername    String                     azureUser
                adminPassword    SecureString
                dnsLabelPrefix   String                     armpocps2
                ubuntuOSVersion  String                     14.04.2-LTS

Outputs           :
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;あっさり完了しました。本当にVMが消えているが確認します。出力が冗長ですがご容赦ください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Workspace&amp;gt; Find-AzureRmResource -ResourceGroupNameContains ARMEval

Name              : myPublicIP
ResourceId        :     /subscriptions/your-subscription-id/resourceGroups/ARMEval/providers/Microsoft.Network/publicIPAddresses/myPublicIP
ResourceName      : myPublicIP
ResourceType      : Microsoft.Network/publicIPAddresses
ResourceGroupName : ARMEval
Location          : japanwest
SubscriptionId    : your-subscription-id

Name              : myVMNic
ResourceId        : /subscriptions/your-subscription-id/resourceGroups/ARMEval/providers/Microsoft.Network/networkInterfaces/myVMNic
ResourceName      : myVMNic
ResourceType      : Microsoft.Network/networkInterfaces
ResourceGroupName : ARMEval
Location          : japanwest
SubscriptionId    : your-subscription-id

Name              : MyVNET
ResourceId        : /subscriptions/your-subscription-id/resourceGroups/ARMEval/providers/Microsoft.Network/virtualNetworks/MyVNET
ResourceName      : MyVNET
ResourceType      : Microsoft.Network/virtualNetworks
ResourceGroupName : ARMEval
Location          : japanwest
SubscriptionId    : your-subscription-id

Name              : yourstorageaccount
ResourceId        : /subscriptions/your-subscription-id/resourceGroups/ARMEval/providers/Microsoft.Storage/storageAccounts/yourstorageaccount
ResourceName      : yourstorageaccount
ResourceType      : Microsoft.Storage/storageAccounts
ResourceGroupName : ARMEval
Location          : japanwest
SubscriptionId    : your-subscription-id
Tags              : {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;VMだけが消えています。定義からリソースがなくなれば、存在するリソースも消す、これが完全モードです。&lt;/p&gt;

&lt;p&gt;さらに検証。冪等さを求めるのであれば、またリソース定義にVMを加えて再投入したら、涼しい顔で復活してほしい。先ほどazuredeploy.jsonから消したVMリソース定義を、そのまま書き戻して再投入してみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Workspace&amp;gt; New-AzureRmResourceGroupDeployment -ResourceGroupName ARMEval -TemplateFile .\azuredeploy.json -TemplateParameterFile .\azuredeploy.parameters.json  -Mode complete

確認
Are you sure you want to use the complete deployment mode? Resources in the resource group &#39;ARMEval&#39; which are not included in the template will be deleted.
[Y] はい(Y)  [N] いいえ(N)  [S] 中断(S)  [?] ヘルプ (既定値は &amp;quot;Y&amp;quot;): Y

New-AzureRmResourceGroupDeployment : 21:05:52 - Resource Microsoft.Compute/virtualMachines &#39;MyUbuntuVM&#39; failed with message &#39;The resource operation completed with terminal provisioning state &#39;Failed&#39;.&#39;
[snip]
New-AzureRmResourceGroupDeployment : 21:05:52 - One or more errors occurred while preparing VM disks. See disk instance view for details.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;残念ながら失敗しました。どうやらdiskまわりのエラーが発生したようです。&lt;/p&gt;

&lt;p&gt;これは、完全モードでのリソース削除の仕様が原因です。ARMは該当のVMリソースは消すのですが、VMが格納されているストレージを削除しません。リソース作成時は依存関係が考慮されますが、削除時は異なります。&lt;/p&gt;

&lt;p&gt;試しにストレージを消して再実行してみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Workspace&amp;gt; New-AzureRmResourceGroupDeployment -ResourceGroupName ARMEval -TemplateFile .\azuredeploy.json -TemplateParameterFile .\azuredeploy.parameters.json  -Mode complete

[snip]
ProvisioningState : Succeeded
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;定義通りの環境になりました。依存関係をたどって消してほしいのが人情ですが、残したほうがいいケースもあるので、今後の改善を期待しましょう。&lt;/p&gt;

&lt;h2 id=&#34;使い方&#34;&gt;使い方&lt;/h2&gt;

&lt;p&gt;冪等であると言い切れないものの、リソース定義と実行モードを理解したうえで使えば有用。ただ、完全モードによる削除は使い方が難しい。現状ではそんな印象です。&lt;/p&gt;

&lt;p&gt;そこで、ARM Templateをデプロイに組み込む際、ARMによるデプロイはBootstrap用途に限定し、より構成頻度が高いConfiguration用途には、冪等性を持った別のツールを組み合わせるのが現実解と考えます。&lt;/p&gt;

&lt;p&gt;Bootstrap用途では、プラットフォームの提供機能を使ったほうが、機能も多いし最適化されています。Azureで今後この層を担当していくのはARMです。そして、この用途ではChefやAnsibleなど汎用ツールに物足りなさがあります。&lt;/p&gt;

&lt;p&gt;また、Bootstrapは1回切りであるケースが多いので、失敗したらリソースグループをばっさり消して再作成する、と割り切りやすいです。それならば冪等でなくともいいでしょう。&lt;/p&gt;

&lt;p&gt;長くなったので、デプロイツールの組み合わせについては、あたらめて書きたいと思います。&lt;/p&gt;

&lt;p&gt;参考: &lt;a href=&#34;http://mizzy.org/blog/2013/10/29/1/&#34;&gt;インフラ系技術の流れ Bootstrapping/Configuration/Orchestration&lt;/a&gt;&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>OpenStackとAzureにDocker Swarmをかぶせてみた</title>
      <link>https://ToruMakabe.github.io/post/azure_openstack_swarm/</link>
      <pubDate>Sat, 19 Dec 2015 00:01:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_openstack_swarm/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;どこいってもいじられる&#34;&gt;どこいってもいじられる&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.adventar.org/calendars/968&#34;&gt;OpenStack Advent Calendar 2015&lt;/a&gt; 参加作品、19夜目のエントリです。&lt;/p&gt;

&lt;p&gt;OpenStackの最前線から離れて3か月がたちました。OpenStackつながりな方にお会いするたび、マイルドなかわいがりをうけます。ほんとうにありがとうございます。仕事としては専門でなくなりましたが、ユーザ会副会長の任期はまだ残っているので、積極的にいじられに行く所存です。でも笑いながら蹴ったりするのはやめてください。&lt;/p&gt;

&lt;p&gt;さて、毎年参加しているOpenStack Advent Calendarですが、せっかくだからいまの専門とOpenStackを組み合わせたいと思います。ここはひとつ、OpenStackとAzureを組み合わせて何かやってみましょう。&lt;/p&gt;

&lt;h2 id=&#34;乗るしかないこのdockerウェーブに&#34;&gt;乗るしかないこのDockerウェーブに&lt;/h2&gt;

&lt;p&gt;どうせなら注目されている技術でフュージョンしたいですね。2015年を振り返って、ビッグウェーブ感が高かったのはなんでしょう。はい、Dockerです。Dockerを使ってOpenStackとAzureを組み合わせてみます。あまり難しいことをせず、シンプルにサクッとできることを。年末ですし、「正月休みにやってみっか」というニーズにこたえます。&lt;/p&gt;

&lt;p&gt;ところでOpenStack環境はどうやって調達しましょう。ちょっと前までは身の回りに売るほどあったのですが。探さないといけないですね。せっかくなので日本のサービスを探してみましょう。&lt;/p&gt;

&lt;p&gt;条件はAPIを公開していること。じゃないと、Dockerの便利なツール群が使えません。Linuxが動くサービスであれば、Docker環境をしみじみ手作業で夜なべして作れなくもないですが、嫌ですよね。正月休みは修行じゃなくて餅食って酒飲みたい。安心してください、わかってます。人力主義では、せっかくサクサク使えるDockerが台無しです。&lt;/p&gt;

&lt;p&gt;あと、当然ですが個人で気軽にオンラインで契約できることも条件です。&lt;/p&gt;

&lt;p&gt;そうすると、ほぼ一択。&lt;a href=&#34;https://www.conoha.jp/&#34;&gt;Conoha&lt;/a&gt;です。かわいらしい座敷童の&lt;a href=&#34;https://www.conoha.jp/conohadocs/?btn_id=top_footer_conotsu&#34;&gt;&amp;ldquo;このは&amp;rdquo;&lt;/a&gt;がイメージキャラのサービスです。作っているのは手練れなOSSANたちですが。&lt;/p&gt;

&lt;p&gt;では、AzureとConohaにDocker環境をサクッと作り、どちらにもサクッと同じコンテナを作る。もちろん同じCLIから。ということをしてみようと思います。&lt;/p&gt;

&lt;p&gt;今回大活躍するDoker Machine、Swarmの説明はしませんが、関心のある方は&lt;a href=&#34;http://www.slideshare.net/zembutsu/whats-new-aobut-docker-2015-network-and-orchestration&#34;&gt;前佛さんの資料&lt;/a&gt;を参考にしてください。&lt;/p&gt;

&lt;h2 id=&#34;ローカル環境&#34;&gt;ローカル環境&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Mac OS X (El Capitan)

&lt;ul&gt;
&lt;li&gt;Docker Toolbox 1.9.1&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ローカル、Azure、ConohaすべてのDocker環境はDocker Machineでサクッと作ります。
また、Swarmのマスタはローカルに配置します。&lt;/p&gt;

&lt;h2 id=&#34;いざ実行&#34;&gt;いざ実行&lt;/h2&gt;

&lt;p&gt;まず、Docker Machineにクラウドの諸設定を食わせます。&lt;/p&gt;

&lt;p&gt;Azure向けにサブスクリプションIDとCertファイルの場所を指定します。詳細は&lt;a href=&#34;https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-docker-machine/&#34;&gt;ここ&lt;/a&gt;を。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export AZURE_SUBSCRIPTION_ID=hoge-fuga-hoge-fuga-hoge
$ export AZURE_SUBSCRIPTION_CERT=~/.ssh/yourcert.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Conoha向けにOpenStack関連の環境変数をセットします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export OS_USERNAME=yourname
$ export OS_TENANT_NAME=yourtenantname
$ export OS_PASSWORD=yourpass
$ export OS_AUTH_URL=https://identity.tyo1.conoha.io/v2.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次はローカルコンテナ環境を整えます。&lt;/p&gt;

&lt;p&gt;Swarmコンテナを起動し、ディスカバリトークンを生成します。このトークンがSwarmクラスタの識別子です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine create -d virtualbox local
$ eval &amp;quot;$(docker-machine env local)&amp;quot;
$ docker run swarm create    
Status: Downloaded newer image for swarm:latest
tokentokentokentoken
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このトークンは控えておきましょう。&lt;/p&gt;

&lt;p&gt;ではSwarmのマスタをローカルに作ります。先ほど生成したトークン指定を忘れずに。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine create -d virtualbox --swarm --swarm-master --swarm-discovery token://tokentokentokentoken head
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SwarmのエージェントをAzureに作ります。VMを作って、OSとDockerをインストールして、なんて不要です。Docker Machineがやってくれます。ここでもトークン指定を忘れずに。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ eval &amp;quot;$(docker-machine env head)&amp;quot;
$ docker-machine create -d azure --swarm --swarm-discovery token://tokentokentokentoken worker-azure01 --azure-location &amp;quot;East Asia&amp;quot; worker-azure00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Conohaにも同様に。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine create -d openstack --openstack-flavor-name g-1gb --openstack-image-name vmi-ubuntu-14.04-amd64 --openstack-sec-groups &amp;quot;default,gncs-ipv4-all&amp;quot; --swarm --swarm-discovery token://tokentokentokentoken worker-conoha00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;さあ環境がサクッと出来上がりました。これ以降はSwarmクラスタ全体を操作対象にします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ eval &amp;quot;$(docker-machine env --swarm head)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;環境をチラ見してみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker info
Containers: 4
Images: 3
 Role: primary
 Strategy: spread
 Filters: health, port, dependency, affinity, constraint
 Nodes: 3
 head: 192.168.99.101:2376
  └ Containers: 2
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.021 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.1.13-boot2docker, operatingsystem=Boot2Docker 1.9.1 (TCL 6.4.1); master : cef800b - Fri Dec 18 19:33:59 UTC 2015, provider=virtualbox, storagedriver=aufs
 worker-azure00: xxx.cloudapp.net:2376
  └ Containers: 1
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.721 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=3.13.0-36-generic, operatingsystem=Ubuntu 14.04.1 LTS, provider=azure, storagedriver=aufs
 worker-conoha00: www.xxx.yyy.zzz:2376
  └ Containers: 1
  └ Reserved CPUs: 0 / 2
  └ Reserved Memory: 0 B / 1.019 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-51-generic, operatingsystem=Ubuntu 14.04.3 LTS, provider=openstack, storagedriver=aufs
CPUs: 4
Total Memory: 3.761 GiB
Name: 1234abcd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;どこにどんな環境が作られたかが分かりますね。出力結果の4行目&amp;rdquo;Strategy: spread&amp;rdquo;を覚えておいてください。&lt;/p&gt;

&lt;p&gt;ではコンテナを作ってみましょう。Nginxコンテナ三連星です。どの環境に作るか、という指定はしません。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ for i in `seq 1 3`; do docker run -d -p 80:80 nginx; done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;どんな具合でしょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                NAMES
9cc2f5594fa5        nginx               &amp;quot;nginx -g &#39;daemon off&amp;quot;   5 seconds ago       Up 4 seconds        192.168.99.101:80-&amp;gt;80/tcp, 443/tcp   head/goofy_goldberg
b9d54d794a85        nginx               &amp;quot;nginx -g &#39;daemon off&amp;quot;   32 seconds ago      Up 31 seconds       www.xxx.yyy.zzz:80-&amp;gt;80/tcp, 443/tcp   worker-conoha00/clever_chandrasekhar
19e9d0e229a2        nginx               &amp;quot;nginx -g &#39;daemon off&amp;quot;   45 seconds ago      Up 42 seconds       zzz.yyy.xxx.www:80-&amp;gt;80/tcp, 443/tcp    worker-azure00/reverent_bhaskara
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nginxコンテナがきれいに散らばっているのが分かります。これは先ほど覚えた&amp;rdquo;Strategy: spread&amp;rdquo;が効いているからです。StrategyはSwarmのコンテナ配置ポリシーで、speradを指定すると散らしにいきます。Strategyをbinpackにしておけば、ノードを埋めようとします。埋まったら他、です。randomであれば、ランダムに。&lt;/p&gt;

&lt;p&gt;まだシンプルですが、今後このStrategyやリソース管理が賢くなると、「ローカルが埋まったら、リモートを使う」とか、使い道が広がりそうですね。最近Docker社が買収した&lt;a href=&#34;https://www.tutum.co/&#34;&gt;Tutum&lt;/a&gt;との関係、今後どう進化していくのか、注目です。&lt;/p&gt;

&lt;h2 id=&#34;ツールから入るハイブリッドクラウドも-またよし&#34;&gt;ツールから入るハイブリッドクラウドも、またよし&lt;/h2&gt;

&lt;p&gt;ハイブリッドクラウドはまだ言葉先行です。まだクラウドを使ってない、使いこなしていない段階でツールの話だけが先行することも多いです。ナイフとフォークしか使ったことのない人が、お箸を使う和食や中華を選ぶ前に「どんなお箸がいいかねぇ」と議論している感じ。僕は、そうじゃなくて、その前に食べたいもの = クラウドを選びましょうよ、というスタンスでした。&lt;/p&gt;

&lt;p&gt;でも、コンテナ+Dockerって、お箸に弁当ついてきたような感じなんですよね。お箸が使える人であれば、弁当持ち込める場所さえ確保すればいい。インパクトでかいです。ちょっと考えを改めました。&lt;/p&gt;

&lt;p&gt;もちろん、だからクラウドは何でもいい、と言っているわけではありません。弁当持ち込みとしても、スペースが広い、個室で静か、お茶がうまい、お茶がタダ、揚げたてのから揚げを出してくれる、などなど、特徴は出てくるでしょう。APIを公開していないような「持ち込みやめて」のクラウドは、先々心配ですが。&lt;/p&gt;

&lt;p&gt;簡単 = 正義です。簡単であれば使う人が増えて、要望が増えて、育ちます。かっちり感は後からついてくる。もしDockerで複数のクラウド環境を簡単に使いこなせるようになるのであれば、順番が逆ではありますが、お箸、Dockerというツールから入るのもいいかもしれません。&lt;/p&gt;

&lt;p&gt;まずは開発、検証環境など、リスク低いところから試して慣れていくのがおすすめです。触っていくうちに、いろいろ見えてくるでしょう。Dockerはもちろんですが、それぞれのクラウドの特徴も。&lt;/p&gt;

&lt;p&gt;OpenStackもAzureも、特徴を活かし、うまく使いこなしてほしいと思っております。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Azure Docker VM Extensionを使う3つの理由</title>
      <link>https://ToruMakabe.github.io/post/azure_docker_extension/</link>
      <pubDate>Thu, 05 Nov 2015 15:40:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/azure_docker_extension/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;まずはじめに&#34;&gt;まずはじめに&lt;/h2&gt;

&lt;p&gt;先月からMicrosoftで働いてます。Azure担当のソリューションアーキテクトになりました。これからAzureネタが増えると思いますが、ひとつよろしくお願いします。Microsoftテクノロジーとオープンソースの間あたりを、積極的にこすっていく所存です。&lt;/p&gt;

&lt;p&gt;もちろん、技術者個人として、中立的に、公開できるネタを書きます。&lt;/p&gt;

&lt;p&gt;AzureはMicrosoftテクノロジーとオープンソースの交差点です。できないと思っていたことが、実はできたりします。いまだに「AzureでLinux動くのね、知らなかった」と言われたり。また、その逆もしかり。SDKが色々あるからできると思っていたら、制約があった、とか。&lt;/p&gt;

&lt;p&gt;なので、小ネタであっても、実践的な情報には価値があります。今後、公式ドキュメントでカバーされなかったり、細かすぎて伝わりづらいなことを、書いていこうかと。&lt;/p&gt;

&lt;h2 id=&#34;azure-docker-vm-extension-を使う3つの理由&#34;&gt;Azure Docker VM Extension を使う3つの理由&lt;/h2&gt;

&lt;p&gt;さて、今回は話題沸騰のDocker関連のネタ、&lt;a href=&#34;https://github.com/Azure/azure-docker-extension&#34;&gt;Azure Docker VM Extension&lt;/a&gt;について。名前通り、Azure上でDockerをのせたVMを動かすときに便利な拡張機能です。&lt;/p&gt;

&lt;p&gt;このDocker VM Extension、AzureのARMテンプレートによく登場します。なんとなくおすすめっぽいです。ですが「自分でDockerをインストールするのと何が違うのよ」という疑問も、あるかと思います。実際、よく聞かれます。&lt;/p&gt;

&lt;p&gt;ずばり、答えはGithubの&lt;a href=&#34;https://github.com/Azure/azure-docker-extension&#34;&gt;README&lt;/a&gt;にまとまっています。この拡張機能のうれしさは、&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Docker EngineのStable最新版をインストールしてくれる&lt;/li&gt;
&lt;li&gt;Docker デーモンの起動オプションや認証まわりを設定できる (オプション)

&lt;ul&gt;
&lt;li&gt;ポートマッピング、認証まわり、Docker Registoryサーバの定義など&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Docker Composeのパラメータを渡すことができる (オプション)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;以上です。2と3はJSONで記述できます。要するに、毎度山ほどオプションつけてdockerコマンド打つよりは、宣言的にDockerを楽に使えますよ、ということです。必須ではありません。また、山ほどあるDockerのオプションを隅々まで網羅しているわけではありません。カバー範囲は基本的なところです。&lt;/p&gt;

&lt;p&gt;Dockerの環境構築、はじめはコマンドを打つことをおすすめします。オプションがいろいろあるので、その中身を理解することには意味があります。&lt;/p&gt;

&lt;p&gt;ですが、一度理解したあとは、かったるいことこの上ないので、この手のツールはあったほうがいいですね。&lt;/p&gt;

&lt;p&gt;Dockerは本家のみならずエコシステムも急激に変化しているので、まだ環境構築ツールのファイナルアンサーはないでしょう。どれを学ぶか悩ましいところです。ですが、この拡張は気軽に使えますし、依存性も低いので、おすすめです。&lt;/p&gt;

&lt;p&gt;なお、このDocker拡張、ARM属性で言うpublisherは&amp;rdquo;Microsoft.Azure.Extensions&amp;rdquo;ですが、古い&amp;rdquo;MSOpenTech.Extensions&amp;rdquo;を指定しているARMテンプレートがまだあったりします。拡張のインストール時に「そんなのねぇよ」と怒られたら、疑ってみてください。伝統を重んじるUSのリージョンでは動いて、Japanで動かないテンプレートでは、MSOpenTechが指定されているかもしれません。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Hugoへ移行</title>
      <link>https://ToruMakabe.github.io/post/migrate-to-hugo/</link>
      <pubDate>Sun, 20 Sep 2015 15:27:03 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/migrate-to-hugo/</guid>
      <description></description>
      
      <content>

&lt;h2 id=&#34;jekyllからhugoへ移行&#34;&gt;JekyllからHugoへ移行&lt;/h2&gt;

&lt;p&gt;サイトジェネレータをJekyllからHugoに変えました。深い理由はありません。気分転換です。Githubでソース管理、Werckerで自動ビルド、最後にGithub Pagesにデプロイするフローを作りました。&lt;/p&gt;

&lt;p&gt;移行にあたり、Jekyllの前(Blogger)に使っていたフォーマットを変換するのが面倒だったので、その時代のエントリーをえいっと削除しました。リンクいただいたみなさん、すいません。内容も古くなっていたので、リフレッシュのいい機会ということで、ご容赦を。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>いきなり Terraform OpenStack Provider</title>
      <link>https://ToruMakabe.github.io/post/terraform-openstack-minimum/</link>
      <pubDate>Sat, 04 Apr 2015 00:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/terraform-openstack-minimum/</guid>
      <description></description>
      
      <content>

&lt;h3 id=&#34;terraform-0-4でopenstack-providerリリース&#34;&gt;Terraform 0.4でOpenStack Providerリリース&lt;/h3&gt;

&lt;p&gt;以前からOpenStack対応は表明されていたのですが、いよいよ&lt;a href=&#34;https://hashicorp.com/blog/terraform-0-4.html&#34;&gt;v0.4&lt;/a&gt;でリリースされました。&lt;/p&gt;

&lt;h3 id=&#34;小さくはじめましょう&#34;&gt;小さくはじめましょう&lt;/h3&gt;

&lt;p&gt;この手のツールを試すときは、はじめから欲張ると苦労します。最小限の設定でひとまず動かすとクイックに幸せが訪れます。目標は10分。&lt;/p&gt;

&lt;h3 id=&#34;テストした環境&#34;&gt;テストした環境&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Terraform 0.4&lt;/li&gt;
&lt;li&gt;Mac OS 10.10.2&lt;/li&gt;
&lt;li&gt;HP Helion Public Cloud&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;openstackerのみだしなみ-環境変数&#34;&gt;OpenStackerのみだしなみ、環境変数&lt;/h3&gt;

&lt;p&gt;下記、環境変数はセットされてますよね。要確認。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;OS_AUTH_URL&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;OS_USERNAME&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;OS_PASSWORD&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;OS_REGION_NAME&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;OS_TENANT_NAME&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;最小限の構成ファイル&#34;&gt;最小限の構成ファイル&lt;/h3&gt;

&lt;script type=&#34;text/javascript&#34; src=&#34;http://gist.github.com/977209064bcfda66d085.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;これだけ。Providerの設定は書かなくていいです。Terraformは環境変数を見に行きます。Resource部は、最小限ということで、まずはインスタンスを起動し、Floating IPをつけるとこまで持っていきましょう。&lt;/p&gt;

&lt;h3 id=&#34;さあ実行&#34;&gt;さあ実行&lt;/h3&gt;

&lt;p&gt;まずはterraform planコマンドで、実行計画を確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ terraform plan
Refreshing Terraform state prior to plan...


The Terraform execution plan has been generated and is shown below.
Resources are shown in alphabetical order for quick scanning. Green resources
will be created (or destroyed and then created if an existing resource exists), yellow resources are being changed in-place, and red resources will be destroyed.

Note: You didn&#39;t specify an &amp;quot;-out&amp;quot; parameter to save this plan, so when &amp;quot;apply&amp;quot; is called, Terraform can&#39;t guarantee this is what will execute.

+ openstack_compute_instance_v2.sample-server
    access_ip_v4:      &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;
    access_ip_v6:      &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;
    flavor_id:         &amp;quot;&amp;quot; =&amp;gt; &amp;quot;my_flavor_id&amp;quot;
    flavor_name:       &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;
    floating_ip:       &amp;quot;&amp;quot; =&amp;gt; &amp;quot;aaa.bbb.ccc.ddd&amp;quot;
    image_id:          &amp;quot;&amp;quot; =&amp;gt; &amp;quot;my_image_id&amp;quot;
    image_name:        &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;
    key_pair:          &amp;quot;&amp;quot; =&amp;gt; &amp;quot;my_keypair&amp;quot;
    name:              &amp;quot;&amp;quot; =&amp;gt; &amp;quot;tf-sample&amp;quot;
    network.#:         &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;
    region:            &amp;quot;&amp;quot; =&amp;gt; &amp;quot;my_region&amp;quot;
    security_groups.#: &amp;quot;&amp;quot; =&amp;gt; &amp;quot;1&amp;quot;
    security_groups.0: &amp;quot;&amp;quot; =&amp;gt; &amp;quot;my_sg&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;定義通りに動きそうですね。では実行。applyです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ terraform apply  
openstack_compute_instance_v2.sample-server: Creating...  
    access_ip_v4:      &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;  
    access_ip_v6:      &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;  
    flavor_id:         &amp;quot;&amp;quot; =&amp;gt; &amp;quot;my_flavor&amp;quot;  
    flavor_name:       &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;  
    floating_ip:       &amp;quot;&amp;quot; =&amp;gt; &amp;quot;aaa.bbb.ccc.ddd&amp;quot;  
    image_id:          &amp;quot;&amp;quot; =&amp;gt; &amp;quot;my_image_id&amp;quot;  
    image_name:        &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;  
    key_pair:          &amp;quot;&amp;quot; =&amp;gt; &amp;quot;my_keypair&amp;quot;  
    name:              &amp;quot;&amp;quot; =&amp;gt; &amp;quot;tf-sample&amp;quot;  
    network.#:         &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;  
    region:            &amp;quot;&amp;quot; =&amp;gt; &amp;quot;my_region&amp;quot;
    security_groups.#: &amp;quot;&amp;quot; =&amp;gt; &amp;quot;1&amp;quot;
    security_groups.0: &amp;quot;&amp;quot; =&amp;gt; &amp;quot;my_sg&amp;quot;
openstack_compute_instance_v2.test-server: Creation complete

Apply complete! Resources: 1 added, 0 changed, 0 destroyed.

The state of your infrastructure has been saved to the path below. This state is required to modify and destroy your infrastructure, so keep it safe. To inspect the complete state use the `terraform show` command.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;とても楽ちんですね。あとはオプションを追加して込み入った構成に挑戦してみてください。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>君はOpenStack Monascaを知っているか</title>
      <link>https://ToruMakabe.github.io/post/monasca/</link>
      <pubDate>Fri, 12 Dec 2014 00:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/monasca/</guid>
      <description></description>
      
      <content>

&lt;p&gt;このエントリーは、&lt;a href=&#34;http://www.adventar.org/calendars/602&#34;&gt;OpenStack (2枚目) Advent Calendar 2014&lt;/a&gt;の12夜目担当作品です。&lt;/p&gt;

&lt;h3 id=&#34;monitoring-as-a-service&#34;&gt;Monitoring as a Service&lt;/h3&gt;

&lt;p&gt;監視をサービスとして提供するって、どういうことでしょうか。&lt;/p&gt;

&lt;p&gt;[Monitoring]&lt;br /&gt;
従来の監視。担当者が事前に監視項目を定義する。静的。&lt;/p&gt;

&lt;p&gt;[Monitoring as a Service]
監視機能をサービスとして提供する。不特定多数のユーザーが、自分の監視したい測定項目を定義し、自分の好きなタイミングでチェックする。GUIはもちろん、APIでデータ取得できる。動的。&lt;/p&gt;

&lt;p&gt;まあ、AWSのCloudWatchみたいなものです。先に言うべきでしたね、すいません。&lt;/p&gt;

&lt;p&gt;このMonitoring as a Service、技術的なハードルは結構高いんです。刻々と上がってくるイベントをさばき、蓄積し、APIをバシバシ叩くユーザーリクエストに応えなきゃいけない。監視というと裏方のイメージがありますが、これは、対価をいただくに値する、立派なサービスです。&lt;/p&gt;

&lt;p&gt;そこでOpenStackのMonitoring as a Service事情はどうでしょうか。一見、それを実現できそうなCeilometerがあります。ただ、もともとCeilomerは課金のための利用情報収集をする、という生まれなので、マルチテナントで、ユーザーが自らメトリックを定義し、チェックするという使い方に向いていません。ユーザー向けというより、管理者向けなんです。&lt;/p&gt;

&lt;p&gt;そこで&lt;a href=&#34;https://wiki.openstack.org/wiki/Monasca&#34;&gt;Monasca&lt;/a&gt;の登場です。まだ正式機能ではありませんが、いずれ昇格するのでは、と個人的に期待しています。&lt;/p&gt;

&lt;p&gt;では、アーキテクチャーを見てみましょう。&lt;br /&gt;
&lt;img src=&#34;https://wiki.openstack.org/w/images/4/4a/Monasca-arch-component-diagram.png&#34; alt=&#34;MonascaArc&#34; /&gt;&lt;/p&gt;

&lt;p&gt;ひゃー、ワクワクしますがちょっと重いですね。特にイベントを処理するメッセージキュー、イベントを貯めるDBは工夫が要りそうです。現時点で、キューにはApache Kafka、DBにはカラムナーDBのVerticaや、時系列DBのInflux DBがサポートされています。正直、無理目のスタックです。&lt;/p&gt;

&lt;p&gt;と思っていたら。&lt;/p&gt;

&lt;p&gt;なんと、&lt;a href=&#34;https://github.com/stackforge/monasca-vagrant&#34;&gt;Monasca-Vagrant&lt;/a&gt;なんてものができているじゃありませんか。VagrantとAnsibleでサクっと環境を作れるとな。まじか。本当か。本当だった。1時間くらいでできた。&lt;/p&gt;

&lt;h3 id=&#34;気をつけること&#34;&gt;気をつけること&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;動作実績のあるわたしの環境は、MacBook Pro Late 2013 / 2.3 GHz Intel Core i7、メモリ16GB、Yosemite。&lt;/li&gt;
&lt;li&gt;Vagrantfileを見る限り、メモリ7GBと6GBのVMを作る。ここいじって動くかは要検証。&lt;/li&gt;
&lt;li&gt;git cloneしたディレクトリ直下にansibleのrequirementファイルが置かれるので、そこで作業&lt;/li&gt;
&lt;li&gt;vagrant upで2つのVM、devstackとmini-monが作られる、ここは時間と帯域がいるので、スタバな人は要注意&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;気をつけるのはこれくらいです。レッツトライ。&lt;/p&gt;

&lt;p&gt;年末年始休暇のお楽しみが増えましたね。&lt;/p&gt;

&lt;p&gt;これでわたしの2014年Advent Calendarシリーズは完了です。メリークリスマス &amp;amp; 良いお年を。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>OpenStackと長期バージョン固定</title>
      <link>https://ToruMakabe.github.io/post/longtermsupport/</link>
      <pubDate>Tue, 09 Dec 2014 00:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/longtermsupport/</guid>
      <description></description>
      
      <content>

&lt;p&gt;このエントリーは、&lt;a href=&#34;http://www.adventar.org/calendars/602&#34;&gt;OpenStack (2枚目) Advent Calendar 2014&lt;/a&gt;の9夜目担当作品です。&lt;/p&gt;

&lt;h3 id=&#34;ソフトウェア-バージョン-サポート&#34;&gt;ソフトウェア、バージョン、サポート&lt;/h3&gt;

&lt;p&gt;たいていのソフトウェアには、バージョンがあります。そしてそれぞれのソフトウェアには「直近2バージョンをサポートする。ユーザーがそれよりも古いバージョンを使いたい場合、ベストエフォートで対応する。サポート対象外のバージョンで不具合対応ができるかどうかは、場合による。」なんていうポリシーがあったりします。&lt;/p&gt;

&lt;h3 id=&#34;進化著しいソフト-openstackでは&#34;&gt;進化著しいソフト、OpenStackでは&lt;/h3&gt;

&lt;p&gt;OpenStackは現在、半年ごとにアップデートします。進化が早いです。そして&lt;a href=&#34;https://wiki.openstack.org/wiki/Releases&#34;&gt;公式サイト&lt;/a&gt;を見て分かるとおり、直近2バージョンがサポート対象です。ちょっと短いですね。長期サポートよりも新規開発を優先しているわけですが、「もうちょっと長くサポートしてくれんか」というのが人情でしょう。&lt;/p&gt;

&lt;h3 id=&#34;でも-長期バージョン固定するとどうなるか&#34;&gt;でも、長期バージョン固定するとどうなるか&lt;/h3&gt;

&lt;p&gt;では仮に「そのバージョンがリリースされてから3年間、同じバージョンで運用する」というポリシーでクラウドを作ったとしましょう。その間に、5〜6バージョン、進化してしまうわけですが。以下、ちょっと未来の想像です。&lt;/p&gt;

&lt;p&gt;[とあるクラウド その1]&lt;br /&gt;
- (Dev)  今度のシステムでAっちゅうライブラリ使いたいんだけど、OpenStackだと、サポートがLからなんだよね。&lt;br /&gt;
- (Ops) あー、うちの環境Jよ。&lt;br /&gt;
- (Dev) そうすか。じゃあ他のにするわ。&lt;/p&gt;

&lt;p&gt;使われないクラウド。悲しい。これからOpenStackに対応したアプリやライブラリ、たくさん出てきそうなのに。&lt;/p&gt;

&lt;p&gt;[とあるクラウド その2]&lt;br /&gt;
- (Ops) うちはJで3年間バージョン固定、長期サポートです!! アップデート作業のために環境を止めたりしません!!&lt;br /&gt;
- (Dev) おーいいね。決定。3年のんびりするわ。&lt;br /&gt;
〜3年後〜&lt;br /&gt;
- (Ops) 約束の3年です。長年放置したのでバージョンアップは大手術です。システム止めます!!&lt;br /&gt;
- (Dev) いやいやいやいや、アプリも運用も、そんな準備できてないし。&lt;/p&gt;

&lt;p&gt;リスクの先送りと大噴火。「小さな変更をこまめに行い、リスクを最小化する。人もプロセスも、アプリの作りも、変化に強くなる。」という、最近のDevOpsなりCI/CDといったトレンドとは逆のやり口です。&lt;/p&gt;

&lt;h3 id=&#34;アップデートの仕組みに投資したほうが建設的と思う&#34;&gt;アップデートの仕組みに投資したほうが建設的と思う&lt;/h3&gt;

&lt;p&gt;もちろん、OpenStackの開発が落ち着いてきたら、長期のバージョン固定サポートは価値が高いと思います。ただし、イノベーションを求めて活発に開発しているソフトでは、結局それはユーザーにとって不利益になるのではないでしょうか。&lt;/p&gt;

&lt;p&gt;それよりは、アップデーターの開発、複数コントロールプレーンの平行運用の確立、アプリや運用でも対応するなど、「変化を受け入れる」ほうが建設的なのではと考える次第です。&lt;/p&gt;

&lt;p&gt;最後に、&lt;a href=&#34;http://superuser.openstack.org/articles/openstack-user-survey-insights-november-2014&#34;&gt;最新のOpenStack User Survey&lt;/a&gt;を紹介します。注目はBusiness Driverです。OpenStackを使う、動機です。&lt;/p&gt;

&lt;p&gt;OpenStackのBusiness Driverとして、最もユーザーが重視しているのは、&amp;rdquo;Ability to innovate&amp;rdquo;なんですよね。
あまり変化なく、3年とか5年とか、言葉は悪いですが、塩漬けで使うような従来型システムとは、優先すべきところが違うのではなかろうかと。&lt;/p&gt;

&lt;p&gt;メインフレームから、クライアント/サーバー、そしてWebと、テクノロジーリフレッシュの機会が、これまではありました。そろそろ、リフレッシュしてみませんか。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>ハンサムOpenStack</title>
      <link>https://ToruMakabe.github.io/post/handsome-openstack/</link>
      <pubDate>Sat, 06 Dec 2014 00:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/handsome-openstack/</guid>
      <description></description>
      
      <content>

&lt;p&gt;このエントリーは、&lt;a href=&#34;http://www.adventar.org/calendars/602&#34;&gt;OpenStack (2枚目) Advent Calendar 2014&lt;/a&gt;の6夜目担当作品です。&lt;/p&gt;

&lt;h3 id=&#34;出オチ&#34;&gt;出オチ&lt;/h3&gt;

&lt;p&gt;OpenStackも人気が出て、Advent Calendarが1枚ではおさまさなくなりました。2枚目です。ハンサムです。だからハンサムOpenStackです。&lt;/p&gt;

&lt;p&gt;こんなテーマで何か書けるんでしょうか? 何をおっしゃる、芸術とは制約から生まれるのです。&lt;/p&gt;

&lt;h3 id=&#34;そもそも2枚目とは&#34;&gt;そもそも2枚目とは&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;「二枚目」という用語は、歌舞伎用語をもとに、江戸時代に生まれた。歌舞伎の看板は、通常は8枚から成っていた。一枚目の看板は「書き出し」と言われ、主役の名が書かれ、二枚目の看板には若い色男の役者の名が書かれることになっていた。また、三枚目の看板には道化役の名が書かれることになっていた。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://ja.wikipedia.org/wiki/%E4%BA%8C%E6%9E%9A%E7%9B%AE&#34;&gt;「二枚目」（2013年5月28日 (火) 02:17 UTCの版）『ウィキペディア日本語版』&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;8枚あるらしいぞ-いじってみよう&#34;&gt;8枚あるらしいぞ いじってみよう&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;一枚目：主役：そのまま主役。「一枚看板」という用法もある。&lt;br /&gt;
&lt;em&gt;Novaですね。主役です。&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;二枚目：色男：優男で色事担当&lt;br /&gt;
&lt;em&gt;これはあとにとっておきましょう。&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;三枚目：道化：お笑い担当&lt;br /&gt;
&lt;em&gt;Glanceですね。何も変なことしてないのにネタにされる。&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;四枚目：中軸：中堅役者　まとめ役&lt;br /&gt;
&lt;em&gt;Cinderでしょうか。主役のNovaを活かす名脇役。&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;五枚目：敵役：一般的な敵役&lt;br /&gt;
&lt;em&gt;Heatかな。はじめのCloudFormation形式ってのが気に入らなかった。HOTが出てきたので許す。&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;六枚目：実敵：憎めない善要素のある敵役&lt;br /&gt;
&lt;em&gt;Ceilometerです。重いです。絶賛チューニング中です。&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;七枚目：実悪：巨悪　ラスボス　全ての悪事の黒幕&lt;br /&gt;
&lt;em&gt;Neutron。でもまあ、Neutronに罪はないか。取り巻きが良くなかったのよきっと。これから良くなるよ。&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;八枚目：座長：元締め&lt;br /&gt;
&lt;em&gt;Swiftで決まり。ほとばしる安定感。というかAWSに依存したS3互換製品とかやめてみんなオープンなSwift互換にするといいよ。&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;じゃあ二枚目は&#34;&gt;じゃあ二枚目は?&lt;/h3&gt;

&lt;p&gt;*Trove*です。若さと期待の大きさを込めてキャスティングしました。というか&lt;a href=&#34;http://www.publickey1.jp/blog/14/paasdbaasapaas12amazonidc_japan.html&#34;&gt;これ&lt;/a&gt;を見ても分かるとおり、当面PaaSと言えばDBaaSです。DBの構築とか運用面倒ですものね。Troveはレプリケーション機能が追加されたり、いよいよこれから本格化と思います。&lt;/p&gt;

&lt;p&gt;2枚目のカレンダーなので、ネタ感あふれる副音声モードでお届けしました。ではメリークリスマス。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>OpenStackのツール環境をImmutableに整える</title>
      <link>https://ToruMakabe.github.io/post/openstack-tools/</link>
      <pubDate>Sun, 14 Sep 2014 00:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/openstack-tools/</guid>
      <description></description>
      
      <content>

&lt;h3 id=&#34;タイトルは釣りです&#34;&gt;タイトルは釣りです&lt;/h3&gt;

&lt;p&gt;すいません。でも、日本のどこかに、わたしを待ってる、理解し合える人がいらっしゃると思います。&lt;/p&gt;

&lt;h3 id=&#34;なぜ必要か&#34;&gt;なぜ必要か?&lt;/h3&gt;

&lt;p&gt;いけてるOpenStackerは、相手にするOpenStack環境がオンプレであろうがパブリッククラウドであろうが、すぐにコマンド叩いて「なるほどこの環境は。。。ニヤリ」とできるものです。そういうものです。&lt;/p&gt;

&lt;h3 id=&#34;やりたいこと&#34;&gt;やりたいこと&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;OpenStack CLIなどのツールを詰め込んだ環境を、必要な時に、すぐ使いたい・作りたい&lt;/li&gt;
&lt;li&gt;Windows、Macどちらでも同様の環境にしたい&lt;/li&gt;
&lt;li&gt;相手にするOpenStackがオンプレでも、パブリッククラウドでも、また、ツールがぶら下がっているネットワーク環境の違いも、設定やスクリプトで吸収&lt;/li&gt;
&lt;li&gt;Windows、Mac環境を汚さない、また、汚されない&lt;/li&gt;
&lt;li&gt;コマンド2、3発程度で、気軽に作って消せる&lt;/li&gt;
&lt;li&gt;VMできたらすぐログイン、即OpenStack CLIが使える&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;方針&#34;&gt;方針&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;OpenStackの各種ツールを動かすOSはLinuxとし、VM上に作る&lt;/li&gt;
&lt;li&gt;VagrantでWindows/Macの違いを吸収する&lt;/li&gt;
&lt;li&gt;VMイメージをこねくり回さず、常にまっさらなベースOSに対し構成管理ツールでプロビジョニングを行う&lt;/li&gt;
&lt;li&gt;構成管理ツールはAnsibleを使う(本を買ったので、使いたかっただけ)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;前提条件&#34;&gt;前提条件&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Windows 8.1 &amp;amp; VMware Worksation 10.0.3&lt;/li&gt;
&lt;li&gt;OSX 10.9.4 &amp;amp; VirtualBox 4.3.16&lt;/li&gt;
&lt;li&gt;Vagrant 1.6.5  (VMware用ライセンス買いました)&lt;/li&gt;
&lt;li&gt;ひとまずOpenStack CLIを使えるところまで作る&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ではvagrantfileを見てみましょう&#34;&gt;ではVagrantfileを見てみましょう&lt;/h3&gt;

&lt;script type=&#34;text/javascript&#34; src=&#34;http://gist.github.com/a470e86a1477cd76d4f4.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;これがわたしが作ったVagrantfileです。見ての通りですが、以下に補足します。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;VMwareとVirtualBoxでなるべく環境を合わせるため、opscodeの&lt;a href=&#34;https://github.com/opscode/bento&#34;&gt;Bento&lt;/a&gt;で、事前にboxファイルを作ってます。ubuntu14.04としました。&lt;/li&gt;
&lt;li&gt;実行ディレクトリにprovision.shを置きます。&lt;/li&gt;
&lt;li&gt;provision.shでubuntuへansibleをインストールし、追って入れたてホヤホヤのansibleで環境を整えます。&lt;/li&gt;
&lt;li&gt;実行ディレクトリ内のansibleディレクトリに、ansibleのplaybook(site.yml)と変数定義ファイル(vars/env.yml)を置きます。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;hostsファイルには以下のようにlocalhostを定義します。&lt;/p&gt;

&lt;p&gt;[localhost]&lt;br /&gt;
127.0.0.1 ansible_connection=local&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;provision-sh解説&#34;&gt;provision.sh解説&lt;/h4&gt;

&lt;script type=&#34;text/javascript&#34; src=&#34;http://gist.github.com/57ae9f8edbe6cf30cd16.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;ansibleのインストールとplaybookの実行。playbookの実行が回りくどい感じなのは、Vagrantのフォルダ同期機能でパーミッションが正しく設定できなかったゆえのワークアラウンドです。&lt;/p&gt;

&lt;h4 id=&#34;playbook-site-yml-解説&#34;&gt;playbook(site.yml)解説&lt;/h4&gt;

&lt;script type=&#34;text/javascript&#34; src=&#34;http://gist.github.com/6c5d8ae296948b8d4070.js&#34;&gt;&lt;/script&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;varsディレクトリ配下に、環境変数を定義したenv.ymlを置きます。ここで対象のOpenStack環境を指定します。&lt;/p&gt;

&lt;p&gt;OS_TENANT_NAME: your_tenant_name&lt;br /&gt;
OS_USERNAME: your_username&lt;br /&gt;
&amp;hellip;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;という感じで並べてください。.bashrcに追加されます。
- タイムゾーンをAsia/Tokyoにします。
- 必要なパッケージ、pipの導入後、OpenStack CLI群をインストールします。&lt;/p&gt;

&lt;h3 id=&#34;windowsでの実行例&#34;&gt;Windowsでの実行例&lt;/h3&gt;

&lt;p&gt;Vagrant &amp;amp; AnsibleはMacの情報が多いので、ここではWindowsでの実行例を。PowerShellを管理者権限で起動し、Vagrantfileやprovision.sh、ansible関連ファイルが住むディレクトリでvagrant up。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Users\hoge&amp;gt; vagrant up
Bringing machine &#39;default&#39; up with &#39;vmware_workstation&#39; provider...
==&amp;gt; default: Cloning VMware VM: &#39;opscode-ubuntu1404&#39;. This can take some time...
(snip)
==&amp;gt; default: TASK: [install OpenStack CLIs] ************************************************
==&amp;gt; default: changed: [127.0.0.1] =&amp;gt; (item=python-neutronclient)
==&amp;gt; default: changed: [127.0.0.1] =&amp;gt; (item=python-novaclient)
==&amp;gt; default: changed: [127.0.0.1] =&amp;gt; (item=python-cinderclient)
==&amp;gt; default: changed: [127.0.0.1] =&amp;gt; (item=python-keystoneclient)
==&amp;gt; default: changed: [127.0.0.1] =&amp;gt; (item=python-swiftclient)
==&amp;gt; default: changed: [127.0.0.1] =&amp;gt; (item=python-keystoneclient)
==&amp;gt; default: changed: [127.0.0.1] =&amp;gt; (item=python-glanceclient)
==&amp;gt; default: changed: [127.0.0.1] =&amp;gt; (item=python-troveclient)
==&amp;gt; default: changed: [127.0.0.1] =&amp;gt; (item=python-designateclient)
==&amp;gt; default:
==&amp;gt; default: PLAY RECAP ********************************************************************
==&amp;gt; default: 127.0.0.1                  : ok=8    changed=7    unreachable=0    failed=0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;うまく動いたようです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Users\hoge&amp;gt; vagrant ssh
cygwin warning:
  MS-DOS style path detected: C:/Users/hoge/.vagrant.d/insecure_private_key
  Preferred POSIX equivalent is: /cygdrive/c/Users/hoge/.vagrant.d/insecure_private_key
  CYGWIN environment variable option &amp;quot;nodosfilewarning&amp;quot; turns off this warning.
  Consult the user&#39;s guide for more details about POSIX paths:
    http://cygwin.com/cygwin-ug-net/using.html#using-pathnames
Welcome to Ubuntu 14.04 LTS (GNU/Linux 3.13.0-24-generic x86_64)

 * Documentation:  https://help.ubuntu.com/
Last login: Sun Apr 20 02:21:46 2014 from 172.16.230.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;vagrant sshでサクッとログイン。ちなみに、これだけのためにcygwin入れてます。負けは認めます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vagrant@vagrant:~$ nova list
+----+------+--------+------------+-------------+----------+
| ID | Name | Status | Task State | Power State | Networks |
+----+------+--------+------------+-------------+----------+
+----+------+--------+------------+-------------+----------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;いきなりnovaコマンド使えます。&lt;/p&gt;

&lt;p&gt;なおproxy環境下では、/etc/apt/apt.conf、.bashrcやplaybookにproxy設定をするよう、provision.shとplaybook(site.yml)をいじれば動くと思います。まだやってませんが。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Vagrant-hpからVagrant-openstack-pluginへ</title>
      <link>https://ToruMakabe.github.io/post/vagrant-openstack/</link>
      <pubDate>Sat, 06 Sep 2014 00:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/vagrant-openstack/</guid>
      <description></description>
      
      <content>

&lt;h3 id=&#34;ツールやsdkはボチボチ集約したほうが&#34;&gt;ツールやSDKはボチボチ集約したほうが&lt;/h3&gt;

&lt;p&gt;これまでHP Public Cloudむけの&lt;a href=&#34;http://www.vagrantup.com/&#34;&gt;Vagrant&lt;/a&gt;は、&lt;a href=&#34;https://github.com/mohitsethi/vagrant-hp&#34;&gt;vagrant-hp plug-in&lt;/a&gt;を&lt;a href=&#34;http://torumakabe.github.io/tips/2014/05/05/vagrant-hpcloud/&#34;&gt;使って&lt;/a&gt;ました。でも最近、より汎用的で開発が活発な&lt;a href=&#34;https://github.com/cloudbau/vagrant-openstack-plugin&#34;&gt;vagrant-openstack-plugin&lt;/a&gt;へ鞍替えを画策しております。そろそろOpenStackのツールやSDKは、スタンダードになりそうなものを盛り上げた方がいいかな、と思っていたところだったので。&lt;/p&gt;

&lt;p&gt;多様性はオープンソースの魅力ですが、選択肢が多すぎるとユーザーは迷子になります。OpenStackのアプリデベロッパーは増えつつあるので、そろそろコミュニティでツールやSDKの集約を考える時期かなあ、と。&lt;/p&gt;

&lt;p&gt;さて、このPlug-in、あまり情報ないので、使用感をまとめておきます。&lt;/p&gt;

&lt;h3 id=&#34;前提条件&#34;&gt;前提条件&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Vagrant 1.6.3&lt;/li&gt;
&lt;li&gt;vagrant-openstack-plugin 0.8.0&lt;/li&gt;
&lt;li&gt;HP Public Cloud (2014/9/6)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;プラグインのインストールと前準備&#34;&gt;プラグインのインストールと前準備&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cloudbau/vagrant-openstack-plugin&#34;&gt;Github&lt;/a&gt;を見て、プラグインのインストールとboxファイルの作成を行ってください。boxファイルがない状態でvagrant upすると怒られます。&lt;/p&gt;

&lt;h3 id=&#34;ではvagrantfileを見てみましょう&#34;&gt;ではVagrantfileを見てみましょう&lt;/h3&gt;

&lt;script type=&#34;text/javascript&#34; src=&#34;http://gist.github.com/c9de20c61752864aca86.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;これがわたしが作ったVagrantfileです。見ての通りですが、以下に補足します。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;フレーバーとイメージ名は正規表現で指定できます。&lt;/li&gt;
&lt;li&gt;OpenStack CLI群と同じ環境変数を使ってます。&lt;/li&gt;
&lt;li&gt;Floating IPは&amp;rdquo;:auto&amp;rdquo;指定にてVMへ自動割当できますが、IPは事前に確保しておいてください。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;で、ふつーに動きます。乗り換え決定です。&lt;/p&gt;

&lt;h3 id=&#34;スナップショット便利&#34;&gt;スナップショット便利&lt;/h3&gt;

&lt;p&gt;vagrant-hpでは使えなかったはず。こいつは便利だ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vagrant openstack snapshot -n lab01_snap
==&amp;gt; default: This server instance is snapshoting!
==&amp;gt; default: Snapshot is ok
&lt;/code&gt;&lt;/pre&gt;
</content>
      
    </item>
    
    <item>
      <title>いま最も楽にIcehouse環境を作る方法</title>
      <link>https://ToruMakabe.github.io/post/vagrant-icehouse/</link>
      <pubDate>Tue, 06 May 2014 00:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/vagrant-icehouse/</guid>
      <description></description>
      
      <content>

&lt;h3 id=&#34;あえて言おう-これは甘えであると&#34;&gt;あえて言おう、これは甘えであると&lt;/h3&gt;

&lt;p&gt;現時点でもっとも楽にIcehouse環境を構築できる方法だと思う。所要時間、約30分。&lt;/p&gt;

&lt;p&gt;では始めましょう。&lt;a href=&#34;http://openstackr.wordpress.com/2014/05/01/openstack-cloud-computing-cookbook-the-icehouse-scripts/&#34;&gt;OpenStack Cloud Computing Cookbook&lt;/a&gt;の著者が提供しているツールを使います。使うのはVagrant、VirtualBox、Git。&lt;/p&gt;

&lt;h3 id=&#34;ほんと-これだけ&#34;&gt;ほんと、これだけ&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Vagrant、VirtualBox、Gitが入ってること、バージョンと大まかな手順を&lt;a href=&#34;http://openstackr.wordpress.com/2014/05/01/openstack-cloud-computing-cookbook-the-icehouse-scripts/&#34;&gt;このページ&lt;/a&gt;で確認&lt;/li&gt;
&lt;li&gt;$ vagrant plugin install vagrant-cachier&lt;/li&gt;
&lt;li&gt;$ git clone &lt;a href=&#34;https://github.com/OpenStackCookbook/OpenStackCookbook.git&#34;&gt;https://github.com/OpenStackCookbook/OpenStackCookbook.git&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;$ cd OpenStackCookbook&lt;/li&gt;
&lt;li&gt;$ git checkout icehouse&lt;/li&gt;
&lt;li&gt;$ vagrant up&lt;/li&gt;
&lt;li&gt;何度か管理者パスワードを入力&lt;/li&gt;
&lt;li&gt;$ vagrant ssh controller&lt;/li&gt;
&lt;li&gt;$ . /vagrant/openrc&lt;/li&gt;
&lt;li&gt;$/vagrant/demo.sh&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;以上。Horizonコンソールは &lt;a href=&#34;http://172.16.0.200/&#34;&gt;http://172.16.0.200/&lt;/a&gt; から。&lt;/p&gt;

&lt;h3 id=&#34;この環境だと30分でできた&#34;&gt;この環境だと30分でできた&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Vagrant 1.5.4&lt;/li&gt;
&lt;li&gt;VirtualBox 4.3.10&lt;/li&gt;
&lt;li&gt;Macbook Pro 2.3GHz クアッドコアIntel Core i7/メモリ16GB/SSD512GB&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;デモや新機能の試用くらいであればこれで十分ですね。&lt;br /&gt;
著者に感謝。わたしは買いました。 &amp;ndash; &lt;a href=&#34;http://www.amazon.co.jp/OpenStack-Computing-Cookbook-Second-Edition-ebook/dp/B00FZMREUM/&#34;&gt;OpenStack Cloud Computing Cookbook Second Edition(Amazon.co.jp)&lt;/a&gt;&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Vagrant HP Public Cloud Pluginを試す</title>
      <link>https://ToruMakabe.github.io/post/vagrant-hpcloud/</link>
      <pubDate>Mon, 05 May 2014 00:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/vagrant-hpcloud/</guid>
      <description></description>
      
      <content>

&lt;h3 id=&#34;みんな大好きhashicorp&#34;&gt;みんな大好きHashiCorp&lt;/h3&gt;

&lt;p&gt;クラウド界隈のデベロッパーから熱く注目されているHashiCorp。&lt;a href=&#34;http://www.packer.io/&#34;&gt;Packer&lt;/a&gt;、&lt;a href=&#34;http://www.serfdom.io/&#34;&gt;Serf&lt;/a&gt;、&lt;a href=&#34;http://www.consul.io/&#34;&gt;Consul&lt;/a&gt;と立て続けにイカしてる製品をリリースしております。まあ小生は、正直なところConsulあたりから置いてかれてますが。でも、やはり代表作は&lt;a href=&#34;http://www.vagrantup.com/&#34;&gt;Vagrant&lt;/a&gt;でしょう。vagrant up! vagrant destroy! いやー気軽でいいですね。&lt;/p&gt;

&lt;p&gt;VagrantはローカルのVirtualBoxやVMwareの他に、Providerとしてパブリッククラウドを選択できるのも魅力です。そこで当エントリではHP Public Cloud向けのVagrant Pluginを試してみます。&lt;/p&gt;

&lt;h3 id=&#34;前提条件&#34;&gt;前提条件&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Vagrant 1.5.4&lt;/li&gt;
&lt;li&gt;vagrant-hp 0.1.4&lt;/li&gt;
&lt;li&gt;HP Public Cloud (2014/5/5)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;プラグインのインストールと前準備&#34;&gt;プラグインのインストールと前準備&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/mohitsethi/vagrant-hp&#34;&gt;Github&lt;/a&gt;を見て、プラグインのインストールとboxファイルの作成を行ってください。boxファイルがない状態でvagrant upすると怒られます。&lt;/p&gt;

&lt;h3 id=&#34;ではvagrantfileを見てみましょう&#34;&gt;ではVagrantfileを見てみましょう&lt;/h3&gt;

&lt;script type=&#34;text/javascript&#34; src=&#34;http://gist.github.com/25a33c679492676bb626.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;これがわたしが作ったVagrantfileです。見ての通りですが、以下に補足します。2014/5/5時点、&lt;a href=&#34;https://github.com/mohitsethi/vagrant-hp&#34;&gt;Github&lt;/a&gt;の説明には若干のトラップがありますのでご注意を。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;イメージにUbuntu 14.04 LTSを使う例です。&lt;/li&gt;
&lt;li&gt;Availability Zoneパラメータには、Regionを指定してください。おっぷ。ここでちょいハマった。&lt;/li&gt;
&lt;li&gt;Security Groupは任意ですが、sshしたい場合はsshを通すSecurity Groupを指定してください。&lt;/li&gt;
&lt;li&gt;Floating IPは任意ですが、外部ネットワークからsshしたいときは必須です。&lt;/li&gt;
&lt;li&gt;ネットワーク指定は任意ですが、複数ネットワークを有している場合は、いずれか指定してください。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;それではさっそくvagrant-up&#34;&gt;それではさっそくvagrant up&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ vagrant up --provider=hp
Bringing machine &#39;default&#39; up with &#39;hp&#39; provider...
WARNING: Nokogiri was built against LibXML version 2.8.0, but has dynamically loaded 2.9.1
==&amp;gt; default: Warning! The HP provider doesn&#39;t support any of the Vagrant
==&amp;gt; default: high-level network configurations (`config.vm.network`). They
==&amp;gt; default: will be silently ignored.
==&amp;gt; default: Finding flavor for server...
==&amp;gt; default: Finding image for server...
==&amp;gt; default: Finding floating-ip...
==&amp;gt; default: Launching a server with the following settings...
==&amp;gt; default:  -- Flavor: standard.xsmall
==&amp;gt; default:  -- Image: Ubuntu Server 14.04 LTS (amd64 20140416.1) - Partner Image
==&amp;gt; default:  -- Name: hogehoge
==&amp;gt; default:  -- Key-name: your_keypair_name
==&amp;gt; default:  -- Security Groups: [&amp;quot;default&amp;quot;, &amp;quot;http&amp;quot;]
==&amp;gt; default: Finding network...
==&amp;gt; default: Waiting for the server to be built...
==&amp;gt; default: Waiting for SSH to become available...
==&amp;gt; default: Machine is booted and ready for use!
==&amp;gt; default: Rsyncing folder: /your_path/data/ =&amp;gt; /vagrant/data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;できたっぽい。&amp;ndash;provider=hpを忘れずに。&lt;/p&gt;

&lt;h3 id=&#34;間髪入れずにvagrant-ssh&#34;&gt;間髪入れずにvagrant ssh&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ vagrant ssh
WARNING: Nokogiri was built against LibXML version 2.8.0, but has dynamically loaded 2.9.1
Welcome to Ubuntu 14.04 LTS (GNU/Linux 3.13.0-24-generic x86_64)

* Documentation:  https://help.ubuntu.com/

System information disabled due to load higher than 1.0

Get cloud support with Ubuntu Advantage Cloud Guest:
    http://www.ubuntu.com/business/services/cloud

0 packages can be updated.
0 updates are security updates.


ubuntu@hogehoge:~$ ls /vagrant/data
test.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;フォルダ同期も効いてますね。んー、楽ちん。&lt;/p&gt;

&lt;p&gt;それではお楽しみ下さい。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>OpenStack超入門シリーズ 初期3部作完結</title>
      <link>https://ToruMakabe.github.io/post/openstack-3primer/</link>
      <pubDate>Thu, 01 May 2014 00:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/openstack-3primer/</guid>
      <description></description>
      
      <content>

&lt;h3 id=&#34;よく聞かれることをまとめました&#34;&gt;よく聞かれることをまとめました&lt;/h3&gt;

&lt;p&gt;ひとまず、初期3部作完結。また書くかもしれません。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/ToruMakabe/openstack-nova&#34;&gt;OpenStack超入門シリーズ いまさら聞けない Novaのディスク周り&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/ToruMakabe/openstack-32905609&#34;&gt;OpenStack超入門シリーズ いまさら聞けない Swiftの使い方&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/ToruMakabe/openstack-neutron&#34;&gt;OpenStack超入門シリーズ いまさら聞けない Neutronの使い方&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
      
    </item>
    
    <item>
      <title>うちのクラウド、空いてます</title>
      <link>https://ToruMakabe.github.io/post/cloud-vacancy/</link>
      <pubDate>Sun, 13 Apr 2014 00:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/cloud-vacancy/</guid>
      <description></description>
      
      <content>

&lt;h3 id=&#34;無限なんてあり得ない&#34;&gt;無限なんてあり得ない&lt;/h3&gt;

&lt;p&gt;「お客様はキャパシティのことを気にすることはありません。事実上無限、それがクラウドのメリットです！！」なんていうクラウドサービスのうたい文句、けっこう目にします。&lt;/p&gt;

&lt;p&gt;そのいっぽうで、「1000台のサーバーを1時間だけ使って、料金は数万円で済みました」という事例をアピールしているサービスの、別ユーザーが「この前、数10台のサーバー追加を依頼したら、在庫切れって言われてねぇ」と言っていたり。&lt;/p&gt;

&lt;p&gt;クラウドコンピューティングの概念は雲かもしれませんが、その向こうには物理リソースがあるわけで、無限というのは、残念ながら無理があります。&lt;/p&gt;

&lt;p&gt;空いているときもあれば、空いていない時もあります。また、事業者によって、キャパシティプランニングのスタンスは違います。規模も違います。&lt;/p&gt;

&lt;h3 id=&#34;空き状況を公開すると面白いかも知れない&#34;&gt;空き状況を公開すると面白いかも知れない&lt;/h3&gt;

&lt;p&gt;駐車場の空き状況がわかる街があります。便利です。最近はWebでレストランの空きがわかる&lt;a href=&#34;http://www.opentable.jp/default.aspx&#34;&gt;サービス&lt;/a&gt;もあります。これまた便利。&lt;/p&gt;

&lt;p&gt;また、震災以降、電力の需給状況が可視化されました。どのくらい余裕があるか、を意識できるようになっています。便利という話ではないですが、リソースの空きを意識して生活している、身近な例ではないでしょうか。&lt;/p&gt;

&lt;p&gt;そこで、クラウドサービスでも、リソースの空き状況を公開すると面白いのになぁ、と思うのです。でもわたしは、そのようなサービスを見たことがありません。&lt;/p&gt;

&lt;p&gt;ユーザーがそのサービスを判断する情報になりますし、電力事業者間で電力をやりとりしているように、事業者間でリソースを融通するような、新しい仕組みにつながる気もします。&lt;/p&gt;

&lt;p&gt;ビジネス上、難しいことは重々承知で書いています。でも、クラウドサービスが本当に「ユーティリティ」を目指すのなら、いつか問われる課題ではないかと。&lt;/p&gt;

&lt;p&gt;みなさんは、どう思われますか?&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>OpenStack超入門シリーズ</title>
      <link>https://ToruMakabe.github.io/post/openstack-primer/</link>
      <pubDate>Thu, 27 Mar 2014 00:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/openstack-primer/</guid>
      <description></description>
      
      <content>

&lt;h3 id=&#34;基本的な質問が増えてきた&#34;&gt;基本的な質問が増えてきた&lt;/h3&gt;

&lt;p&gt;OpenStackerというと、これまではOpenStack環境を「作る」人が多かったわけですが、最近「使う」人が増えてきた気がします。なぜかというとユーザー・デベロッパー目線での基本的な質問が増えてきたからです。これはいい傾向。&lt;/p&gt;

&lt;h3 id=&#34;よく聞かれることはまとめておこう&#34;&gt;よく聞かれることはまとめておこう&lt;/h3&gt;

&lt;p&gt;もちろん気軽に質問していただきたいのですが、ググってすぐ見つかったほうがいいので、今後、よく聞かれることは資料にまとめておこうかと思います。&lt;/p&gt;

&lt;p&gt;第一弾
&lt;a href=&#34;http://www.slideshare.net/ToruMakabe/openstack-nova&#34;&gt;OpenStack超入門シリーズ いまさら聞けない Novaのディスク周り&lt;/a&gt;&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>OpenStack Swiftへのファイル分割アップロード</title>
      <link>https://ToruMakabe.github.io/post/swift-upload/</link>
      <pubDate>Sun, 23 Mar 2014 00:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/swift-upload/</guid>
      <description></description>
      
      <content>

&lt;h3 id=&#34;swiftへ-ファイルを分割してアップロードできる&#34;&gt;Swiftへ、ファイルを分割してアップロードできる&lt;/h3&gt;

&lt;p&gt;今週偶然にも、何度か質問されたり、TwitterのTLにこの話題が流れてたり。もしかしたら世の関心が高い話題かもしれないので、まとめておきます。&lt;/p&gt;

&lt;h3 id=&#34;アップロード形式は大きく3つ-そのまま-dlo-slo&#34;&gt;アップロード形式は大きく3つ &amp;ndash; そのまま、DLO、SLO&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;そのまま、ファイルに手を加えずにアップロードします。この場合、ファイルサイズの上限は5GBです。5GBを超えるファイルをアップロードする場合、後述のDLO、SLOどちらかの形式でファイルを分割する必要があります。&lt;/li&gt;
&lt;li&gt;DLO(Dynamic Large Object)形式。ファイルを任意のサイズに分割し、Swift上で1つのファイルに見せかけます。「指定のコンテナ/疑似フォルダ下にあるファイルを結合する」というルールなのでアップロード手順がシンプルです。また、後からのセグメント追加/削除が容易です。&lt;/li&gt;
&lt;li&gt;SLO(Static Large Object)形式。ファイルを任意のサイズに分割し、Swift上で1つのファイルに見せかけます。分割セグメントファイルのハッシュ値をリストした、マニフェストファイルの作成が必要です。Swift上でハッシュのチェックが行われるため、データの完全性がDLOより高いです。また、セグメントを任意のコンテナに分散できるため、負荷分散の手段が増えます。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;動きを見てみよう&#34;&gt;動きを見てみよう&lt;/h3&gt;

&lt;p&gt;環境は以下の通り。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;HP Public Cloud US-West Region&lt;/li&gt;
&lt;li&gt;Swift Clientを動かすCompute Node &amp;ndash; standard.large / ubuntu 12.04&lt;/li&gt;
&lt;li&gt;Swift CLI &amp;ndash; 2.0.3&lt;/li&gt;
&lt;li&gt;約900MBあるubuntu desktopのisoファイルをアップロード&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;そのままアップロード&#34;&gt;そのままアップロード&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;$time swift -v upload mak-cont ./ubuntu-13.10-desktop-amd64.iso --object-name non-seg.iso
No handlers could be found for logger &amp;quot;keystoneclient.httpclient&amp;quot;
non-seg.iso

real    0m24.557s
user    0m12.617s
sys 0m11.197s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ハンドラーが無いとか怒られましたが、別事案なので気にせずにいきましょう。そのまま送ると、25秒くらい。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$curl -H &amp;quot;X-Auth-Token: hoge&amp;quot; -I https://region-a.geo-1.objects.hpcloudsvc.com/v1/fuga/mak-cont/non-seg.iso

HTTP/1.1 200 OK
Content-Length: 925892608
Content-Type: application/x-iso9660-image
Accept-Ranges: bytes
Last-Modified: Sun, 23 Mar 2014 01:16:53 GMT
Etag: 21ec41563ff34da27d4a0b56f2680c4f
X-Timestamp: 1395537413.17419
X-Object-Meta-Mtime: 1381950899.000000
X-Trans-Id: txfee207024dd04bd599292-00532e3c5e
Date: Sun, 23 Mar 2014 01:43:58 GMT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ヘッダはこんな感じ。&lt;/p&gt;

&lt;h4 id=&#34;dlo形式でアップロード&#34;&gt;DLO形式でアップロード&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;$time swift -v upload mak-cont ./ubuntu-13.10-desktop-amd64.iso --object-name dlo.iso --segment-size 104857600
No handlers could be found for logger &amp;quot;keystoneclient.httpclient&amp;quot;
dlo.iso segment 0
dlo.iso segment 5
dlo.iso segment 1
dlo.iso segment 2
dlo.iso segment 3
dlo.iso segment 4
dlo.iso segment 8
dlo.iso segment 7
dlo.iso segment 6
dlo.iso

real    0m11.568s
user    0m7.960s
sys 0m4.448s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Swift CLIが各セグメントを100MBに分割してアップロードしています。並列でアップロードしているので、
分割しない場合とくらべて転送時間は半分以下です。転送時間を重視するなら、ファイルサイズが5GB以下でも分割は有用です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$curl -H &amp;quot;X-Auth-Token: hoge&amp;quot; -I https://region-a.geo-1.objects.hpcloudsvc.com/v1/fuga/mak-cont/dlo.iso
HTTP/1.1 200 OK
Content-Length: 925892608
X-Object-Meta-Mtime: 1381950899.000000
Accept-Ranges: bytes
X-Object-Manifest: mak-cont_segments/dlo.iso/1381950899.000000/925892608/104857600/
Last-Modified: Sun, 23 Mar 2014 01:22:25 GMT
Etag: &amp;quot;7085388575f90df99531b60f9d9b1291&amp;quot;
X-Timestamp: 1395537755.32458
Content-Type: application/x-iso9660-image
X-Trans-Id: txd90ac8f8f6a64c749de2f-00532e3c6f
Date: Sun, 23 Mar 2014 01:44:15 GMT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;X-Object-Manifestという属性が、セグメント化したファイルの置き場所を指しています。&lt;/p&gt;

&lt;h4 id=&#34;slo形式でアップロード&#34;&gt;SLO形式でアップロード&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;$time swift -v upload mak-cont ./ubuntu-13.10-desktop-amd64.iso --object-name slo.iso --segment-size 104857600 --use-slo
No handlers could be found for logger &amp;quot;keystoneclient.httpclient&amp;quot;
slo.iso segment 3
slo.iso segment 7
slo.iso segment 1
slo.iso segment 4
slo.iso segment 8
slo.iso segment 0
slo.iso segment 5
slo.iso segment 2
slo.iso segment 6
slo.iso

real    0m12.039s
user    0m8.189s
sys 0m4.820s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;転送時間はDLOと同等です。Swift CLIを使う場合は &amp;ndash;use-sloオプションを指定するだけなので、データ完全性の観点からSLOがおすすめです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$curl -H &amp;quot;X-Auth-Token: hoge&amp;quot; -I https://region-a.geo-1.objects.hpcloudsvc.com/v1/fuga/mak-cont/slo.iso
HTTP/1.1 200 OK
Content-Length: 925892608
X-Object-Meta-Mtime: 1381950899.000000
Accept-Ranges: bytes
Last-Modified: Sun, 23 Mar 2014 01:24:08 GMT
Etag: &amp;quot;7085388575f90df99531b60f9d9b1291&amp;quot;
X-Timestamp: 1395537859.11815
X-Static-Large-Object: True
Content-Type: application/x-iso9660-image
X-Trans-Id: tx6cec436f525f4eb89dcfc-00532e3c7b
Date: Sun, 23 Mar 2014 01:44:27 GMT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;X-Static-Large-Object属性がTrueになりました。&lt;/p&gt;

&lt;p&gt;参考情報
- &lt;a href=&#34;http://docs.openstack.org/developer/swift/overview_large_objects.html&#34;&gt;Swift Documentaion &amp;ndash; Large Object Support&lt;/a&gt;
- &lt;a href=&#34;https://docs.hpcloud.com/api/object-storage/#large_objects-jumplink-span&#34;&gt;HP Cloud Object Storage API Reference&lt;/a&gt;&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>クラウド = 発電所?</title>
      <link>https://ToruMakabe.github.io/post/cloud-and-power/</link>
      <pubDate>Sun, 16 Mar 2014 00:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/cloud-and-power/</guid>
      <description></description>
      
      <content>

&lt;h3 id=&#34;思考停止してないだろうか&#34;&gt;思考停止してないだろうか&lt;/h3&gt;

&lt;p&gt;クラウドコンピューティングは、発電所に例えられることが多いです。そのうちみんな自家発電をやめて、発電所に任せるようになるよ、と。ふーん、そうかもなぁ。&lt;/p&gt;

&lt;p&gt;でも本当にそうなんでしょうか。雰囲気だけで、ちゃんと考えてない、思考停止している気がしています。&lt;/p&gt;

&lt;h3 id=&#34;本当に発電は大規模発電所任せ&#34;&gt;本当に発電は大規模発電所任せ?&lt;/h3&gt;

&lt;p&gt;最近CMでよく見ますが、&lt;a href=&#34;http://home.tokyo-gas.co.jp/enefarm_special/index.html&#34;&gt;エネファーム&lt;/a&gt;ってご存じですか。これ、ざっくり言うと戸別の発電所じゃないでしょうか。エネルギーは、使うところの近くで作った方が効率的なので、こういう仕組みが出てきたんでしょうね。&lt;/p&gt;

&lt;p&gt;戸別の発電は、将来的には燃料電池など新しい技術も使われて、さらに普及するんじゃないでしょうか。使う人の近くで、その使い方に合わせて発電した方が、合理的ですものね。&lt;/p&gt;

&lt;h3 id=&#34;オンプレit基盤は無くなる&#34;&gt;オンプレIT基盤は無くなる?&lt;/h3&gt;

&lt;p&gt;で、クラウドの話です。オンプレミスのIT基盤は無くなって、全部クラウドに移ってしまうのでしょうか。でも、もしその考えが「発電所のように」という根拠であれば、「従来の」という前提が要ります。なぜなら、発電の仕組みも進化しているので。エネファームのように。&lt;/p&gt;

&lt;p&gt;ネットワーク遅延を考えれば、処理する場所の近くにデータがあったほうがいいですよね。利用者みんなが、クラウドが動くデータセンターの近くにいるわけではないので。&lt;/p&gt;

&lt;p&gt;また、発電所は、ピークに合わせて設備投資をしています。いっぽうで、いざというとき、あなたの契約しているクラウドは、契約者全員が使えるだけの能力を供給できるでしょうか。ピークに合わせて設備投資しているでしょうか。そもそもピークをどう定義しているでしょうか。災害時にそれを期待した利用者の需要で、パンクしないでしょうか。&lt;/p&gt;

&lt;h3 id=&#34;そのインフラ知識は捨てないほうがいい&#34;&gt;そのインフラ知識は捨てないほうがいい&lt;/h3&gt;

&lt;p&gt;発電の世界で燃料電池が期待されているように、ITの世界でも不揮発性メモリなど、その使い方が大きく変わるイノベーションの種が研究開発されています。普段は電源を切っておいて、処理するときだけ瞬時に起動、処理、また停止するようなコンピュータ。家とかオフィスに置きたくないですか。性能的にも経済的にもリスク管理的にも、合理的かもしれませんよ。&lt;/p&gt;

&lt;p&gt;そして、その設備ではリソースが足りない、読めない、あまり使わないものはクラウドに置き、動かせばいいのではないかと思います。あと、クラウドのほうが、いいものをすぐに使える場合。SaaSとか。&lt;/p&gt;

&lt;p&gt;「未来はどっちだ」という白か黒かの議論をして、どちらかと心中する必要はありません。どっちも使えばいいです。かっこいい言い方をすると、ハイブリッドです。&lt;/p&gt;

&lt;p&gt;なので、「クラウド時代に自分で基盤作ることなんてなくなるから、インフラの知識はもう要らないよね」とか言って、せっかく身につけたハードウェア、ITインフラの知識は、捨てるどころか磨いた方がいいですよ。知識は荷物になりません、あなたを守る懐刀。&lt;/p&gt;
</content>
      
    </item>
    
    <item>
      <title>Transparency is Noisy</title>
      <link>https://ToruMakabe.github.io/post/transparency-is-noisy/</link>
      <pubDate>Sun, 02 Feb 2014 00:00:00 +0900</pubDate>
      <author>Toru Makabe</author>
      <guid>https://ToruMakabe.github.io/post/transparency-is-noisy/</guid>
      <description></description>
      
      <content>

&lt;h3 id=&#34;雑音が多いのは透明性の裏返し&#34;&gt;雑音が多いのは透明性の裏返し&lt;/h3&gt;

&lt;p&gt;秘密裏に開発し、ある日突然リリースする。いわゆる不言実行。&lt;/p&gt;

&lt;p&gt;いっぽうで、みなで仕様を議論し、期限を定めて開発する。その過程を公開。こっちは有言実行。&lt;/p&gt;

&lt;p&gt;有言実行は、叩かれやすいんです。雑音も多く生まれます。でもそれは、オープンである裏返し。&lt;/p&gt;

&lt;p&gt;答えを出すのは、ユーザーとデベロッパーです。安全地帯にいる、外野ではありません。&lt;/p&gt;

&lt;p&gt;元記事は、&lt;a href=&#34;http://www.openstack.org/blog/2014/01/openstack-2014-powered-by-users/&#34;&gt;ここ&lt;/a&gt;。&lt;/p&gt;
</content>
      
    </item>
    
  </channel>
</rss>