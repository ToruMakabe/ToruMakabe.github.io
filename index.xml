<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>re-imagine</title>
    <link>http://torumakabe.github.io/index.xml</link>
    <description>Recent content on re-imagine</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Sun, 11 Feb 2018 00:20:00 +0900</lastBuildDate>
    <atom:link href="http://torumakabe.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>AKSのIngress TLS証明書を自動更新する</title>
      <link>http://torumakabe.github.io/post/aks_tls_autorenewal/</link>
      <pubDate>Sun, 11 Feb 2018 00:20:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/aks_tls_autorenewal/</guid>
      <description>

&lt;h2 id=&#34;カジュアルな証明書管理方式が欲しい&#34;&gt;カジュアルな証明書管理方式が欲しい&lt;/h2&gt;

&lt;p&gt;ChromeがHTTPサイトに対する警告を&lt;a href=&#34;https://japan.cnet.com/article/35100589/&#34;&gt;強化するそうです&lt;/a&gt;。非HTTPSサイトには、生きづらい世の中になりました。&lt;/p&gt;

&lt;p&gt;さてそうなると、TLS証明書の入手と更新、めんどくさいですね。ガチなサイトでは証明書の維持管理を計画的に行うべきですが、検証とかちょっとした用途で立てるサイトでは、とにかくめんどくさい。カジュアルな方式が望まれます。&lt;/p&gt;

&lt;p&gt;そこで、Azure Container Service(AKS)で使える気軽な方法をご紹介します。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;TLSはIngress(NGINX Ingress Controller)でまとめて終端&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://letsencrypt.org/&#34;&gt;Let&amp;rsquo;s Encypt&lt;/a&gt;から証明書を入手&lt;/li&gt;
&lt;li&gt;Kubenetesのアドオンである&lt;a href=&#34;https://github.com/jetstack/cert-manager/&#34;&gt;cert-manager&lt;/a&gt;で証明書の入手、更新とIngressへの適用を自動化

&lt;ul&gt;
&lt;li&gt;ACME(Automatic Certificate Management Environment)対応&lt;/li&gt;
&lt;li&gt;cert-managerはまだ歴史の浅いプロジェクトだが、&lt;a href=&#34;https://github.com/jetstack/cert-manager/&#34;&gt;kube-lego&lt;/a&gt;の後継として期待&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;なおKubernetes/AKSは開発ペースやエコシステムの変化が速いので要注意。この記事は2018/2/10に書いています。&lt;/p&gt;

&lt;h2 id=&#34;使い方&#34;&gt;使い方&lt;/h2&gt;

&lt;p&gt;AKSクラスターと、Azure DNS上に利用可能なゾーンがあることを前提にします。ない場合、それぞれ公式ドキュメントを参考にしてください。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/aks/kubernetes-walkthrough&#34;&gt;Azure Container Service (AKS) クラスターのデプロイ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/dns/dns-getstarted-cli&#34;&gt;Azure CLI 2.0 で Azure DNS の使用を開始する&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;まずAKSにNGINX Ingress Controllerを導入します。helmで入れるのが楽でしょう。&lt;a href=&#34;http://torumakabe.github.io/post/aks_ingress_quickdeploy/&#34;&gt;この記事&lt;/a&gt;も参考に。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm install stable/nginx-ingress --name my-nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サービスの状況を確認します。NGINX Ingress ControllerにEXTERNAL-IPが割り当てられるまで、待ちます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get svc
NAME                                     TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                     AGE
kubernetes                               ClusterIP      10.0.0.1       &amp;lt;none&amp;gt;           443/TCP                     79d
my-nginx-nginx-ingress-controller        LoadBalancer   10.0.2.105     52.234.148.138   80:30613/TCP,443:30186/TCP   6m
my-nginx-nginx-ingress-default-backend   ClusterIP      10.0.102.246   &amp;lt;none&amp;gt;           80/TCP                     6m
nginx                                    NodePort       10.0.73.190    &amp;lt;none&amp;gt;           80:32625/TCP                 4m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;EXTERNAL-IPが割り当てられたら、Azure DNSで名前解決できるようにします。Azure CLIを使います。Ingressのホスト名をwww.example.comとする例です。このホスト名で、後ほどLet&amp;rsquo;s Encryptから証明書を取得します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az network dns record-set a add-record -z example.com -g your-dnszone-rg -n www -a 52.234.148.138
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cert-managerのソースをGitHubから取得し、contribからhelm installします。いずれstableを使えるようになるでしょう。なお、このAKSクラスターはまだRBACを使っていないので、&amp;rdquo;&amp;ndash;set rbac.create=false&amp;rdquo;オプションを指定しています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/jetstack/cert-manager
$ cd cert-manager/
$ helm install --name cert-manager --namespace kube-system contrib/charts/cert-manager --set rbac.create=false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;では任意の作業ディレクトリに移動し、以下の内容でマニフェストを作ります。cm-issuer-le-staging-sample.yamlとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: certmanager.k8s.io/v1alpha1
kind: Issuer
metadata:
  name: letsencrypt-staging
  namespace: default
spec:
  acme:
    # The ACME server URL
    server: https://acme-staging.api.letsencrypt.org/directory
    # Email address used for ACME registration
    email: hoge@example.com
    # Name of a secret used to store the ACME account private key
    privateKeySecretRef:
      name: letsencrypt-staging
    # Enable the HTTP-01 challenge provider
    http01: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;証明書を発行してもらうLet&amp;rsquo;s EncryptをIssuerとして登録するわけですが、まずはステージングのAPIエンドポイントを指定しています。Let&amp;rsquo;s Encryptには&lt;a href=&#34;https://letsencrypt.org/docs/rate-limits/&#34;&gt;Rate Limit&lt;/a&gt;があり、失敗した時に痛いからです。Let&amp;rsquo;s EncryptのステージングAPIを使うとフェイクな証明書(Fake LE Intermediate X1)が発行されますが、流れの確認やマニフェストの検証は、できます。&lt;/p&gt;

&lt;p&gt;なお、Let&amp;rsquo;s Encryptとのチャレンジには今回、HTTPを使います。DNSチャレンジも&lt;a href=&#34;https://github.com/jetstack/cert-manager/pull/246&#34;&gt;いずれ対応する見込み&lt;/a&gt;です。&lt;/p&gt;

&lt;p&gt;では、Issuerを登録します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f cm-issuer-le-staging-sample.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次は証明書の設定です。マニフェストはcm-cert-le-staging-sample.yamlとします。acme節にACME構成を書きます。チャレンジはHTTP、ingressClassはnginxです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: example-com
  namespace: default
spec:
  secretName: example-com-tls
  issuerRef:
    name: letsencrypt-staging
  commonName: www.example.com
  dnsNames:
  - www.example.com
  acme:
    config:
    - http01:
        ingressClass: nginx
      domains:
      - www.example.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;証明書設定をデプロイします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f cm-cert-le-staging-sample.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;証明書の発行状況を確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe certificate example-com
Name:         example-com
Namespace:    default
[snip]
Events:
  Type     Reason                 Age              From                     Message
  ----     ------                 ----             ----                     -------
  Warning  ErrorCheckCertificate  8m               cert-manager-controller  Error checking existing TLS certificate: secret &amp;quot;example-com-tls&amp;quot; not found
  Normal   PrepareCertificate     8m               cert-manager-controller  Preparing certificate with issuer
  Normal   PresentChallenge       8m               cert-manager-controller  Presenting http-01 challenge for domain www.example.com
  Normal   SelfCheck              8m               cert-manager-controller  Performing self-check for domain www.example.com
  Normal   ObtainAuthorization    7m               cert-manager-controller  Obtained authorization for domain www.example.com
  Normal   IssueCertificate       7m               cert-manager-controller  Issuing certificate...
  Normal   CeritifcateIssued      7m               cert-manager-controller  Certificated issuedsuccessfully
  Normal   RenewalScheduled       7m (x2 over 7m)  cert-manager-controller  Certificate scheduled for renewal in 1438 hours
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;無事に証明書が発行され、更新もスケジュールされました。手順やマニフェストの書きっぷりは問題なさそうです。これをもってステージング完了としましょう。&lt;/p&gt;

&lt;p&gt;ではLet&amp;rsquo;s EncryptのAPIエンドポイントをProduction向けに変更し、新たにIssuer登録します。cm-issuer-le-prod-sample.yamlとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: certmanager.k8s.io/v1alpha1
kind: Issuer
metadata:
  name: letsencrypt-prod
  namespace: default
spec:
  acme:
    # The ACME server URL
    server: https://acme-v01.api.letsencrypt.org/directory
    # Email address used for ACME registration
    email: hoge@example.com
    # Name of a secret used to store the ACME account private key
    privateKeySecretRef:
      name: letsencrypt-prod
    # Enable the HTTP-01 challenge provider
    http01: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デプロイします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f cm-issuer-le-prod-sample.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同様に、Production向けの証明書設定をします。cm-cert-le-prod-sample.yamlとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: prod-example-com
  namespace: default
spec:
  secretName: prod-example-com-tls
  issuerRef:
    name: letsencrypt-prod
  commonName: www.example.com
  dnsNames:
  - www.example.com
  acme:
    config:
    - http01:
        ingressClass: nginx
      domains:
      - www.example.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デプロイします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f cm-cert-le-prod-sample.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;発行状況を確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe certificate prod-example-com
Name:         prod-example-com
Namespace:    default
[snip]
Events:
  Type     Reason                 Age              From                     Message
  ----     ------                 ----             ----                     -------
  Warning  ErrorCheckCertificate  27s              cert-manager-controller  Error checking existing TLS certificate: secret &amp;quot;prod-example-com-tls&amp;quot; not found
  Normal   PrepareCertificate     27s              cert-manager-controller  Preparing certificate with issuer
  Normal   PresentChallenge       26s              cert-manager-controller  Presenting http-01 challenge for domain www.example.com
  Normal   SelfCheck              26s              cert-manager-controller  Performing self-check for domain www.example.com
  Normal   IssueCertificate       7s               cert-manager-controller  Issuing certificate...
  Normal   ObtainAuthorization    7s               cert-manager-controller  Obtained authorization for domain www.example.com
  Normal   RenewalScheduled       6s (x3 over 5m)  cert-manager-controller  Certificate scheduled for renewal in 1438 hours
  Normal   CeritifcateIssued      6s               cert-manager-controller  Certificated issuedsuccessfully
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;証明書が発行され、1438時間(約60日)内の更新がスケジュールされました。&lt;/p&gt;

&lt;p&gt;ではバックエンドを設定して確認してみましょう。バックエンドにNGINXを立て、exposeします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run nginx --image nginx --port 80
$ kubectl expose deployment nginx --type NodePort
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ingressを設定します。ファイル名はingress-nginx-sample.yamlとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
  name: ingress-nginx-sample
spec:
  rules:
    - host: www.example.com
      http:
        paths:
          - path: /
            backend:
              serviceName: nginx
              servicePort: 80
  tls:
    - hosts:
      - www.example.com
      secretName: prod-example-com-tls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デプロイします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f ingress-nginx-sample.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;いざ確認。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl https://www.example.com/
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
[snip]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;便利ですね。Let&amp;rsquo;s Encryptをはじめ、関連プロジェクトに感謝です。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AKSのNGINX Ingress Controllerのデプロイで悩んだら</title>
      <link>http://torumakabe.github.io/post/aks_ingress_quickdeploy/</link>
      <pubDate>Sat, 10 Feb 2018 11:00:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/aks_ingress_quickdeploy/</guid>
      <description>

&lt;h2 id=&#34;楽したいならhelmで入れましょう&#34;&gt;楽したいならhelmで入れましょう&lt;/h2&gt;

&lt;p&gt;AKSに限った話ではありませんが、Kubernetesにぶら下げるアプリの数が多くなってくると、URLマッピングやTLS終端がしたくなります。方法は色々あるのですが、シンプルな選択肢はNGINX Ingress Controllerでしょう。&lt;/p&gt;

&lt;p&gt;さて、そのNGINX Ingress Controllerのデプロイは&lt;a href=&#34;https://github.com/kubernetes/ingress-nginx/blob/master/deploy/README.md&#34;&gt;GitHubのドキュメント&lt;/a&gt;通りに淡々とやればいいのですが、&lt;a href=&#34;https://github.com/kubernetes/helm&#34;&gt;helm&lt;/a&gt;を使えばコマンド一発です。そのようにドキュメントにも書いてあるのですが、最後の方で出てくるので「それ早く言ってよ」な感じです。&lt;/p&gt;

&lt;p&gt;せっかくなので、Azure(AKS)での使い方をまとめておきます。開発ペースやエコシステムの変化が速いので要注意。この記事は2018/2/10に書いています。&lt;/p&gt;

&lt;h2 id=&#34;使い方&#34;&gt;使い方&lt;/h2&gt;

&lt;p&gt;AKSクラスターと、Azure DNS上に利用可能なゾーンがあることを前提にします。ない場合、それぞれ公式ドキュメントを参考にしてください。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/aks/kubernetes-walkthrough&#34;&gt;Azure Container Service (AKS) クラスターのデプロイ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/dns/dns-getstarted-cli&#34;&gt;Azure CLI 2.0 で Azure DNS の使用を開始する&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ではhelmでNGINX Ingress Controllerを導入します。helmを使っていなければ、&lt;a href=&#34;https://github.com/kubernetes/helm#install&#34;&gt;入れておいてください&lt;/a&gt;。デプロイはこれだけ。Chartは&lt;a href=&#34;https://github.com/kubernetes/charts/tree/master/stable/nginx-ingress&#34;&gt;ここ&lt;/a&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm install stable/nginx-ingress --name my-nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;バックエンドへのつなぎが機能するか、Webアプリを作ってテストします。NGINXとApacheを選びました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run nginx --image nginx --port 80
$ kubectl run apache --image httpd --port 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サービスとしてexposeします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl expose deployment nginx --type NodePort
$ kubectl expose deployment apache --type NodePort
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;現時点のサービスたちを確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get svc
NAME                                     TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)                  AGE
apache                                   NodePort       10.0.244.167   &amp;lt;none&amp;gt;          80:30928/TCP                 14h
kubernetes                               ClusterIP      10.0.0.1       &amp;lt;none&amp;gt;          443/TCP                  79d
my-nginx-nginx-ingress-controller        LoadBalancer   10.0.91.78     13.72.108.187   80:32448/TCP,443:31991/TCP   14h
my-nginx-nginx-ingress-default-backend   ClusterIP      10.0.74.104    &amp;lt;none&amp;gt;          80/TCP                  14h
nginx                                    NodePort       10.0.191.16    &amp;lt;none&amp;gt;          80:30752/TCP                 14h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;AKSの場合はパブリックIPがNGINX Ingress Controllerに割り当てられます。EXTERNAL-IPがpendingの場合は割り当て中なので、しばし待ちます。&lt;/p&gt;

&lt;p&gt;割り当てられたら、EXTERNAL-IPをAzure DNSで名前解決できるようにしましょう。Azure CLIを使います。dev.example.comの例です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az network dns record-set a add-record -z example.com -g your-dnszone-rg -n dev -a 13.72.108.187
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;TLSが終端できるかも検証したいので、Secretを作ります。証明書とキーはLet&amp;rsquo;s Encryptで作っておきました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create secret tls example-tls --key privkey.pem --cert fullchain.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ではIngressを構成しましょう。以下をファイル名ingress-nginx-sample.yamlとして保存します。IngressでTLSを終端し、/へのアクセスは先ほどexposeしたNGINXのサービスへ、/apacheへのアクセスはApacheへ流します。rewrite-targetをannotaionsで指定するのを、忘れずに。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
  name: ingress-nginx-sample
spec:
  rules:
    - host: dev.example.com
      http:
        paths:
          - path: /
            backend:
              serviceName: nginx
              servicePort: 80
          - path: /apache
            backend:
              serviceName: apache
              servicePort: 80
  tls:
    - hosts:
      - dev.example.com
      secretName: example-tls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;あとは反映するだけ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f ingress-nginx-sample.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;curlで確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl https://dev.example.com
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
[snip]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/apacheへのパスも確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl https://dev.example.com/apache
&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;It works!&amp;lt;/h1&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;簡単ですね。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Azureのリソースグループ限定 共同作成者をいい感じに作る</title>
      <link>http://torumakabe.github.io/post/azure_rg_contributor/</link>
      <pubDate>Mon, 22 Jan 2018 22:00:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/azure_rg_contributor/</guid>
      <description>

&lt;h2 id=&#34;共同作成者は-ちょっと強い&#34;&gt;共同作成者は、ちょっと強い&lt;/h2&gt;

&lt;p&gt;Azureのリソースグループは、リソースを任意のグループにまとめ、ライフサイクルや権限の管理を一括して行える便利なコンセプトです。&lt;/p&gt;

&lt;p&gt;ユースケースのひとつに、&amp;rdquo;本番とは分離した開発向けリソースグループを作って、アプリ/インフラ開発者に開放したい&amp;rdquo;、があります。新しい技術は試行錯誤で身につくので、こういった環境は重要です。&lt;/p&gt;

&lt;p&gt;なのですが、このようなケースで、権限付与の落とし穴があります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;サブスクリプション所有者が開発用リソースグループを作る&lt;/li&gt;
&lt;li&gt;スコープを開発用リソースグループに限定し、開発者に対し共同作成者ロールを割り当てる&lt;/li&gt;
&lt;li&gt;開発者はリソースグループ限定で、のびのび試行錯誤できて幸せ&lt;/li&gt;
&lt;li&gt;開発者がスッキリしたくなり、リソースグループごとバッサリ削除 (共同作成者なので可能)&lt;/li&gt;
&lt;li&gt;開発者にはサブスクリプションレベルの権限がないため、リソースグループを作成できない&lt;/li&gt;
&lt;li&gt;詰む&lt;/li&gt;
&lt;li&gt;サブスクリプション所有者が、リソースグループ作成と権限付与をやり直し&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;共同作成者ロールから、リソースグループの削除権限だけを除外できると、いいんですが。そこでカスタムロールの出番です。リソースグループ限定、グループ削除権限なしの共同作成者を作ってみましょう。&lt;/p&gt;

&lt;h2 id=&#34;いい感じのカスタムロールを作る&#34;&gt;いい感じのカスタムロールを作る&lt;/h2&gt;

&lt;p&gt;Azureのカスタムロールは、個別リソースレベルで粒度の細かい権限設定ができます。ですが、やり過ぎると破綻するため、シンプルなロールを最小限作る、がおすすめです。&lt;/p&gt;

&lt;p&gt;シンプルに行きましょう。まずはカスタムロールの定義を作ります。role.jsonとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &amp;quot;Name&amp;quot;: &amp;quot;Resource Group Contributor&amp;quot;,
    &amp;quot;IsCustom&amp;quot;: true,
    &amp;quot;Description&amp;quot;: &amp;quot;Lets you manage everything except access to resources, but can not delete Resouce Group&amp;quot;,
    &amp;quot;Actions&amp;quot;: [
        &amp;quot;*&amp;quot;
    ],
    &amp;quot;NotActions&amp;quot;: [
        &amp;quot;Microsoft.Authorization/*/Delete&amp;quot;,
        &amp;quot;Microsoft.Authorization/*/Write&amp;quot;,
        &amp;quot;Microsoft.Authorization/elevateAccess/Action&amp;quot;,
        &amp;quot;Microsoft.Resources/subscriptions/resourceGroups/Delete&amp;quot;
    ],
    &amp;quot;AssignableScopes&amp;quot;: [
        &amp;quot;/subscriptions/your-subscriotion-id&amp;quot;
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;組み込みロールの共同作成者をテンプレに、NotActionsでリソースグループの削除権限を除外しました。AssignableScopesでリソースグループを限定してもいいですが、リソースグループの数だけロールを作るのはつらいので、ここでは指定しません。後からロールを割り当てる時にスコープを指定します。&lt;/p&gt;

&lt;p&gt;では、カスタムロールを作成します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az role definition create --role-definition ./role.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;出力にカスタムロールのIDが入っていますので、控えておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;id&amp;quot;: &amp;quot;/subscriptions/your-subscriotion-id/providers/Microsoft.Authorization/roleDefinitions/your-customrole-id&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;カスタムロールをユーザー-グループ-サービスプリンシパルに割り当てる&#34;&gt;カスタムロールをユーザー、グループ、サービスプリンシパルに割り当てる&lt;/h2&gt;

&lt;p&gt;次に、ユーザー/グループに先ほど作ったカスタムロールを割り当てます。スコープはリソースグループに限定します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az role assignment create --assignee-object-id your-user-or-group-object-id --role your-customrole-id --scope &amp;quot;/subscriptions/your-subscriotion-id/resourceGroups/sample-dev-rg&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サービスプリンシパル作成時に割り当てる場合は、以下のように。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az ad sp create-for-rbac -n &amp;quot;rgcontributor&amp;quot; -p &amp;quot;your-password&amp;quot; --role your-customrole-id --scopes &amp;quot;/subscriptions/your-subscriotion-id/resourceGroups/sample-dev-rg&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;余談ですが、&amp;rdquo;az ad sp create-for-rbac&amp;rdquo;コマンドはAzure ADアプリケーションを同時に作るため、別途アプリを作ってサービスプリンシパルと紐づける、という作業が要りません。&lt;/p&gt;

&lt;h2 id=&#34;試してみる&#34;&gt;試してみる&lt;/h2&gt;

&lt;p&gt;ログインして試してみましょう。サービスプリンシパルの例です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az login --service-principal -u &amp;quot;http://rgcontributor&amp;quot; -p &amp;quot;your-password&amp;quot; -t &amp;quot;your-tenant-id&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;検証したサブスクリプションには多数のリソースグループがあるのですが、スコープで指定したものだけが見えます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az group list -o table
Name              Location    Status
----------------  ----------  ---------
sample-dev-rg  japaneast   Succeeded
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このリソースグループに、VMを作っておきました。リストはしませんが、ストレージやネットワークなど関連リソースもこのグループにあります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az vm list -o table
Name              ResourceGroup     Location
----------------  ----------------  ----------
sampledevvm01     sample-dev-rg  japaneast
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;試しにリソースグループを作ってみます。サブスクリプションスコープの権限がないため怒られます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az group create -n rgc-poc-rg -l japaneast
The client &#39;aaaaa-bbbbb-ccccc-ddddd-eeeee&#39; with object id &#39;aaaaa-bbbbb-ccccc-ddddd-eeeee&#39; does not have authorization to perform action &#39;Microsoft.Resources/subscriptions/resourcegroups/write&#39; over scope &#39;/subscriptions/your-subscriotion-id/resourcegroups/rgc-poc-rg&#39;.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;リソースグループを消してみます。消すかい？ -&amp;gt; y -&amp;gt; ダメ、という、持ち上げて落とす怒り方です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az group delete -n sample-dev-rg
Are you sure you want to perform this operation? (y/n): y
The client &#39;aaaaa-bbbbb-ccccc-ddddd-eeeee&#39; with object id &#39;aaaaa-bbbbb-ccccc-ddddd-eeeee&#39; does not have authorization to perform action &#39;Microsoft.Resources/subscriptions/resourcegroups/delete&#39; over scope &#39;/subscriptions/your-subscriotion-id/resourcegroups/sample-dev-rg&#39;.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;でもリソースグループのリソースを一括削除したい&#34;&gt;でもリソースグループのリソースを一括削除したい&lt;/h2&gt;

&lt;p&gt;でも、リソースグループは消せなくても、リソースをバッサリ消す手段は欲しいですよね。そんな時には空のリソースマネージャーテンプレートを、completeモードでデプロイすると、消せます。&lt;/p&gt;

&lt;p&gt;空テンプレートを、empty.jsonとしましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &amp;quot;$schema&amp;quot;: &amp;quot;http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#&amp;quot;,
    &amp;quot;contentVersion&amp;quot;: &amp;quot;1.0.0.0&amp;quot;,
    &amp;quot;parameters&amp;quot;: {},
    &amp;quot;variables&amp;quot;: {},
    &amp;quot;resources&amp;quot;: [],
    &amp;quot;outputs&amp;quot;: {}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;破壊的空砲を打ちます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az group deployment create --mode complete -g sample-dev-rg --template-file ./empty.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;リソースグループは残ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az group list -o table
Name              Location    Status
----------------  ----------  ---------
sample-dev-rg  japaneast   Succeeded
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;VMは消えました。リストしませんが、他の関連リソースもバッサリ消えています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az vm list -o table

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>TerraformでAzure サンプル 2018/1版</title>
      <link>http://torumakabe.github.io/post/terraform_azure_sample_201801/</link>
      <pubDate>Mon, 08 Jan 2018 16:30:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/terraform_azure_sample_201801/</guid>
      <description>

&lt;h2 id=&#34;サンプルのアップデート&#34;&gt;サンプルのアップデート&lt;/h2&gt;

&lt;p&gt;年末にリポジトリの大掃除をしていて、2年前に書いたTerraform &amp;amp; Azureの&lt;a href=&#34;http://torumakabe.github.io/post/azure_tf_fundamental_rules/&#34;&gt;記事&lt;/a&gt;に目が止まりました。原則はいいとして、&lt;a href=&#34;https://github.com/ToruMakabe/Terraform_Azure_Sample&#34;&gt;サンプル&lt;/a&gt;は2年物で腐りかけです。ということでアップデートします。&lt;/p&gt;

&lt;h2 id=&#34;インパクトの大きな変更点&#34;&gt;インパクトの大きな変更点&lt;/h2&gt;

&lt;p&gt;Terraformの、ここ2年の重要なアップデートは以下でしょうか。Azure視点で。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;BackendにAzure Blobを使えるようになった&lt;/li&gt;
&lt;li&gt;Workspaceで同一コード・複数環境管理ができるようになった&lt;/li&gt;
&lt;li&gt;対応リソースが増えた&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://registry.terraform.io/&#34;&gt;Terraform Module Registry&lt;/a&gt;が公開された&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;更新版サンプルの方針&#34;&gt;更新版サンプルの方針&lt;/h2&gt;

&lt;p&gt;重要アップデートをふまえ、以下の方針で新サンプルを作りました。&lt;/p&gt;

&lt;h3 id=&#34;チーム-複数端末での運用&#34;&gt;チーム、複数端末での運用&lt;/h3&gt;

&lt;p&gt;BackendにAzure Blobがサポートされたので、チーム、複数端末でstateの共有がしやすくなりました。ひとつのプロジェクトや環境を、チームメンバーがどこからでも、だけでなく、複数プロジェクトでのstate共有もできます。&lt;/p&gt;

&lt;h3 id=&#34;workspaceの導入&#34;&gt;Workspaceの導入&lt;/h3&gt;

&lt;p&gt;従来は /dev /stage /prodなど、環境別にコードを分けて管理していました。ゆえに環境間のコード同期が課題でしたが、TerraformのWorkspace機能で解決しやすくなりました。リソース定義で ${terraform.workspace} 変数を参照するように書けば、ひとつのコードで複数環境を扱えます。&lt;/p&gt;

&lt;p&gt;要件によっては、従来通り環境別にコードを分けた方がいいこともあるでしょう。環境間の差分が大きい、開発とデプロイのタイミングやライフサイクルが異なるなど、Workspaceが使いづらいケースもあるでしょう。その場合は無理せず従来のやり方で。今回のサンプルは「Workspaceを使ったら何ができるか？」を考えるネタにしてください。&lt;/p&gt;

&lt;h3 id=&#34;module-terraform-module-registryの活用&#34;&gt;Module、Terraform Module Registryの活用&lt;/h3&gt;

&lt;p&gt;TerraformのModuleはとても強力な機能なのですが、あーでもないこーでもないと、こだわり過ぎるとキリがありません。「うまいやり方」を見てから使いたいのが人情です。そこでTerraform Module Registryを活かします。お墨付きのVerifiedモジュールが公開されていますので、そのまま使うもよし、ライセンスを確認の上フォークするのもよし、です。&lt;/p&gt;

&lt;h3 id=&#34;リソースグループは環境ごとに準備し-管理をterraformから分離&#34;&gt;リソースグループは環境ごとに準備し、管理をTerraformから分離&lt;/h3&gt;

&lt;p&gt;AzureのリソースをプロビジョニングするTerraformコードの多くは、Azureのリソースグループを管理下に入れている印象です。すなわちdestroyするとリソースグループごとバッサリ消える。わかりやすいけど破壊的。&lt;/p&gt;

&lt;p&gt;TerraformはApp ServiceやACIなどPaaS、アプリ寄りのリソースも作成できるようになってきたので、アプリ開発者にTerraformを開放したいケースが増えてきています。dev環境をアプリ開発者とインフラ技術者がコラボして育て、そのコードをstageやprodにデプロイする、など。&lt;/p&gt;

&lt;p&gt;ところで。TerraformのWorkspaceは、こんな感じで簡単に切り替えられます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform workspace select prod
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;みなまで言わなくても分かりますね。悲劇はプラットフォーム側で回避しましょう。今回のサンプルではリソースグループをTerraform管理下に置かず、別途作成します。Terraformからはdata resourcesとしてRead Onlyで参照する実装です。環境別のリソースグループを作成し、dev環境のみアプリ開発者へ権限を付与します。&lt;/p&gt;

&lt;h2 id=&#34;サンプル解説&#34;&gt;サンプル解説&lt;/h2&gt;

&lt;p&gt;サンプルは&lt;a href=&#34;https://github.com/ToruMakabe/Terraform_Azure_Sample_201801&#34;&gt;GitHub&lt;/a&gt;に置きました。合わせてご確認ください。&lt;/p&gt;

&lt;p&gt;このコードをapplyすると、以下のリソースが出来上がります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;NGINX on Ubuntu Webサーバー VMスケールセット&lt;/li&gt;
&lt;li&gt;VMスケールセット向けロードバランサー&lt;/li&gt;
&lt;li&gt;踏み台サーバー&lt;/li&gt;
&lt;li&gt;上記を配置するネットワーク (仮想ネットワーク、サブネット、NSG)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;リポジトリ構造&#34;&gt;リポジトリ構造&lt;/h3&gt;

&lt;p&gt;サンプルのリポジトリ構造です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;├── modules
│   ├── computegroup
│   │   ├── main.tf
│   │   ├── os
│   │   │   ├── outputs.tf
│   │   │   └── variables.tf
│   │   ├── outputs.tf
│   │   └── variables.tf
│   ├── loadbalancer
│   │   ├── main.tf
│   │   ├── outputs.tf
│   │   └── variables.tf
│   └── network
│       ├── main.tf
│       ├── outputs.tf
│       └── variables.tf
└── projects
    ├── project_a
    │   ├── backend.tf
    │   ├── main.tf
    │   ├── outputs.tf
    │   └── variables.tf
    └── shared
        ├── backend.tf
        ├── main.tf
        ├── outputs.tf
        └── variables.tf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/modulesには&lt;a href=&#34;https://registry.terraform.io/browse?provider=azurerm&#34;&gt;Terraform Module Registry&lt;/a&gt;でVerifiedされているモジュールをフォークしたコードを入れました。フォークした理由は、リソースグループをdata resource化して参照のみにしたかったためです。&lt;/p&gt;

&lt;p&gt;そして、/projectsに2つのプロジェクトを作りました。プロジェクトでリソースとTerraformの実行単位、stateを分割します。sharedで土台となる仮想ネットワークと踏み台サーバー関連リソース、project_aでVMスケールセットとロードバランサーを管理します。&lt;/p&gt;

&lt;p&gt;このボリュームだとプロジェクトを分割する必然性は低いのですが、以下のケースにも対応できるように分けました。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;アプリ開発者がproject_a下でアプリ関連リソースに集中したい&lt;/li&gt;
&lt;li&gt;性能観点で分割したい (Terraformはリソース量につれて重くなりがち)&lt;/li&gt;
&lt;li&gt;有事を考慮し影響範囲を分割したい&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;プロジェクト間では、stateをremote_stateを使って共有します。サンプルではsharedで作成した仮想ネットワークのサブネットIDを&lt;a href=&#34;https://github.com/ToruMakabe/Terraform_Azure_Sample_201801/blob/master/projects/shared/outputs.tf#L1&#34;&gt;output&lt;/a&gt;し、project_aで参照できるよう&lt;a href=&#34;https://github.com/ToruMakabe/Terraform_Azure_Sample_201801/blob/master/projects/project_a/backend.tf.sample#L10&#34;&gt;定義&lt;/a&gt;しています。&lt;/p&gt;

&lt;h2 id=&#34;使い方&#34;&gt;使い方&lt;/h2&gt;

&lt;h3 id=&#34;前提&#34;&gt;前提&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Linux、WSL、macOSなどbash環境の実行例です&lt;/li&gt;
&lt;li&gt;SSHの公開鍵をTerraform実行環境の ~/.ssh/id_rsa.pub として準備してください&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;管理者向けのサービスプリンシパルを用意する&#34;&gt;管理者向けのサービスプリンシパルを用意する&lt;/h3&gt;

&lt;p&gt;インフラのプロビジョニングの主体者、管理者向けのサービスプリンシパルを用意します。リソースグループを作成できる権限が必要です。&lt;/p&gt;

&lt;p&gt;もしなければ作成します。組み込みロールでは、サブスクリプションに対するContributorが妥当でしょう。&lt;a href=&#34;https://www.terraform.io/docs/providers/azurerm/authenticating_via_service_principal.html&#34;&gt;Terraformのドキュメント&lt;/a&gt;も参考に。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az ad sp create-for-rbac --role=&amp;quot;Contributor&amp;quot; --scopes=&amp;quot;/subscriptions/SUBSCRIPTION_ID&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;出力されるappId、password、tenantを控えます。既存のサービスプリンシパルを使うのであれば、同情報を確認してください。&lt;/p&gt;

&lt;p&gt;なお参考までに。Azure Cloud ShellなどAzure CLIが導入されている環境では、特に認証情報の指定なしでterraform planやapply時にAzureのリソースにアクセスできます。TerraformがCLIの認証トークンを&lt;a href=&#34;https://github.com/terraform-providers/terraform-provider-azurerm/blob/master/azurerm/helpers/authentication/config.go&#34;&gt;使う&lt;/a&gt;からです。&lt;/p&gt;

&lt;p&gt;そしてBackendをAzure Blobとする場合、Blobにアクセスするためのキーが別途必要です。ですが、残念ながらBackendロジックでキーを得る際に、このトークンが&lt;a href=&#34;https://github.com/hashicorp/terraform/blob/master/backend/remote-state/azure/backend.go&#34;&gt;使われません&lt;/a&gt;。キーを明示することもできますが、Blobのアクセスキーは漏洩時のリカバリーが大変です。できれば直に扱いたくありません。&lt;/p&gt;

&lt;p&gt;サービスプリンシパル認証であれば、Azureリソースへのプロビジョニング、Backendアクセスどちらも&lt;a href=&#34;https://www.terraform.io/docs/backends/types/azurerm.html&#34;&gt;対応できます&lt;/a&gt;。これがこのサンプルでサービスプリンシパル認証を選んだ理由です。&lt;/p&gt;

&lt;h3 id=&#34;管理者の環境変数を設定する&#34;&gt;管理者の環境変数を設定する&lt;/h3&gt;

&lt;p&gt;Terraformが認証関連で必要な情報を環境変数で設定します。先ほど控えた情報を使います。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export ARM_SUBSCRIPTION_ID=&amp;quot;&amp;lt;your subscription id&amp;gt;&amp;quot;
export ARM_CLIENT_ID=&amp;quot;&amp;lt;your servicce principal appid&amp;gt;&amp;quot;
export ARM_CLIENT_SECRET=&amp;quot;&amp;lt;your service principal password&amp;gt;&amp;quot;
export ARM_TENANT_ID=&amp;quot;&amp;lt;your service principal tenant&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;workspaceを作る&#34;&gt;Workspaceを作る&lt;/h3&gt;

&lt;p&gt;開発(dev)/ステージング(stage)/本番(prod)、3つのWorkspaceを作る例です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform workspace new dev
terraform workspace new stage
terraform workspace new prod
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;リソースグループを作る&#34;&gt;リソースグループを作る&lt;/h3&gt;

&lt;p&gt;まずWorkspace別にリソースグループを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az group create -n tf-sample-dev-rg -l japaneast
az group create -n tf-sample-stage-rg -l japaneast
az group create -n tf-sample-prod-rg -l japaneast
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;リソースグループ名にはルールがあります。Workspace別にリソースグループを分離するため、Terraformのコードで ${terraform.workspace} 変数を使っているためです。この変数は実行時に評価されます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data &amp;quot;azurerm_resource_group&amp;quot; &amp;quot;resource_group&amp;quot; {
  name = &amp;quot;${var.resource_group_name}-${terraform.workspace}-rg&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;${var.resource_group_name} は接頭辞です。サンプルではvariables.tfで&amp;rdquo;tf-sample&amp;rdquo;と指定しています。&lt;/p&gt;

&lt;p&gt;次にBackend、state共有向けリソースグループを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az group create -n tf-sample-state-rg -l japaneast
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このリソースグループは、各projectのbackend.tfで指定しています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform {
  backend &amp;quot;azurerm&amp;quot; {
    resource_group_name  = &amp;quot;tf-sample-state-rg&amp;quot;
    storage_account_name = &amp;quot;&amp;lt;your storage account name&amp;gt;&amp;quot;
    container_name       = &amp;quot;tfstate-project-a&amp;quot;
    key                  = &amp;quot;terraform.tfstate&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最後にアプリ開発者がリソースグループtf-sample-dev-rg、tf-sample-state-rgへアクセスできるよう、アプリ開発者向けサービスプリンシパルを作成します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az ad sp create-for-rbac --role=&amp;quot;Contributor&amp;quot; --scopes &amp;quot;/subscriptions/&amp;lt;your subscription id&amp;gt;/resourceGroups/tf-sample-dev-rg&amp;quot; &amp;quot;/subscriptions/&amp;lt;your subscription id&amp;gt;/resourceGroups/tf-sample-state-rg&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;出力されるappId、password、tenantは、アプリ開発者向けに控えておきます。&lt;/p&gt;

&lt;h3 id=&#34;backendを準備する&#34;&gt;Backendを準備する&lt;/h3&gt;

&lt;p&gt;project別にストレージアカウントとコンテナーを作ります。tf-sample-state-rgに&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ストレージアカウント (名前は任意)&lt;/li&gt;
&lt;li&gt;コンテナー *2 (tfstate-project-a, tfstate-shared)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;を作ってください。GUIでもCLIでも、お好きなやり方で。&lt;/p&gt;

&lt;p&gt;その後、project_a/backend.tf.sample、shared/backend.tf.sampleをそれぞれbackend.tfにリネームし、先ほど作ったストレージアカウント名を指定します。以下はproject_a/backend.tf.sampleの例。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform {
  backend &amp;quot;azurerm&amp;quot; {
    resource_group_name  = &amp;quot;tf-sample-state-rg&amp;quot;
    storage_account_name = &amp;quot;&amp;lt;your storage account name&amp;gt;&amp;quot;
    container_name       = &amp;quot;tfstate-project-a&amp;quot;
    key                  = &amp;quot;terraform.tfstate&amp;quot;
  }
}

data &amp;quot;terraform_remote_state&amp;quot; &amp;quot;shared&amp;quot; {
  backend = &amp;quot;azurerm&amp;quot;

  config {
    resource_group_name  = &amp;quot;tf-sample-state-rg&amp;quot;
    storage_account_name = &amp;quot;&amp;lt;your storage account name&amp;gt;&amp;quot;
    container_name       = &amp;quot;tfstate-shared&amp;quot;
    key                  = &amp;quot;terraform.tfstateenv:${terraform.workspace}&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これで準備完了です。&lt;/p&gt;

&lt;h3 id=&#34;実行&#34;&gt;実行&lt;/h3&gt;

&lt;p&gt;Workspaceをdevに切り替えます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform workspace select dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;まずは土台となるリソースを作成するsharedから。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd shared
terraform init
terraform plan
terraform apply
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;土台となるリソースが作成されたら、次はproject_aを。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd ../project_a
terraform init
terraform plan
terraform apply
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここでは割愛しますが、dev向けサービスプリンシパルで認証しても、dev Workspaceではplan、apply可能です。&lt;/p&gt;

&lt;p&gt;dev Workspaceでコードが育ったら、stage/prod Workspaceに切り替えて実行します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform workspace select stage
[以下devと同様の操作]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然、dev向けサービスプリンシパルで認証している場合は、stage/prodでのplan、apply、もちろんdestroyも失敗します。stage/prod リソースグループにアクセスする権限がないからです。&lt;/p&gt;

&lt;h2 id=&#34;参考情報&#34;&gt;参考情報&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/terraform/&#34;&gt;Terraform on Azure のドキュメント&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/terraform-providers/terraform-provider-azurerm/tree/master/examples&#34;&gt;サンプル集 on GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Windows上でLinux向けGoバイナリをDockerでビルドする</title>
      <link>http://torumakabe.github.io/post/golang_build_onwin_tolnx_docker/</link>
      <pubDate>Mon, 04 Dec 2017 22:00:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/golang_build_onwin_tolnx_docker/</guid>
      <description>

&lt;h2 id=&#34;小ネタです&#34;&gt;小ネタです&lt;/h2&gt;

&lt;p&gt;Goはクロスプラットフォーム開発しやすい言語なのですが、Windows上でLinux向けバイナリーをビルドするなら、gccが要ります。正直なところ入れたくありません。なのでDockerでやります。&lt;/p&gt;

&lt;h2 id=&#34;条件&#34;&gt;条件&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Docker for Windows

&lt;ul&gt;
&lt;li&gt;Linuxモード&lt;/li&gt;
&lt;li&gt;ドライブ共有&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;powershell窓で実行&#34;&gt;PowerShell窓で実行&lt;/h2&gt;

&lt;p&gt;ビルドしたいGoのソースがあるディレクトリで以下のコマンドを実行します。Linux向けバイナリーが同じディレクトリに出来ます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run --rm -it -e GOPATH=/go --mount type=bind,source=${env:GOPATH},target=/go --mount type=bind,source=${PWD},target=/work -w /work golang:1.9.2-alpine go build -a -tags netgo -installsuffix netgo -o yourapp_linux
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;golang:1.9.2-alpine DockerイメージはGOPATHに/goを&lt;a href=&#34;https://github.com/docker-library/golang/blob/0f5ee2149d00dcdbf48fca05acf582e45d8fa9a5/1.9/alpine3.6/Dockerfile&#34;&gt;設定して&lt;/a&gt;ビルドされていますが、念のため実行時にも設定&lt;/li&gt;
&lt;li&gt;-v オプションでのマウントは&lt;a href=&#34;https://docs.docker.com/engine/admin/volumes/bind-mounts/&#34;&gt;非推奨&lt;/a&gt;になったので &amp;ndash;mount で&lt;/li&gt;
&lt;li&gt;スタティックリンク&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Azure Blob アップローダーをGoで書いた、そしてその理由</title>
      <link>http://torumakabe.github.io/post/azblob_golang/</link>
      <pubDate>Tue, 28 Nov 2017 08:45:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/azblob_golang/</guid>
      <description>

&lt;h2 id=&#34;azure-blob-アップローダーをgoで書いた&#34;&gt;Azure Blob アップローダーをGoで書いた&lt;/h2&gt;

&lt;p&gt;ふたつほど理由があり、GolangでAzure Blobのファイルアップローダーを書きました。&lt;/p&gt;

&lt;h2 id=&#34;ひとつめの理由-sdkが新しくなったから&#34;&gt;ひとつめの理由: SDKが新しくなったから&lt;/h2&gt;

&lt;p&gt;最近公式ブログで&lt;a href=&#34;https://azure.microsoft.com/en-us/blog/preview-the-new-azure-storage-sdk-for-go-storage-sdks-roadmap/&#34;&gt;紹介された&lt;/a&gt;通り、Azure Storage SDK for Goが再設計され、プレビューが始まりました。GoはDockerやKubernetes、Terraformなど最近話題のプラットフォームやツールを書くのに使われており、ユーザーも増えています。再設計してもっと使いやすくしてちょ、という要望が多かったのも、うなずけます。&lt;/p&gt;

&lt;p&gt;ということで、新しいSDKで書いてみたかった、というのがひとつめの理由です。ローカルにあるファイルを読んでBlobにアップロードするコードは、こんな感じ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
	&amp;quot;context&amp;quot;
	&amp;quot;flag&amp;quot;
	&amp;quot;fmt&amp;quot;
	&amp;quot;log&amp;quot;
	&amp;quot;net/url&amp;quot;
	&amp;quot;os&amp;quot;

	&amp;quot;github.com/Azure/azure-storage-blob-go/2016-05-31/azblob&amp;quot;
)

var (
	accountName    string
	accountKey     string
	containerName  string
	fileName       string
	blockSize      int64
	blockSizeBytes int64
)

func init() {
	flag.StringVar(&amp;amp;accountName, &amp;quot;account-name&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;(Required) Storage Account Name&amp;quot;)
	flag.StringVar(&amp;amp;accountKey, &amp;quot;account-key&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;(Required) Storage Account Key&amp;quot;)
	flag.StringVar(&amp;amp;containerName, &amp;quot;c&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;(Required - short option) Blob Container Name&amp;quot;)
	flag.StringVar(&amp;amp;containerName, &amp;quot;container-name&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;(Required) Blob Container Name&amp;quot;)
	flag.StringVar(&amp;amp;fileName, &amp;quot;f&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;(Required - short option) Upload filename&amp;quot;)
	flag.StringVar(&amp;amp;fileName, &amp;quot;file&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;(Required) Upload filename&amp;quot;)
	flag.Int64Var(&amp;amp;blockSize, &amp;quot;b&amp;quot;, 4, &amp;quot;(Optional - short option) Blob Blocksize (MB) - From 1 to 100. Max filesize depends on this value. Max filesize = Blocksize * 50,000 blocks&amp;quot;)
	flag.Int64Var(&amp;amp;blockSize, &amp;quot;blocksize&amp;quot;, 4, &amp;quot;(Optional) Blob Blocksize (MB) - From 1 to 100. Max filesize depends on this value. Max filesize = Blocksize * 50,000 blocks&amp;quot;)
	flag.Parse()

	if (blockSize &amp;lt; 1) || (blockSize) &amp;gt; 100 {
		fmt.Println(&amp;quot;Blocksize must be from 1MB to 100MB&amp;quot;)
		os.Exit(1)
	}
	blockSizeBytes = blockSize * 1024 * 1024
}

func main() {
	file, err := os.Open(fileName)
	if err != nil {
		log.Fatal(err)
	}
	defer file.Close()
	fileSize, err := file.Stat()
	if err != nil {
		log.Fatal(err)
	}

	u, _ := url.Parse(fmt.Sprintf(&amp;quot;https://%s.blob.core.windows.net/%s/%s&amp;quot;, accountName, containerName, fileName))
	blockBlobURL := azblob.NewBlockBlobURL(*u, azblob.NewPipeline(azblob.NewSharedKeyCredential(accountName, accountKey), azblob.PipelineOptions{}))

	ctx := context.Background()

	fmt.Println(&amp;quot;Uploading block blob...&amp;quot;)
	putBlockList, err := azblob.UploadStreamToBlockBlob(ctx, file, fileSize.Size(), blockBlobURL,
		azblob.UploadStreamToBlockBlobOptions{
			BlockSize: blockSizeBytes,
			Progress: func(bytesTransferred int64) {
				fmt.Printf(&amp;quot;Uploaded %d of %d bytes.\n&amp;quot;, bytesTransferred, fileSize.Size())
			},
		})
	if err != nil {
		log.Fatal(err)
	}
	_ = putBlockList // Avoid compiler&#39;s &amp;quot;declared and not used&amp;quot; error

	fmt.Println(&amp;quot;Done&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以前のSDKと比較し、スッキリ書けるようになりました。進行状況もPipelineパッケージを使って、楽に取れるようになっています。ブロック分割のロジックを書く必要もなくなりました。ブロックサイズを指定すればOK。&lt;/p&gt;

&lt;p&gt;ちなみにファイルサイズがブロックサイズで割り切れると最終ブロックの転送がエラーになるバグを見つけたのですが、&lt;a href=&#34;https://github.com/Azure/azure-storage-blob-go/issues/8&#34;&gt;修正してもらった&lt;/a&gt;ので、次のリリースでは解決していると思います。&lt;/p&gt;

&lt;h2 id=&#34;ふたつめの理由-レガシー対応&#34;&gt;ふたつめの理由: レガシー対応&lt;/h2&gt;

&lt;p&gt;Blobのアップロードが目的であれば、Azure CLIをインストールすればOK。以上。なのですが、残念ながらそれができないケースがあります。&lt;/p&gt;

&lt;p&gt;たとえば。Azure CLI(2.0)はPythonで書かれています。なので、Pythonのバージョンや依存パッケージの兼ね合いで、「ちょっとそれウチのサーバーに入れるの？汚さないでくれる？ウチはPython2.6よ」と苦い顔をされることが、あるんですね。気持ちはわかります。立場の数だけ正義があります。Docker?その1歩半くらい前の話です。&lt;/p&gt;

&lt;p&gt;ですが、オンプレのシステムからクラウドにデータをアップロードして処理したい、なんていうニーズが急増している昨今、あきらめたくないわけであります。どうにか既存環境に影響なく入れられないものかと。そこでシングルバイナリーを作って、ポンと置いて、動かせるGoは尊いわけです。&lt;/p&gt;

&lt;p&gt;ファイルのアップロードだけでなく、Azureにちょっとした処理を任せたい、でもそれはいじりづらいシステムの上なのねん、って話は、結構多いんですよね。ということでシングルバイナリーを作って、ポンと置いて、動かせるGoは尊いわけです。大事なことなので2回書きました。&lt;/p&gt;

&lt;p&gt;C#やNode、Python SDKと比較してGoのそれはまだ物足りないところも多いわけですが、今後注目ということで地道に盛り上がっていこうと思います。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>自動化を愛するWindows使いへ Boxstarterのすすめ</title>
      <link>http://torumakabe.github.io/post/intro_boxstarter/</link>
      <pubDate>Fri, 13 Oct 2017 14:30:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/intro_boxstarter/</guid>
      <description>

&lt;h2 id=&#34;windowsのセットアップどうする問題&#34;&gt;Windowsのセットアップどうする問題&lt;/h2&gt;

&lt;p&gt;そろそろFall Creators Updateが来ますね。これを機にクリーンインストールしようか、という人も多いのではないでしょうか。端末って使っているうちに汚れていく宿命なので、わたしは定期的に「こうあるべき」という状態に戻します。年に2～3回はスッキリしたい派なので、アップデートはいいタイミングです。&lt;/p&gt;

&lt;p&gt;でもクリーンインストールすると、設定やアプリケーションの導入をGUIでやり直すのが、すこぶるめんどくせぇわけです。自動化したいですね。そこでBoxstarterをおすすめします。便利なのに、意外に知られていない。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://boxstarter.org/&#34;&gt;Boxstarter&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;わたしはマイクロソフトの仲間、Jessieの&lt;a href=&#34;https://blog.jessfraz.com/post/windows-for-linux-nerds/&#34;&gt;ポスト&lt;/a&gt;で知りました。サンクスJessie。&lt;/p&gt;

&lt;h2 id=&#34;boxstarterで出来ること&#34;&gt;Boxstarterで出来ること&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;シンプルなスクリプトで

&lt;ul&gt;
&lt;li&gt;Windowsの各種設定&lt;/li&gt;
&lt;li&gt;Chocolateyパッケージの導入&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;設定ファイルをネットワーク経由で読み込める

&lt;ul&gt;
&lt;li&gt;Gistから&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ベアメタルでも仮想マシンでもOK&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;実行手順&#34;&gt;実行手順&lt;/h2&gt;

&lt;p&gt;手順は&lt;a href=&#34;http://boxstarter.org/Learn/WebLauncher&#34;&gt;Boxstarterのサイト&lt;/a&gt;で紹介されています。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;スクリプトを作る&lt;/li&gt;
&lt;li&gt;Gistに上げる&lt;/li&gt;
&lt;li&gt;Boxstarterを導入する&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;PowerShell 3以降であれば&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;. { iwr -useb http://boxstarter.org/bootstrapper.ps1 } | iex; get-boxstarter -Force
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Gist上のスクリプトを指定して実行する&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;なお2017/10/13時点で、Boxstarterサイトのサンプルにはtypoがあるので注意 (-PackageNameオプション)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Install-BoxstarterPackage -PackageName &amp;quot;https://gist.githubusercontent.com/ToruMakabe/976ceab239ec930f8651cfd72087afac/raw/4fc77a1d08f078869962ae82233b2f8abc32d31f/boxstarter.txt&amp;quot; -DisableReboots
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上。&lt;/p&gt;

&lt;h2 id=&#34;サンプルスクリプト&#34;&gt;サンプルスクリプト&lt;/h2&gt;

&lt;p&gt;スクリプトは&lt;a href=&#34;https://gist.github.com/ToruMakabe/976ceab239ec930f8651cfd72087afac&#34;&gt;こんな感じ&lt;/a&gt;に書きます。&lt;/p&gt;

&lt;p&gt;ちなみに、わたしの環境です。こまごまとした設定やツールの導入はもちろん、Hyper-Vやコンテナ、Windows Subsystem for Linuxの導入も、一気にやっつけます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Learn more: http://boxstarter.org/Learn/WebLauncher

# Chocolateyパッケージがないもの、パッケージ更新が遅いものは別途入れます。メモです。
# Install manually (Ubuntu, VS, snip, Azure CLI/PS/Storage Explorer, Terraform, Go, 1Password 6, Driver Management Tool)

#---- TEMPORARY ---
Disable-UAC

#--- Fonts ---
choco install inconsolata
  
#--- Windows Settings ---
# 可能な設定はここで確認 --&amp;gt; [Boxstarter WinConfig Features](http://boxstarter.org/WinConfig)
Disable-GameBarTips

Set-WindowsExplorerOptions -EnableShowHiddenFilesFoldersDrives -EnableShowFileExtensions
Set-TaskbarOptions -Size Small -Dock Bottom -Combine Full -Lock

Set-ItemProperty -Path HKCU:\Software\Microsoft\Windows\CurrentVersion\Explorer\Advanced -Name NavPaneShowAllFolders -Value 1

#--- Windows Subsystems/Features ---
choco install Microsoft-Hyper-V-All -source windowsFeatures
choco install Microsoft-Windows-Subsystem-Linux -source windowsfeatures
choco install containers -source windowsfeatures

#--- Tools ---
choco install git.install
choco install yarn
choco install sysinternals
choco install 7zip

#--- Apps ---
choco install googlechrome
choco install docker-for-windows
choco install microsoft-teams
choco install slack
choco install putty
choco install visualstudiocode

#--- Restore Temporary Settings ---
Enable-UAC
Enable-MicrosoftUpdate
Install-WindowsUpdate -acceptEula
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;便利。&lt;/p&gt;

&lt;p&gt;ちなみにわたしはドキュメント類はOneDrive、コードはプライベートGit/GitHub、エディタの設定はVisual Studio Code &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=Shan.code-settings-sync&#34;&gt;Settings Sync拡張&lt;/a&gt;を使っているので、Boxstarterと合わせ、 環境の再現は2～3時間もあればできます。最近、バックアップからのリストアとか、してないです。&lt;/p&gt;

&lt;p&gt;新しい端末の追加もすぐできるので、物欲が捗るという副作用もあります。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Azure VPN Gateway Active/Active構成のスループット検証(リージョン内)</title>
      <link>http://torumakabe.github.io/post/azure_vpngw_act_act_perf/</link>
      <pubDate>Sun, 08 Oct 2017 10:30:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/azure_vpngw_act_act_perf/</guid>
      <description>

&lt;h2 id=&#34;動機&#34;&gt;動機&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kogelog.com/&#34;&gt;焦げlogさん&lt;/a&gt;で、とても興味深いエントリを拝見しました。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kogelog.com/2017/10/06/20171006-01/&#34;&gt;Azure VPN ゲートウェイをアクティブ/アクティブ構成した場合にスループットが向上するのか検証してみました&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;確かにActive/Active構成にはスループット向上を期待したくなります。その伸びが測定されており、胸が熱くなりました。ですが、ちょっと気になったのは&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;※それと、VpnGw3 よりも VpnGw2 のほうがスループットがよかったのが一番の謎ですが…&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ここです。VPN GatewayのSKU、VpnGw3とVpnGw2には小さくない価格差があり、その基準はスループットです。ここは現状を把握しておきたいところ。すごく。&lt;/p&gt;

&lt;p&gt;そこで、焦げlogさんの検証パターンの他に、追加で検証しました。それは同一リージョン内での測定です。リージョン内でVPNを張るケースはまれだと思いますが、リージョンが分かれることによる&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;遅延&lt;/li&gt;
&lt;li&gt;リージョン間通信に関するサムシング&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;を除き、VPN Gateway自身のスループットを測定したいからです。焦げlogさんの測定は東日本/西日本リージョン間で行われたので、その影響を確認する価値はあるかと考えました。&lt;/p&gt;

&lt;h2 id=&#34;検証方針&#34;&gt;検証方針&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;同一リージョン(東日本)に、2つのVNETを作る&lt;/li&gt;
&lt;li&gt;それぞれのVNETにVPN Gatewayを配置し、接続する&lt;/li&gt;
&lt;li&gt;比較しやすいよう、焦げlogさんの検証と条件を合わせる

&lt;ul&gt;
&lt;li&gt;同じ仮想マシンサイズ: DS3_V2&lt;/li&gt;
&lt;li&gt;同じストレージ: Premium Storage Managed Disk&lt;/li&gt;
&lt;li&gt;同じOS: Ubuntu 16.04&lt;/li&gt;
&lt;li&gt;同じツール: ntttcp&lt;/li&gt;
&lt;li&gt;同じパラメータ: ntttcp -r -m 16,*,&lt;IP&gt; -t 300&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;送信側 VNET1 -&amp;gt; 受信側 VNET2 のパターンに絞る&lt;/li&gt;
&lt;li&gt;スループットのポテンシャルを引き出す検証はしない&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;結果&#34;&gt;結果&lt;/h2&gt;

&lt;h3 id=&#34;vpngw1-650mbps&#34;&gt;VpnGW1(650Mbps)&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;パターン　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;送信側GW構成　　　　　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;受信側GW構成　　　　　　　　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;送信側スループット　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　受信側スループット&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　スループット平均&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　パターン1との比較&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン1　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;677.48Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;676.38Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;676.93Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン2　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;674.34Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;673.85Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;674.10Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;99%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン3　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;701.19Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;699.91Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;700.55Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;103%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;vpngw2-1gbps&#34;&gt;VpnGW2(1Gbps)&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;パターン　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;送信側GW構成　　　　　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;受信側GW構成　　　　　　　　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;送信側スループット　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　受信側スループット&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　スループット平均&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　パターン1との比較&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン1　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;813.09Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;805.60Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;809.35Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン2　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.18Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.18Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.18Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン3　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.02Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;256%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;vpngw3-1-25gbps&#34;&gt;VpnGW3(1.25Gbps)&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;パターン　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;送信側GW構成　　　　　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;受信側GW構成　　　　　　　　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;送信側スループット　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　受信側スループット&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　スループット平均&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　パターン1との比較&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン1　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;958.56Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;953.72Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;956.14Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン2　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.39Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.39Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.39Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン3　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.19Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.19Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.19Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;234%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;sku視点-パターン1-act-stb-to-act-stb&#34;&gt;SKU視点 パターン1(Act/Stb to Act/Stb)&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;SKU　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　スループット平均&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　VpnGw1との比較&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw1　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;676.93Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw2　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;809.35Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;119%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw3　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;956.14Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;141%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;sku視点-パターン2-act-stb-to-act-act&#34;&gt;SKU視点 パターン2(Act/Stb to Act/Act)&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;SKU　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　スループット平均&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　VpnGw1との比較&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw1　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;674.10Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw2　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.18Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;179%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw3　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.39Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;211%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;sku視点-パターン3-act-act-to-act-act&#34;&gt;SKU視点 パターン3(Act/Act to Act/Act)&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;SKU　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　スループット平均&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　VpnGw1との比較&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw1　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;700.55Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw2　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;297%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw3　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.19Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;320%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;考察と推奨&#34;&gt;考察と推奨&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;リージョン間の遅延やサムシングを除くと、SKUによるGatewayのスループット差は測定できる

&lt;ul&gt;
&lt;li&gt;Act/Actでないパターン1(Act/Stb to Act/Stb)で、その差がわかる&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;公式ドキュメントの通り、GatewayのAct/Act構成は可用性向上が目的であるため、スループットの向上はボーナスポイントと心得る

&lt;ul&gt;
&lt;li&gt;期待しちゃうのが人情ではありますが&lt;/li&gt;
&lt;li&gt;VpnGw2がコストパフォーマンス的に最適という人が多いかもしれませんね 知らんけど&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Azure Event GridでBlobイベントを拾う</title>
      <link>http://torumakabe.github.io/post/azure_blobevent/</link>
      <pubDate>Tue, 05 Sep 2017 12:00:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/azure_blobevent/</guid>
      <description>

&lt;h2 id=&#34;event-gridがblobに対応&#34;&gt;Event GridがBlobに対応&lt;/h2&gt;

&lt;p&gt;Event GridがBlobのイベントを拾えるように&lt;a href=&#34;https://azure.microsoft.com/en-us/blog/announcing-azure-blob-storage-events-preview/&#34;&gt;なりました&lt;/a&gt;。まだ申請が必要なプライベートプレビュー段階ですが、使い勝手の良いサービスに育つ予感がします。このたび検証する機会があったので、共有を。&lt;/p&gt;

&lt;p&gt;プレビュー中なので、今後仕様が変わるかもしれないこと、不具合やメンテナンス作業の可能性などは、ご承知おきください。&lt;/p&gt;

&lt;h2 id=&#34;event-gridがblobに対応して何がうれしいか&#34;&gt;Event GridがBlobに対応して何がうれしいか&lt;/h2&gt;

&lt;p&gt;Event Gridは、Azureで発生した様々なイベントを検知してWebhookで通知するサービスです。カスタムトピックも作成できます。&lt;/p&gt;

&lt;p&gt;イベントの発生元をPublisherと呼びますが、このたびPublisherとしてAzureのBlobがサポートされました。Blobの作成、削除イベントを検知し、Event GridがWebhookで通知します。通知先はHandlerと呼びます。Publisherとそこで拾うイベント、Handlerを紐づけるのがSubscriptionです。Subscriptionにはフィルタも定義できます。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/ff3644c9-58ab-4729-8939-66a83ab0605d.png&#34; alt=&#34;コンセプト&#34; title=&#34;Concept&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Event Gridに期待する理由はいくつかあります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;フィルタ

&lt;ul&gt;
&lt;li&gt;特定のBlobコンテナーにあるjpegファイルの作成イベントのみで発火させる、なんてことができます&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;信頼性

&lt;ul&gt;
&lt;li&gt;リトライ機能があるので、Handlerが一時的に黙ってしまっても対応できます&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;スケールと高スループット

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/azure-functions/functions-bindings-storage-blob#blob-storage-triggers-and-bindings&#34;&gt;Azure Functions Blobトリガー&lt;/a&gt;のようにHandler側で定期的にスキャンする必要がありません。これまではファイル数が多いとつらかった&lt;/li&gt;
&lt;li&gt;具体的な数値はプレビュー後に期待しましょう&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ファンアウト

&lt;ul&gt;
&lt;li&gt;ひとつのイベントを複数のHandlerに紐づけられます&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Azureの外やサードパーティーとの連携

&lt;ul&gt;
&lt;li&gt;Webhookでシンプルにできます&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;前提条件&#34;&gt;前提条件&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Publisherに設定できるストレージアカウントはBlobストレージアカウントのみです。汎用ストレージアカウントは対応していません&lt;/li&gt;
&lt;li&gt;現時点ではWest Central USリージョンのみで提供しています&lt;/li&gt;
&lt;li&gt;プライベートプレビューは申請が必要です&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Azure CLIの下記コマンドでプレビューに申請できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az provider register --namespace  Microsoft.EventGrid
az feature register --name storageEventSubscriptions --namespace Microsoft.EventGrid
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドで確認し、statusが&amp;rdquo;Registered&amp;rdquo;であれば使えます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az feature show --name storageEventSubscriptions --namespace Microsoft.EventGrid
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;使い方&#34;&gt;使い方&lt;/h2&gt;

&lt;p&gt;ストレージアカウントの作成からSubscription作成までの流れを追ってみましょう。&lt;/p&gt;

&lt;p&gt;リソースグループを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az group create -n blobeventpoc-rg -l westcentralus
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Blobストレージアカウントを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az storage account create -n blobeventpoc01 -l westcentralus -g blobeventpoc-rg --sku Standard_LRS --kind BlobStorage --access-tier Hot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ではいよいよEvent GridのSubscriptionを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az eventgrid resource event-subscription create --endpoint https://requestb.in/y4jgj2x0 -n blobeventpocsub-jpg --prov
ider-namespace Microsoft.Storage --resource-type storageAccounts --included-event-types Microsoft.Storage.BlobCreated
-g blobeventpoc-rg --resource-name blobeventpoc01 --subject-ends-with jpg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下はパラメーターの補足です。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&amp;ndash;endpoint

&lt;ul&gt;
&lt;li&gt;Handlerのエンドポイントを指定します。ここではテストのために&lt;a href=&#34;https://requestb.in/&#34;&gt;RequestBin&lt;/a&gt;に作ったエンドポイントを指定します&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&amp;ndash;included-event-types

&lt;ul&gt;
&lt;li&gt;イベントの種類をフィルタします。Blobの削除イベントは不要で、作成のみ拾いたいため、Microsoft.Storage.BlobCreatedを指定します&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&amp;ndash;subject-ends-with

&lt;ul&gt;
&lt;li&gt;対象ファイルをフィルタします。Blob名の末尾文字列がjpgであるBlobのみイベントの対象にしました&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;では作成したストレージアカウントにBlobコンテナーを作成し、jpgファイルを置いてみましょう。テストには&lt;a href=&#34;https://azure.microsoft.com/ja-jp/features/storage-explorer/&#34;&gt;Azure Storage Explorer&lt;/a&gt;が便利です。&lt;/p&gt;

&lt;p&gt;RequestBinにWebhookが飛び、中身を見られます。スキーマの確認は&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-overview#event-schema&#34;&gt;こちら&lt;/a&gt;から。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[{
  &amp;quot;topic&amp;quot;: &amp;quot;/subscriptions/xxxxx-xxxxx-xxxxx-xxxxx/resourceGroups/blobeventpoc-rg/providers/Microsoft.Storage/storageAccounts/blobeventpoc01&amp;quot;,
  &amp;quot;subject&amp;quot;: &amp;quot;/blobServices/default/containers/images/blobs/handsomeyoungman.jpg&amp;quot;,
  &amp;quot;eventType&amp;quot;: &amp;quot;Microsoft.Storage.BlobCreated&amp;quot;,
  &amp;quot;eventTime&amp;quot;: &amp;quot;2017-09-02T02:25:15.2635962Z&amp;quot;,
  &amp;quot;id&amp;quot;: &amp;quot;f3ff6b96-001e-001d-6e92-23bdea0684d2&amp;quot;,
  &amp;quot;data&amp;quot;: {
    &amp;quot;api&amp;quot;: &amp;quot;PutBlob&amp;quot;,
    &amp;quot;clientRequestId&amp;quot;: &amp;quot;f3cab560-8f85-11e7-bad1-53b58c70ab53&amp;quot;,
    &amp;quot;requestId&amp;quot;: &amp;quot;f3ff6b96-001e-001d-6e92-23bdea000000&amp;quot;,
    &amp;quot;eTag&amp;quot;: &amp;quot;0x8D4F1A9D8A6703A&amp;quot;,
    &amp;quot;contentType&amp;quot;: &amp;quot;image/jpeg&amp;quot;,
    &amp;quot;contentLength&amp;quot;: 42497,
    &amp;quot;blobType&amp;quot;: &amp;quot;BlockBlob&amp;quot;,
    &amp;quot;url&amp;quot;: &amp;quot;https://blobeventpoc01.blob.core.windows.net/images/handsomeyoungman.jpg&amp;quot;,
    &amp;quot;sequencer&amp;quot;: &amp;quot;0000000000000BAB0000000000060986&amp;quot;,
    &amp;quot;storageDiagnostics&amp;quot;: {
      &amp;quot;batchId&amp;quot;: &amp;quot;f3a538cf-5b88-4bbf-908a-20a37c65e238&amp;quot;
    }
  }
}]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.jpgだけじゃなくて.jpegも使われるかもしれませんね。ということで、エンドポイントが同じでフィルタ定義を変えたSubscriptionを追加します。&amp;ndash;subject-ends-withをjpegとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az eventgrid resource event-subscription create --endpoint https://requestb.in/y4jgj2x0 -n blobeventpocsub-jpeg --pro
vider-namespace Microsoft.Storage --resource-type storageAccounts --included-event-types Microsoft.Storage.BlobCreated -
g blobeventpoc-rg --resource-name blobeventpoc01 --subject-ends-with jpeg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;すると、拡張子.jpegのファイルをアップロードしても発火しました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[{
  &amp;quot;topic&amp;quot;: &amp;quot;/subscriptions/xxxxx-xxxxx-xxxxx-xxxxx/resourceGroups/blobeventpoc-rg/providers/Microsoft.Storage/storageAccounts/blobeventpoc01&amp;quot;,
  &amp;quot;subject&amp;quot;: &amp;quot;/blobServices/default/containers/images/blobs/handsomeyoungman.jpeg&amp;quot;,
  &amp;quot;eventType&amp;quot;: &amp;quot;Microsoft.Storage.BlobCreated&amp;quot;,
  &amp;quot;eventTime&amp;quot;: &amp;quot;2017-09-02T02:36:33.827967Z&amp;quot;,
  &amp;quot;id&amp;quot;: &amp;quot;e8b036ee-001e-00e7-4994-23740d06225b&amp;quot;,
  &amp;quot;data&amp;quot;: {
    &amp;quot;api&amp;quot;: &amp;quot;PutBlob&amp;quot;,
    &amp;quot;clientRequestId&amp;quot;: &amp;quot;883ff7e0-8f87-11e7-bad1-53b58c70ab53&amp;quot;,
    &amp;quot;requestId&amp;quot;: &amp;quot;e8b036ee-001e-00e7-4994-23740d000000&amp;quot;,
    &amp;quot;eTag&amp;quot;: &amp;quot;0x8D4F1AB6D1B24F6&amp;quot;,
    &amp;quot;contentType&amp;quot;: &amp;quot;image/jpeg&amp;quot;,
    &amp;quot;contentLength&amp;quot;: 42497,
    &amp;quot;blobType&amp;quot;: &amp;quot;BlockBlob&amp;quot;,
    &amp;quot;url&amp;quot;: &amp;quot;https://blobeventpoc01.blob.core.windows.net/images/handsomeyoungman.jpeg&amp;quot;,
    &amp;quot;sequencer&amp;quot;: &amp;quot;0000000000000BAB0000000000060D42&amp;quot;,
    &amp;quot;storageDiagnostics&amp;quot;: {
      &amp;quot;batchId&amp;quot;: &amp;quot;9ec5c091-061d-4111-ad82-52d9803ce373&amp;quot;
    }
  }
}]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;azure-functionsに画像リサイズファンクションを作って連携してみる&#34;&gt;Azure Functionsに画像リサイズファンクションを作って連携してみる&lt;/h2&gt;

&lt;p&gt;Gvent Grid側の動きが確認できたので、サンプルアプリを作って検証してみましょう。Azure Functions上に画像ファイルのサイズを変えるHandlerアプリを作ってみます。&lt;/p&gt;

&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;

&lt;p&gt;当初想定したのは、ひとつのファンクションで、トリガーはEventGrid、入出力バインドにBlob、という作りでした。ですが、以下のように設計を変えました。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ToruMakabe/Images/master/blobevent-function-bindings.png&#34; alt=&#34;Bindings&#34; title=&#34;Bindings&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Using &lt;a href=&#34;https://functions-visualizer.azurewebsites.net/&#34;&gt;Azure Functions Bindings Visualizer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;その理由はEvent Grid Blobイベントのペイロードです。Blobファイル名がURLで渡されます。Azure FunctionsのBlob入出力バインド属性、&amp;rdquo;path&amp;rdquo;にURLは使えません。使えるのはコンテナー名+ファイル名です。&lt;/p&gt;

&lt;p&gt;入出力バインドを使わず、アプリのロジック内でStorage SDKを使って入出力してもいいのですが、Azure Functionsの魅力のひとつは宣言的にトリガーとバインドを定義し、アプリをシンプルに書けることなので、あまりやりたくないです。&lt;/p&gt;

&lt;p&gt;そこでイベントを受けてファイル名を取り出してQueueに入れるファンクションと、そのQueueをトリガーに画像をリサイズするファンクションに分けました。&lt;/p&gt;

&lt;p&gt;なお、この悩みはAzureの開発チームも認識しており、Functions側で対応する方針とのことです。&lt;/p&gt;

&lt;h3 id=&#34;handler&#34;&gt;Handler&lt;/h3&gt;

&lt;p&gt;C#(csx)で、Event GridからのWebhookを受けるHandlerを作ります。PublisherがBlobの場合、ペイロードにBlobのURLが入っていますので、そこからファイル名を抽出します。そして、そのファイル名をQueueに送ります。ファンクション名はBlobEventHandlerとしました。なおEventGridTriggerテンプレートは、現在は[試験段階]シナリオに入っています。&lt;/p&gt;

&lt;p&gt;[run.csx]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#r &amp;quot;Newtonsoft.json&amp;quot;
using Microsoft.Azure.WebJobs.Extensions.EventGrid;

public static void Run(EventGridEvent eventGridEvent, out string outputQueueItem, TraceWriter log)
{
    string imageUrl = eventGridEvent.Data[&amp;quot;url&amp;quot;].ToString();
    outputQueueItem = System.IO.Path.GetFileName(imageUrl);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Event GridのWebJobs拡張向けパッケージを指定します。&lt;/p&gt;

&lt;p&gt;[project.json]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
&amp;quot;frameworks&amp;quot;: {
  &amp;quot;net46&amp;quot;:{
    &amp;quot;dependencies&amp;quot;: {
      &amp;quot;Microsoft.Azure.WebJobs.Extensions.EventGrid&amp;quot;: &amp;quot;1.0.0-beta1-10006&amp;quot;
    }
  }
 }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;トリガーとバインドは以下の通りです。&lt;/p&gt;

&lt;p&gt;[function.json]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;bindings&amp;quot;: [
    {
      &amp;quot;type&amp;quot;: &amp;quot;eventGridTrigger&amp;quot;,
      &amp;quot;name&amp;quot;: &amp;quot;eventGridEvent&amp;quot;,
      &amp;quot;direction&amp;quot;: &amp;quot;in&amp;quot;
    },
    {
      &amp;quot;type&amp;quot;: &amp;quot;queue&amp;quot;,
      &amp;quot;name&amp;quot;: &amp;quot;outputQueueItem&amp;quot;,
      &amp;quot;queueName&amp;quot;: &amp;quot;imagefilename&amp;quot;,
      &amp;quot;connection&amp;quot;: &amp;quot;AzureWebJobsStorage&amp;quot;,
      &amp;quot;direction&amp;quot;: &amp;quot;out&amp;quot;
    }
  ],
  &amp;quot;disabled&amp;quot;: false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;resizer&#34;&gt;Resizer&lt;/h3&gt;

&lt;p&gt;Queueをトリガーに、Blobから画像ファイルを取り出し、縮小、出力するファンクションを作ります。ファンクション名はResizerとしました。&lt;/p&gt;

&lt;p&gt;[run.csx]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;using ImageResizer;

public static void Run(string myQueueItem, Stream inputBlob, Stream outputBlob, TraceWriter log)
{
  var imageBuilder = ImageResizer.ImageBuilder.Current;
  var size = imageDimensionsTable[ImageSize.Small];

  imageBuilder.Build(inputBlob, outputBlob,
    new ResizeSettings(size.Item1, size.Item2, FitMode.Max, null), false);

}

public enum ImageSize
{
  Small
}

private static Dictionary&amp;lt;ImageSize, Tuple&amp;lt;int, int&amp;gt;&amp;gt; imageDimensionsTable = new Dictionary&amp;lt;ImageSize, Tuple&amp;lt;int, int&amp;gt;&amp;gt;()
{
  { ImageSize.Small, Tuple.Create(100, 100) }
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ImageResizerのパッケージを指定します。&lt;/p&gt;

&lt;p&gt;[project.json]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
&amp;quot;frameworks&amp;quot;: {
  &amp;quot;net46&amp;quot;:{
    &amp;quot;dependencies&amp;quot;: {
      &amp;quot;ImageResizer&amp;quot;: &amp;quot;4.1.9&amp;quot;
    }
  }
 }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;トリガーとバインドは以下の通りです。{QueueTrigger}メタデータで、QueueのペイロードをBlobのpathに使います。ペイロードにはファイル名が入っています。&lt;/p&gt;

&lt;p&gt;また、画像を保存するBlobストレージアカウントの接続文字列は、環境変数BLOB_IMAGESへ事前に設定しています。なお、リサイズ後の画像を格納するBlobコンテナーは、&amp;rdquo;images-s&amp;rdquo;として別途作成しました。コンテナー&amp;rdquo;images&amp;rdquo;をイベントの発火対象コンテナーとして、Subscriptionにフィルタを定義したいからです。&lt;/p&gt;

&lt;p&gt;[function.json]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;bindings&amp;quot;: [
    {
      &amp;quot;name&amp;quot;: &amp;quot;myQueueItem&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;queueTrigger&amp;quot;,
      &amp;quot;direction&amp;quot;: &amp;quot;in&amp;quot;,
      &amp;quot;queueName&amp;quot;: &amp;quot;imagefilename&amp;quot;,
      &amp;quot;connection&amp;quot;: &amp;quot;AzureWebJobsStorage&amp;quot;
    },
    {
      &amp;quot;name&amp;quot;: &amp;quot;inputBlob&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;blob&amp;quot;,
      &amp;quot;path&amp;quot;: &amp;quot;images/{QueueTrigger}&amp;quot;,
      &amp;quot;connection&amp;quot;: &amp;quot;BLOB_IMAGES&amp;quot;,
      &amp;quot;direction&amp;quot;: &amp;quot;in&amp;quot;
    },
    {
      &amp;quot;name&amp;quot;: &amp;quot;outputBlob&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;blob&amp;quot;,
      &amp;quot;path&amp;quot;: &amp;quot;images-s/{QueueTrigger}&amp;quot;,
      &amp;quot;connection&amp;quot;: &amp;quot;BLOB_IMAGES&amp;quot;,
      &amp;quot;direction&amp;quot;: &amp;quot;out&amp;quot;
    }
  ],
  &amp;quot;disabled&amp;quot;: false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Handlerの準備が整いました。最後にEvent GridのSubscriptionを作成します。Azure FunctionsのBlobEventHandlerのトークン付きエンドポイントは、ポータルの[統合]で確認できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az eventgrid resource event-subscription create --endpoint &amp;quot;https://blobeventpoc.azurewebsites.net/admin/exte
nsions/EventGridExtensionConfig?functionName=BlobEventHandler&amp;amp;code=tokenTOKEN1234567890==&amp;quot; -n blobeventpocsub-jpg --provider-namespace Microsoft.Storage --resource-type storageAccounts --included-event-types &amp;quot;Microsoft.Storage.BlobCreated&amp;quot; -g blobeventpoc-rg --resource-name blobeventpoc01 --subject-begins-with &amp;quot;/blobServices/default/containers/images/&amp;quot;  --subject-ends-with jpg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これで、コンテナー&amp;rdquo;images&amp;rdquo;にjpgファイルがアップロードされると、コンテナー&amp;rdquo;images-s&amp;rdquo;に、リサイズされた同じファイル名の画像ファイルが出来上がります。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Azureでグローバルにデータをコピーするとどのくらい時間がかかるのか</title>
      <link>http://torumakabe.github.io/post/azureblobcopy_perf/</link>
      <pubDate>Tue, 13 Jun 2017 17:00:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/azureblobcopy_perf/</guid>
      <description>

&lt;h2 id=&#34;ファイルコピーの需要は根強い&#34;&gt;ファイルコピーの需要は根強い&lt;/h2&gt;

&lt;p&gt;グローバルでAzureを使うとき、データをどうやって同期、複製するかは悩みの種です。Cosmos DBなどリージョン間でデータ複製してくれるサービスを使うのが、楽ですし、おすすめです。&lt;/p&gt;

&lt;p&gt;でも、ファイルコピーを無くせないもろもろの事情もあります。となると、「地球の裏側へのファイルコピーに、どんだけ時間かかるのよ」は、課題です。&lt;/p&gt;

&lt;h2 id=&#34;調べてみた&#34;&gt;調べてみた&lt;/h2&gt;

&lt;p&gt;ということで、いくつかのパターンで調べたので参考までに。測定環境は以下の通り。&lt;/p&gt;

&lt;h3 id=&#34;ツールと実行環境&#34;&gt;ツールと実行環境&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;AzCopy 6.1.0&lt;/li&gt;
&lt;li&gt;Azure PowerShell 4.1.0&lt;/li&gt;
&lt;li&gt;Windows 10 1703&lt;/li&gt;
&lt;li&gt;ThinkPad X1 Carbon 2017, Core i7-7600U 2.8GHz, 16GB Memory&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;アクセス回線パターン&#34;&gt;アクセス回線パターン&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;一般的な回線&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;自宅(川崎)&lt;/li&gt;
&lt;li&gt;OCN光 100M マンションタイプ&lt;/li&gt;
&lt;li&gt;宅内は802.11ac(5GHz)&lt;/li&gt;
&lt;li&gt;川崎でアクセス回線に入り、横浜(保土ヶ谷)の局舎からインターネットへ&lt;/li&gt;
&lt;li&gt;ゲートウェイ名から推測&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;いい感じの回線&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;日本マイクロソフト 品川オフィス&lt;/li&gt;
&lt;li&gt;1Gbps 有線&lt;/li&gt;
&lt;li&gt;Azureデータセンターへ「ネットワーク的に近くて広帯域」&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;コピーするファイル&#34;&gt;コピーするファイル&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;総容量: 約60GB

&lt;ul&gt;
&lt;li&gt;6160ファイル&lt;/li&gt;
&lt;li&gt;1MB * 5000, 10MB * 1000, 100MB * 100, 500MB * 50, 1000MB * 10&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Linux fallocateコマンドで作成&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ファイル形式パターン&#34;&gt;ファイル形式パターン&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;ファイル、Blobそのまま送る (6160ファイル)&lt;/li&gt;
&lt;li&gt;ディスクイメージで送る (1ファイル)

&lt;ul&gt;
&lt;li&gt;Managed Diskとしてアタッチした100GBの領域にファイルシステムを作成し、6160ファイルを配置&lt;/li&gt;
&lt;li&gt;転送前にデタッチ、エクスポート(Blob SAS形式)&lt;/li&gt;
&lt;li&gt;AzCopyではなくAzure PowerShellでコピー指示 (AzCopyにBlob SAS指定オプションが見当たらなかった)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;対象のazureリージョン&#34;&gt;対象のAzureリージョン&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;東日本 (マスター、複製元と位置づける)&lt;/li&gt;
&lt;li&gt;米国中南部 (太平洋越え + 米国内を見たい)&lt;/li&gt;
&lt;li&gt;ブラジル南部&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;転送パターン&#34;&gt;転送パターン&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;ユーザー拠点の端末からAzureリージョン: AzCopy Upload&lt;/li&gt;
&lt;li&gt;Azureリージョン間 (Storage to Storage)

&lt;ul&gt;
&lt;li&gt;ファイル: AzCopy Copy&lt;/li&gt;
&lt;li&gt;イメージ: PowerShell Start-AzureStorageBlobCopy&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;結果&#34;&gt;結果&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;形式　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;コピー元　　　　　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;コピー先　　　　　　　　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;コマンド　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　並列数&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　実行時間(時:分:秒)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;自宅&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 東日本&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Upload&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;07:55:22&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;自宅&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 米国中南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Upload&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10:22:30&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;自宅&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure ブラジル南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Upload&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12:46:37&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;オフィス&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 東日本&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Upload&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;00:20:47&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;オフィス&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 米国中南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Upload&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;00:45.11&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;オフィス&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure ブラジル南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Upload&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;02:07.58&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 東日本&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 米国中南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Copy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;N/A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;00:28:55&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;イメージ　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 東日本&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 米国中南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PowerShell&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;N/A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;00:11:11&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 東日本&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure ブラジル南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Copy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;N/A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;00.25:33&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;イメージ　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 東日本&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure ブラジル南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PowerShell&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;N/A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;00.09:20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;考察&#34;&gt;考察&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;アクセス回線の差が大きく影響&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;自宅パターンでプロバイダから帯域制限されていたかは不明 (自宅からAzure東日本まで16Mbpsくらいは出た)&lt;/li&gt;
&lt;li&gt;アクセス回線が細い場合はユーザー拠点から「まとめて」送らないほうがいい&lt;/li&gt;
&lt;li&gt;こまめに送る&lt;/li&gt;
&lt;li&gt;Azure内でデータを生成する&lt;/li&gt;
&lt;li&gt;もしくはExpressRouteを引く (自宅で、とは言っていない)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;アクセス回線が細い場合、AzCopy Uploadの並列数を下げる&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AzCopyのデフォルト並列数は実行環境のCPUコア数 *8だが、今回実施した端末での並列数(4コア * 8 = 32)ではかえって性能が劣化した&lt;/li&gt;
&lt;li&gt;アクセス回線に合わせて並列数は調整する&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Azureのリージョン間コピーは早い&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Azureバックボーンを通るから&lt;/li&gt;
&lt;li&gt;端末よりAzureストレージのほうがリソース的に強いし負荷分散しているから&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;地理的な距離感覚だけで考えてはダメ&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;地理的な近さではなく、ネットワーク的な近さと太さ&lt;/li&gt;
&lt;li&gt;Azureバックボーンを使うと日本とブラジルの間でもそれなりのスループット&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ファイル数が多いときはイメージで送るのも手&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ファイル数がコピー時間に影響する (1 vs 6160)&lt;/li&gt;
&lt;li&gt;そもそもアプリがBlobとして使うのか、ファイルシステムとして使うかにもよるが&amp;hellip;&lt;/li&gt;
&lt;li&gt;もしファイルシステムとして、であれば有効な手段&lt;/li&gt;
&lt;li&gt;エクスポートのひと手間は考慮&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Azureバックボーンを使うと、意外にブラジル近い&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;土管か(ない&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Azureバックボーンの帯域にはSLAがありませんが、意識して仕組みを作ると得をします。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Azureユーザー視点のLatency測定 2017/4版</title>
      <link>http://torumakabe.github.io/post/azure_latency/</link>
      <pubDate>Sun, 09 Apr 2017 15:15:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/azure_latency/</guid>
      <description>

&lt;h2 id=&#34;関東の片隅で遅延を測る&#34;&gt;関東の片隅で遅延を測る&lt;/h2&gt;

&lt;p&gt;Twitterで「東阪の遅延って最近どのくらい？」と話題になっていたので。首都圏のAzureユーザー視線で測定しようと思います。&lt;/p&gt;

&lt;p&gt;せっかくなので、&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;太平洋のそれも測定しましょう&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/blog/how-microsoft-builds-its-fast-and-reliable-global-network/&#34;&gt;Azureバックボーンを通るリージョン間通信&lt;/a&gt;も測りましょう&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;計測パターン&#34;&gt;計測パターン&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;自宅(神奈川) -&amp;gt; OCN光 -&amp;gt; インターネット -&amp;gt; Azure東日本リージョン&lt;/li&gt;
&lt;li&gt;自宅(神奈川) -&amp;gt; OCN光 -&amp;gt; インターネット -&amp;gt; Azure西日本リージョン&lt;/li&gt;
&lt;li&gt;自宅(神奈川) -&amp;gt; OCN光 -&amp;gt; インターネット -&amp;gt; Azure米国西海岸リージョン&lt;/li&gt;
&lt;li&gt;Azure東日本リージョン -&amp;gt; Azureバックボーン -&amp;gt; Azure西日本リージョン&lt;/li&gt;
&lt;li&gt;Azure東日本リージョン -&amp;gt; Azureバックボーン -&amp;gt; Azure米国西海岸リージョン&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;もろもろの条件&#34;&gt;もろもろの条件&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;遅延測定ツール

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://technet.microsoft.com/en-us/sysinternals/psping.aspx&#34;&gt;PsPing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Azure各リージョンにD1_v2/Windows Server 2016仮想マシンを作成しPsPing&lt;/li&gt;
&lt;li&gt;NSGでデフォルト許可されているRDPポートへのPsPing&lt;/li&gt;
&lt;li&gt;VPN接続せず、パブリックIPへPsPing&lt;/li&gt;
&lt;li&gt;リージョン間PsPingは仮想マシンから仮想マシンへ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;自宅Wi-Fi環境

&lt;ul&gt;
&lt;li&gt;802.11ac(5GHz)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;自宅加入インターネット接続サービス

&lt;ul&gt;
&lt;li&gt;OCN 光 マンション 100M&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;OCNゲートウェイ

&lt;ul&gt;
&lt;li&gt;(ほげほげ)hodogaya.kanagawa.ocn.ne.jp&lt;/li&gt;
&lt;li&gt;神奈川県横浜市保土ケ谷区の局舎からインターネットに出ているようです&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;米国リージョン

&lt;ul&gt;
&lt;li&gt;US WEST (カリフォルニア)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;測定結果&#34;&gt;測定結果&lt;/h2&gt;

&lt;h3 id=&#34;1-自宅-神奈川-ocn光-インターネット-azure東日本リージョン&#34;&gt;1. 自宅(神奈川) -&amp;gt; OCN光 -&amp;gt; インターネット -&amp;gt; Azure東日本リージョン&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;TCP connect statistics for 104.41.187.55:3389:
  Sent = 4, Received = 4, Lost = 0 (0% loss),
  Minimum = 11.43ms, Maximum = 15.66ms, Average = 12.88ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-自宅-神奈川-ocn光-インターネット-azure西日本リージョン&#34;&gt;2. 自宅(神奈川) -&amp;gt; OCN光 -&amp;gt; インターネット -&amp;gt; Azure西日本リージョン&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;TCP connect statistics for 52.175.148.28:3389:
  Sent = 4, Received = 4, Lost = 0 (0% loss),
  Minimum = 17.96ms, Maximum = 19.64ms, Average = 18.92ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-自宅-神奈川-ocn光-インターネット-azure米国西海岸リージョン&#34;&gt;3. 自宅(神奈川) -&amp;gt; OCN光 -&amp;gt; インターネット -&amp;gt; Azure米国西海岸リージョン&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;TCP connect statistics for 40.83.220.19:3389:
  Sent = 4, Received = 4, Lost = 0 (0% loss),
  Minimum = 137.73ms, Maximum = 422.56ms, Average = 218.85ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-azure東日本リージョン-azureバックボーン-azure西日本リージョン&#34;&gt;4. Azure東日本リージョン -&amp;gt; Azureバックボーン -&amp;gt; Azure西日本リージョン&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;TCP connect statistics for 52.175.148.28:3389:
  Sent = 4, Received = 4, Lost = 0 (0% loss),
  Minimum = 8.61ms, Maximum = 9.38ms, Average = 9.00ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-azure東日本リージョン-azureバックボーン-azure米国西海岸リージョン&#34;&gt;5. Azure東日本リージョン -&amp;gt; Azureバックボーン -&amp;gt; Azure米国西海岸リージョン&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;TCP connect statistics for 40.83.220.19:3389:
  Sent = 4, Received = 4, Lost = 0 (0% loss),
  Minimum = 106.38ms, Maximum = 107.38ms, Average = 106.65ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Azureバックボーンを通すと首都圏からの遅延が半分になりました。Wi-Fiの有無など、ちょっと条件は違いますが。&lt;/p&gt;

&lt;h2 id=&#34;ひとこと&#34;&gt;ひとこと&lt;/h2&gt;

&lt;p&gt;インターネット、および接続サービスの遅延が性能の上がらない原因になっている場合は、Azureで完結させてみるのも手です。&lt;/p&gt;

&lt;p&gt;たとえば、&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;会社で契約しているインターネット接続サービスが、貧弱&lt;/li&gt;
&lt;li&gt;シリコンバレーの研究所からインターネット経由でデータを取得しているが、遅い&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;こんなケースではAzureを間に入れると、幸せになれるかもしれません。なったユーザーもいらっしゃいます。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Azure Resource Manager テンプレートでManaged Diskを作るときのコツ</title>
      <link>http://torumakabe.github.io/post/arm_template_managed_disk/</link>
      <pubDate>Thu, 23 Mar 2017 15:00:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/arm_template_managed_disk/</guid>
      <description>

&lt;h2 id=&#34;お伝えしたいこと&#34;&gt;お伝えしたいこと&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;ARMテンプレートのドキュメントが使いやすくなった&lt;/li&gt;
&lt;li&gt;Visual Studio CodeとAzure Resource Manager Toolsを使おう&lt;/li&gt;
&lt;li&gt;ARMテンプレートでManaged Diskを作る時のコツ&lt;/li&gt;
&lt;li&gt;可用性セットを意識しよう&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;armテンプレートのドキュメントが使いやすくなった&#34;&gt;ARMテンプレートのドキュメントが使いやすくなった&lt;/h2&gt;

&lt;p&gt;docs.microsoft.com の整備にともない、ARMテンプレートのドキュメントも&lt;a href=&#34;https://azure.microsoft.com/ja-jp/blog/azure-resource-manager-template-reference-now-available/&#34;&gt;使いやすくなりました&lt;/a&gt;。ARMテンプレート使いのみなさまは &lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/templates/&#34;&gt;https://docs.microsoft.com/ja-jp/azure/templates/&lt;/a&gt; をブックマークして、サクサク調べちゃってください。&lt;/p&gt;

&lt;h2 id=&#34;visual-studio-codeとazure-resource-manager-toolsを使おう&#34;&gt;Visual Studio CodeとAzure Resource Manager Toolsを使おう&lt;/h2&gt;

&lt;p&gt;これがあまり知られてないようなのでアピールしておきます。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docs.microsoft.com/ja-jp/azure/azure-resource-manager/media/resource-manager-create-first-template/vs-code-show-values.png&#34; alt=&#34;コードアシスト&#34; title=&#34;コードアシスト&#34; /&gt;&lt;/p&gt;

&lt;p&gt;コードアシストしてくれます。&lt;/p&gt;

&lt;p&gt;画面スクロールが必要なほどのJSONをフリーハンドで書けるほど人類は進化していないというのがわたしの見解です。ぜひご活用ください。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/azure-resource-manager/resource-manager-create-first-template?toc=%2fazure%2ftemplates%2ftoc.json&amp;amp;bc=%2Fazure%2Ftemplates%2Fbreadcrumb%2Ftoc.json#get-vs-code-and-extension&#34;&gt;Get VS Code and extension&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;armテンプレートでmanaged-diskを作る時のコツ&#34;&gt;ARMテンプレートでManaged Diskを作る時のコツ&lt;/h2&gt;

&lt;p&gt;Managed Diskが使えるようになって、ARMテンプレートでもストレージアカウントの定義を省略できるようになりました。Managed Diskの実体は内部的にAzureが管理するストレージアカウントに置かれるのですが、ユーザーからは隠蔽されます。&lt;/p&gt;

&lt;p&gt;Managed Diskは &lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/templates/microsoft.compute/disks&#34;&gt;Microsoft.Compute/disks&lt;/a&gt;  で個別に定義できますが、省略もできます。&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/templates/microsoft.compute/virtualmachines&#34;&gt;Microsoft.Compute/virtualMachines&lt;/a&gt; の中に書いてしまうやり口です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;osDisk&amp;quot;: {
  &amp;quot;name&amp;quot;: &amp;quot;[concat(variables(&#39;vmName&#39;),&#39;-md-os&#39;)]&amp;quot;,
  &amp;quot;createOption&amp;quot;: &amp;quot;FromImage&amp;quot;,
  &amp;quot;managedDisk&amp;quot;: {
    &amp;quot;storageAccountType&amp;quot;: &amp;quot;Standard_LRS&amp;quot;
  },
  &amp;quot;diskSizeGB&amp;quot;: 128
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;こんな感じで書けます。ポイントはサイズ指定 &amp;ldquo;diskSizeGB&amp;rdquo; の位置です。&amp;rdquo;managedDisk&amp;rdquo;の下ではありません。おじさんちょっと悩みました。&lt;/p&gt;

&lt;h2 id=&#34;可用性セットを意識しよう&#34;&gt;可用性セットを意識しよう&lt;/h2&gt;

&lt;p&gt;Managed Diskを使う利点のひとつが、可用性セットを意識したディスク配置です。可用性セットに仮想マシンを配置し、かつManaged Diskを使うと、可用性を高めることができます。&lt;/p&gt;

&lt;p&gt;Azureのストレージサービスは、多数のサーバーで構成された分散ストレージで実現されています。そのサーバー群をStorage Unitと呼びます。StampとかClusterと表現されることもあります。Storage Unitは数十のサーバーラック、数百サーバーで構成され、Azureの各リージョンに複数配置されます。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://download.microsoft.com/download/C/0/2/C02C4D26-0472-4688-AC13-199EA321135E/23rdACM_SOSP_WindowsAzureStorage_201110_jpn.pdf&#34;&gt;参考情報:Windows Azure ストレージ: 高可用性と強い一貫性を両立する クラウド ストレージ サービス(PDF)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;可用性セットは、電源とネットワークを共有するグループである&amp;rdquo;障害ドメイン(FD: Fault Domain)&amp;ldquo;を意識して仮想マシンを分散配置する設定です。そして、可用性セットに配置した仮想マシンに割り当てたManaged Diskは、Storage Unitを分散するように配置されます。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://msdnshared.blob.core.windows.net/media/2017/03/92.jpg&#34; alt=&#34;Unmanaged vs Managed&#34; title=&#34;Unmanaged vs Managed&#34; /&gt;&lt;/p&gt;

&lt;p&gt;すなわち、Storage Unitの障害に耐えることができます。Storage Unitは非常に可用性高く設計されており、長期に運用されてきた実績もあるのですが、ダウンする可能性はゼロではありません。可用性セットとManaged Diskの組み合わせは、可用性を追求したいシステムでは、おすすめです。&lt;/p&gt;

&lt;p&gt;さて、この場合の可用性セット定義ですが、以下のように書きます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;type&amp;quot;: &amp;quot;Microsoft.Compute/availabilitySets&amp;quot;,
  &amp;quot;name&amp;quot;: &amp;quot;AvSet01&amp;quot;,
  &amp;quot;apiVersion&amp;quot;: &amp;quot;2016-04-30-preview&amp;quot;,
  &amp;quot;location&amp;quot;: &amp;quot;[resourceGroup().location]&amp;quot;,
  &amp;quot;properties&amp;quot;: {
    &amp;quot;managed&amp;quot;: true,
    &amp;quot;platformFaultDomainCount&amp;quot;: 2,
    &amp;quot;platformUpdateDomainCount&amp;quot;: 5
  }
},
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/templates/microsoft.compute/availabilitysets&#34;&gt;Microsoft.Compute/availabilitySets&lt;/a&gt; を読むと、Managed Diskを使う場合は&amp;rdquo;propaties&amp;rdquo;の&amp;rdquo;managed&amp;rdquo;をtrueにすべし、とあります。なるほど。&lt;/p&gt;

&lt;p&gt;そしてポイントです。合わせて&amp;rdquo;platformFaultDomainCount&amp;rdquo;を指定してください。managedにする場合は必須パラメータです。&lt;/p&gt;

&lt;p&gt;なお、リージョンによって配備されているStorage Unit数には違いがあるのでご注意を。例えば東日本リージョンは2です。3のリージョンもあります。それに合わせて可用性セットの障害ドメイン数を指定します。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/storage/storage-faq-for-disks&#34;&gt;Azure IaaS VM ディスクと Premium 管理ディスクおよび非管理ディスクについてよく寄せられる質問&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Managed Disks を使用する可用性セットでサポートされる障害ドメイン数はいくつですか?

Managed Disks を使用する可用性セットでサポートされる障害ドメイン数は 2 または 3 です。これは、配置されているリージョンによって異なります。
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Docker for WindowsでインストールレスAzure CLI 2.0環境を作る</title>
      <link>http://torumakabe.github.io/post/dockerforwin_azurecli2/</link>
      <pubDate>Tue, 28 Feb 2017 08:00:30 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/dockerforwin_azurecli2/</guid>
      <description>

&lt;h2 id=&#34;azure-cli-2-0版です&#34;&gt;Azure CLI 2.0版です&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://torumakabe.github.io/post/dockerforwin_azurecli/&#34;&gt;Docker for WindowsでインストールレスAzure CLI環境を作る&lt;/a&gt;、のAzure CLI 2.0版です。Azure CLI 2.0の一般提供開始に合わせて書いています。&lt;/p&gt;

&lt;h2 id=&#34;動機&#34;&gt;動機&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Docker for Windows、もっと活用しようぜ&lt;/li&gt;
&lt;li&gt;がんがんアップデートされるAzure CLI2.0をいちいちインストールしたくない、コンテナ引っ張って以上、にしたい&lt;/li&gt;
&lt;li&gt;開発端末の環境を汚したくない、いつでもきれいに作り直せるようにしたい&lt;/li&gt;
&lt;li&gt;WindowsでPythonのバージョン管理するのつらくないですか? コンテナで解決しましょう&lt;/li&gt;
&lt;li&gt;○○レスって言ってみたかった&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;やり口&#34;&gt;やり口&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;もちろんDocker for Windows (on Client Hyper-V) を使う&lt;/li&gt;
&lt;li&gt;いちいちdocker run&amp;hellip;と打たなくていいよう、エイリアス的にPowerShellのfunction &amp;ldquo;az_cli&amp;rdquo; を作る&lt;/li&gt;
&lt;li&gt;&amp;ldquo;az_cli&amp;rdquo;入力にてAzure CLIコンテナを起動&lt;/li&gt;
&lt;li&gt;コンテナとホスト(Windows)間でファイル共有、ホスト側のIDEなりエディタを使えるようにする&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;作業の中身&#34;&gt;作業の中身&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Docker for Windowsを&lt;a href=&#34;https://docs.docker.com/docker-for-windows/install/&#34;&gt;インストール&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;64bit Windows 10 Pro/Enterprise/Education 1511以降に対応&lt;/li&gt;
&lt;li&gt;Hyper-Vの有効化を忘れずに&lt;/li&gt;
&lt;li&gt;Hyper-VとぶつかるVirtualBoxとはお別れです&lt;/li&gt;
&lt;li&gt;モードをLinuxにします。タスクトレイのdockerアイコンを右クリック [Switch to Linux containers]&lt;/li&gt;
&lt;li&gt;ドライブ共有をお忘れなく。 タスクトレイのdockerアイコンを右クリック [settings] &amp;gt; [Shared Drives]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;PowerShell functionを作成

&lt;ul&gt;
&lt;li&gt;のちほど詳しく&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;powershellのfunctionを作る&#34;&gt;PowerShellのfunctionを作る&lt;/h2&gt;

&lt;p&gt;ここが作業のハイライト。&lt;/p&gt;

&lt;p&gt;PowerShellのプロファイルを編集します。ところでエディタはなんでもいいのですが、AzureやDockerをがっつり触る人にはVS Codeがおすすめです。&lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=msazurermtools.azurerm-vscode-tools&#34;&gt;Azure Resource Manager Template&lt;/a&gt;や&lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=PeterJausovec.vscode-docker&#34;&gt;Docker&lt;/a&gt;むけextensionがあります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Workspace\ARM&amp;gt; code $profile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;こんなfunctionを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function az_cli {
   C:\PROGRA~1\Docker\Docker\Resources\bin\docker.exe run -it --rm -v ${HOME}/.azure:/root/.azure -v ${PWD}:/data -w /data azuresdk/azure-cli-python
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;エイリアスでなくfunctionにした理由は、引数です。エイリアスだと引数を渡せないので&lt;/li&gt;
&lt;li&gt;コンテナが溜まるのがいやなので、&amp;ndash;rmで都度消します&lt;/li&gt;
&lt;li&gt;毎度 az login しなくていいよう、トークンが保管されるコンテナの/root/azureディレクトリをホストの${HOME}/.azureと-v オプションで共有します&lt;/li&gt;
&lt;li&gt;ARM TemplateのJSONファイルなど、ホストからファイルを渡したいため、カレントディレクトリ ${PWD} をコンテナと -v オプションで共有します&lt;/li&gt;
&lt;li&gt;コンテナはdocker hubのazuresdk/azure-cli-pythonリポジトリ、latestを引っ張ります。latestで不具合あればバージョン指定してください&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ではテスト。まずはホスト側のファイルを確認。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Workspace\ARM&amp;gt; ls


    ディレクトリ: C:\Workspace\ARM


Mode                LastWriteTime         Length Name
----                -------------         ------ ----
-a----       2017/02/28      8:29           4515 azuredeploy.json
-a----       2017/02/28      8:30            374 azuredeploy.parameters.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;いくつかのファイルがあります。&lt;/p&gt;

&lt;p&gt;コンテナを起動してみましょう。az_cli functionを呼びます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Workspace\ARM&amp;gt; az_cli
bash-4.3#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;コンテナを起動し、入出力をつなぎました。ここからは頭と手をLinuxに切り替えてください。Azure CLI 2.0コンテナは&lt;a href=&#34;https://hub.docker.com/r/azuresdk/azure-cli-python/~/dockerfile/&#34;&gt;alpine linux&lt;/a&gt;ベースです。&lt;/p&gt;

&lt;p&gt;カレントディレクトリを確認。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bash-4.3# pwd
/data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ファイル共有できているか確認。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bash-4.3# ls
azuredeploy.json             azuredeploy.parameters.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;できてますね。&lt;/p&gt;

&lt;p&gt;azコマンドが打てるか確認。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bash-4.3# az --version
azure-cli (2.0.0+dev)

acr (0.1.1b4+dev)
acs (2.0.0+dev)
appservice (0.1.1b5+dev)
batch (0.1.1b4+dev)
cloud (2.0.0+dev)
component (2.0.0+dev)
configure (2.0.0+dev)
container (0.1.1b4+dev)
core (2.0.0+dev)
documentdb (0.1.1b2+dev)
feedback (2.0.0+dev)
iot (0.1.1b3+dev)
keyvault (0.1.1b5+dev)
network (2.0.0+dev)
nspkg (2.0.0+dev)
profile (2.0.0+dev)
redis (0.1.1b3+dev)
resource (2.0.0+dev)
role (2.0.0+dev)
sql (0.1.1b5+dev)
storage (2.0.0+dev)
taskhelp (0.1.1b3+dev)
vm (2.0.0+dev)

Python (Linux) 3.5.2 (default, Dec 27 2016, 21:33:11)
[GCC 5.3.0]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;タブで補完も効きます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bash-4.3# az a
account     acr         acs         ad          appservice
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;しあわせ。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Azure N-SeriesでPaintsChainerを動かす</title>
      <link>http://torumakabe.github.io/post/paintschainer_on_azure/</link>
      <pubDate>Fri, 03 Feb 2017 18:00:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/paintschainer_on_azure/</guid>
      <description>

&lt;h2 id=&#34;paintschainer面白い&#34;&gt;PaintsChainer面白い&lt;/h2&gt;

&lt;p&gt;クラスメソッドさんのDevelopers.IOでのエントリ&lt;a href=&#34;http://dev.classmethod.jp/cloud/paintschainer-on-ec2/&#34;&gt;&amp;ldquo;PaintsChainerをAmazon EC2で動かしてみた&amp;rdquo;&lt;/a&gt;が、とても面白いです。&lt;/p&gt;

&lt;p&gt;畳みこみニューラルネットワークを駆使して白黒線画に色付けしちゃうPaintsChainerすごい。EC2のGPUインスタンスでさくっと動かせるのもいいですね。&lt;/p&gt;

&lt;p&gt;せっかくなのでAzureでもやってみようと思います。AzurerはN-Series &amp;amp; NVIDIA-Dockerのサンプルとして、Azurerでない人はUbuntuでPaintsChainerを動かす参考手順として見ていただいてもいいかと。&lt;/p&gt;

&lt;h2 id=&#34;試した環境&#34;&gt;試した環境&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;米国中南部リージョン&lt;/li&gt;
&lt;li&gt;Standard NC6 (6 コア、56 GB メモリ、NVIDIA Tesla K80)&lt;/li&gt;
&lt;li&gt;Ubuntu 16.04&lt;/li&gt;
&lt;li&gt;NSGはSSH(22)の他にHTTP(80)を受信許可&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;導入手順&#34;&gt;導入手順&lt;/h2&gt;

&lt;h3 id=&#34;nvidia-tesla-driversのインストール&#34;&gt;NVIDIA Tesla driversのインストール&lt;/h3&gt;

&lt;p&gt;マイクロソフト公式ドキュメントの通りに導入します。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/virtual-machines/virtual-machines-linux-n-series-driver-setup&#34;&gt;Set up GPU drivers for N-series VMs&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;dockerのインストール&#34;&gt;Dockerのインストール&lt;/h3&gt;

&lt;p&gt;Docker公式ドキュメントの通りに導入します。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/engine/installation/linux/ubuntu/&#34;&gt;Get Docker for Ubuntu&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;nvidia-dockerのインストール&#34;&gt;NVIDIA Dockerのインストール&lt;/h3&gt;

&lt;p&gt;GitHub上のNVIDIAのドキュメント通りに導入します。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/NVIDIA/nvidia-docker&#34;&gt;NVIDIA Docker&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ここまでの作業に問題がないか、確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo nvidia-docker run --rm nvidia/cuda nvidia-smi
Using default tag: latest
latest: Pulling from nvidia/cuda
8aec416115fd: Pull complete
[...]
Status: Downloaded newer image for nvidia/cuda:latest
Fri Feb  3 06:43:18 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 86BF:00:00.0     Off |                    0 |
| N/A   34C    P8    33W / 149W |      0MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;paintschainer-dockerのインストール&#34;&gt;PaintsChainer-Dockerのインストール&lt;/h3&gt;

&lt;p&gt;Liam Jones氏が公開している&lt;a href=&#34;https://github.com/liamjones/PaintsChainer-Docker&#34;&gt;PaintsChainer-Docker&lt;/a&gt;を使って、PaintsChanierコンテナーを起動します。ポートマッピングはコンテナーホストの80番とコンテナーの8000番です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo nvidia-docker run -p 80:8000 liamjones/paintschainer-docker
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;paintschainerを使ってみる&#34;&gt;PaintsChainerを使ってみる&lt;/h2&gt;

&lt;p&gt;VMのパブリックIP、ポート80番にアクセスすると、先ほどコンテナーで起動したPaintsChainerのページが開きます。クラウディアさんの白黒画像ファイルで試してみましょう。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ToruMakabe/Images/master/paintschainer_cloudia.png&#34; alt=&#34;結果&#34; title=&#34;Cloudia&#34; /&gt;&lt;/p&gt;

&lt;p&gt;PaintsChainer、すごいなぁ。
クラウディアさん、おなか寒そうだけど。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Azure App Service on LinuxのコンテナをCLIで更新する方法</title>
      <link>http://torumakabe.github.io/post/azure_webapponlinux_dockertag/</link>
      <pubDate>Sun, 20 Nov 2016 13:00:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/azure_webapponlinux_dockertag/</guid>
      <description>

&lt;h2 id=&#34;cliでコンテナを更新したい&#34;&gt;CLIでコンテナを更新したい&lt;/h2&gt;

&lt;p&gt;Connect(); 2016にあわせ、Azure App Service on Linuxのコンテナ対応が&lt;a href=&#34;https://azure.microsoft.com/en-us/blog/app-service-on-linux-now-supports-containers-and-asp-net-core/&#34;&gt;発表&lt;/a&gt;されました。Azure Container Serviceほどタップリマシマシな環境ではなく、サクッと楽してコンテナを使いたい人にオススメです。&lt;/p&gt;

&lt;p&gt;さっそくデプロイの自動化どうすっかな、と検討している人もちらほらいらっしゃるようです。CI/CD側でビルド、テストしたコンテナをAPIなりCLIでApp Serviceにデプロイするやり口、どうしましょうか。&lt;/p&gt;

&lt;p&gt;まだプレビューなのでAzureも、VSTSなどCI/CD側も機能追加が今後あると思いますし、使い方がこなれてベストプラクティスが生まれるとは思いますが、アーリーアダプターなあなた向けに、現時点でできることを書いておきます。&lt;/p&gt;

&lt;h2 id=&#34;azure-cli-2-0&#34;&gt;Azure CLI 2.0&lt;/h2&gt;

&lt;p&gt;Azure CLI 2.0に&amp;rdquo;appservice web config container&amp;rdquo;コマンドがあります。これでコンテナイメージを更新できます。&lt;/p&gt;

&lt;p&gt;すでにyourrepoレポジトリのyourcontainerコンテナ、タグ1.0.0がデプロイされているとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az appservice web config container show -n yourcontainerapp -g YourRG
{
  &amp;quot;DOCKER_CUSTOM_IMAGE_NAME&amp;quot;: &amp;quot;yourrepo/yourcontainer:1.0.0&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;新ビルドのタグ1.0.1をデプロイするには、update -c オプションを使います。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az appservice web config container update -n yourcontainerapp -g YourRG -c &amp;quot;yourrepo/yourcontainer:1.0.1&amp;quot;
{
  &amp;quot;DOCKER_CUSTOM_IMAGE_NAME&amp;quot;: &amp;quot;yourrepo/yourcontainer:1.0.1&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これで更新されます。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>