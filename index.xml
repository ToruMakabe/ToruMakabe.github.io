<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>re-imagine</title>
    <link>http://torumakabe.github.io/index.xml</link>
    <description>Recent content on re-imagine</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Fri, 30 Mar 2018 16:30:00 +0900</lastBuildDate>
    <atom:link href="http://torumakabe.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Azure MarketplaceからMSI対応でセキュアなTerraform環境を整える</title>
      <link>http://torumakabe.github.io/post/azure_msi_terraform/</link>
      <pubDate>Fri, 30 Mar 2018 16:30:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/azure_msi_terraform/</guid>
      <description>

&lt;h2 id=&#34;terraformのプロビジョニングがmarketplaceから可能に&#34;&gt;TerraformのプロビジョニングがMarketplaceから可能に&lt;/h2&gt;

&lt;p&gt;Terraform使ってますか。Azureのリソースプロビジョニングの基本はAzure Resource Manager Template Deployである、がわたしの持論ですが、Terraformを使う/併用する方がいいな、というケースは結構あります。使い分けは&lt;a href=&#34;https://www.slideshare.net/ToruMakabe/azure-infrastructure-as-code&#34;&gt;この資料&lt;/a&gt;も参考に。&lt;/p&gt;

&lt;p&gt;さて、先日Azure Marketplaceから&lt;a href=&#34;https://azuremarketplace.microsoft.com/en-us/marketplace/apps/azure-oss.terraform&#34;&gt;Terraform入りの仮想マシン&lt;/a&gt;をプロビジョニングできるようになりました。Ubuntuに以下のアプリが導入、構成されます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Terraform (latest)&lt;/li&gt;
&lt;li&gt;Azure CLI 2.0&lt;/li&gt;
&lt;li&gt;Managed Service Identity (MSI) VM Extension&lt;/li&gt;
&lt;li&gt;Unzip&lt;/li&gt;
&lt;li&gt;JQ&lt;/li&gt;
&lt;li&gt;apt-transport-https&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;いろいろセットアップしてくれるのでしみじみ便利なのですが、ポイントはManaged Service Identity (MSI)です。&lt;/p&gt;

&lt;h2 id=&#34;シークレットをコードにベタ書きする問題&#34;&gt;シークレットをコードにベタ書きする問題&lt;/h2&gt;

&lt;p&gt;MSIの何がうれしいいのでしょう。分かりやすい例を挙げると「GitHubにシークレットを書いたコードをpushする、お漏らし事案」を避ける仕組みです。もちそんそれだけではありませんが。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/active-directory/managed-service-identity/overview&#34;&gt;Azure リソースの管理対象サービス ID (MSI)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;詳細の説明は公式ドキュメントに譲りますが、ざっくり説明すると&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;アプリに認証・認可用のシークレットを書かなくても、アプリの動く仮想マシン上にあるローカルエンドポイントにアクセスすると、Azureのサービスを使うためのトークンが得られるよ&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;です。&lt;/p&gt;

&lt;p&gt;GitHub上に疑わしいシークレットがないかスキャンする&lt;a href=&#34;https://azure.microsoft.com/ja-jp/blog/managing-azure-secrets-on-github-repositories/&#34;&gt;取り組み&lt;/a&gt;もはじまっているのですが、できればお世話になりなくない。MSIを活用しましょう。&lt;/p&gt;

&lt;h2 id=&#34;terraformはmsiに対応している&#34;&gt;TerraformはMSIに対応している&lt;/h2&gt;

&lt;p&gt;TerraformでAzureのリソースをプロビジョニングするには、もちろん認証・認可が必要です。従来はサービスプリンシパルを作成し、そのIDやシークレットをTerraformの実行環境に配布していました。でも、できれば配布したくないですよね。実行環境を特定の仮想マシンに限定し、MSIを使えば、解決できます。&lt;/p&gt;

&lt;p&gt;ところでMSIを使うには、ローカルエンドポイントにトークンを取りに行くよう、アプリを作らなければいけません。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.terraform.io/docs/providers/azurerm/authenticating_via_msi.html&#34;&gt;Authenticating to Azure Resource Manager using Managed Service Identity&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Terraformは対応済みです。環境変数 ARM_USE_MSI をtrueにしてTerraformを実行すればOK。&lt;/p&gt;

&lt;h2 id=&#34;試してみよう&#34;&gt;試してみよう&lt;/h2&gt;

&lt;p&gt;実は、すでに使い方を解説した公式ドキュメントがあります。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/terraform/terraform-vm-msi&#34;&gt;Azure Marketplace イメージを使用して管理対象サービス ID を使用する Terraform Linux 仮想マシンを作成する&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;手順は十分なのですが、理解を深めるための補足情報が、もうちょっと欲しいところです。なので補ってみましょう。&lt;/p&gt;

&lt;h3 id=&#34;marketplaceからterraform入り仮想マシンを作る&#34;&gt;MarketplaceからTerraform入り仮想マシンを作る&lt;/h3&gt;

&lt;p&gt;まず、Marketplaceからのデプロイでどんな仮想マシンが作られたのか、気になります。デプロイに利用されたテンプレートをのぞいてみましょう。注目は以下3つのリソースです。抜き出します。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;MSI VM拡張の導入&lt;/li&gt;
&lt;li&gt;VMに対してリソースグループスコープでContributorロールを割り当て&lt;/li&gt;
&lt;li&gt;スクリプト実行 VM拡張でTerraform関連のプロビジョニング&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;[snip]
        {
            &amp;quot;type&amp;quot;: &amp;quot;Microsoft.Compute/virtualMachines/extensions&amp;quot;,
            &amp;quot;name&amp;quot;: &amp;quot;[concat(parameters(&#39;vmName&#39;),&#39;/MSILinuxExtension&#39;)]&amp;quot;,
            &amp;quot;apiVersion&amp;quot;: &amp;quot;2017-12-01&amp;quot;,
            &amp;quot;location&amp;quot;: &amp;quot;[parameters(&#39;location&#39;)]&amp;quot;,
            &amp;quot;properties&amp;quot;: {
                &amp;quot;publisher&amp;quot;: &amp;quot;Microsoft.ManagedIdentity&amp;quot;,
                &amp;quot;type&amp;quot;: &amp;quot;ManagedIdentityExtensionForLinux&amp;quot;,
                &amp;quot;typeHandlerVersion&amp;quot;: &amp;quot;1.0&amp;quot;,
                &amp;quot;autoUpgradeMinorVersion&amp;quot;: true,
                &amp;quot;settings&amp;quot;: {
                    &amp;quot;port&amp;quot;: 50342
                },
                &amp;quot;protectedSettings&amp;quot;: {}
            },
            &amp;quot;dependsOn&amp;quot;: [
                &amp;quot;[concat(&#39;Microsoft.Compute/virtualMachines/&#39;, parameters(&#39;vmName&#39;))]&amp;quot;
            ]
        },
        {
            &amp;quot;type&amp;quot;: &amp;quot;Microsoft.Authorization/roleAssignments&amp;quot;,
            &amp;quot;name&amp;quot;: &amp;quot;[variables(&#39;resourceGuid&#39;)]&amp;quot;,
            &amp;quot;apiVersion&amp;quot;: &amp;quot;2017-09-01&amp;quot;,
            &amp;quot;properties&amp;quot;: {
                &amp;quot;roleDefinitionId&amp;quot;: &amp;quot;[variables(&#39;contributor&#39;)]&amp;quot;,
                &amp;quot;principalId&amp;quot;: &amp;quot;[reference(concat(resourceId(&#39;Microsoft.Compute/virtualMachines/&#39;, parameters(&#39;vmName&#39;)),&#39;/providers/Microsoft.ManagedIdentity/Identities/default&#39;),&#39;2015-08-31-PREVIEW&#39;).principalId]&amp;quot;,
                &amp;quot;scope&amp;quot;: &amp;quot;[concat(&#39;/subscriptions/&#39;, subscription().subscriptionId, &#39;/resourceGroups/&#39;, resourceGroup().name)]&amp;quot;
            },
            &amp;quot;dependsOn&amp;quot;: [
                &amp;quot;[resourceId(&#39;Microsoft.Compute/virtualMachines/extensions/&#39;, parameters(&#39;vmName&#39;),&#39;MSILinuxExtension&#39;)]&amp;quot;
            ]
        },
        {
            &amp;quot;type&amp;quot;: &amp;quot;Microsoft.Compute/virtualMachines/extensions&amp;quot;,
            &amp;quot;name&amp;quot;: &amp;quot;[concat(parameters(&#39;vmName&#39;),&#39;/customscriptextension&#39;)]&amp;quot;,
            &amp;quot;apiVersion&amp;quot;: &amp;quot;2017-03-30&amp;quot;,
            &amp;quot;location&amp;quot;: &amp;quot;[parameters(&#39;location&#39;)]&amp;quot;,
            &amp;quot;properties&amp;quot;: {
                &amp;quot;publisher&amp;quot;: &amp;quot;Microsoft.Azure.Extensions&amp;quot;,
                &amp;quot;type&amp;quot;: &amp;quot;CustomScript&amp;quot;,
                &amp;quot;typeHandlerVersion&amp;quot;: &amp;quot;2.0&amp;quot;,
                &amp;quot;autoUpgradeMinorVersion&amp;quot;: true,
                &amp;quot;settings&amp;quot;: {
                    &amp;quot;fileUris&amp;quot;: [
                        &amp;quot;[concat(parameters(&#39;artifactsLocation&#39;), &#39;/scripts/infra.sh&#39;, parameters(&#39;artifactsLocationSasToken&#39;))]&amp;quot;,
                        &amp;quot;[concat(parameters(&#39;artifactsLocation&#39;), &#39;/scripts/install.sh&#39;, parameters(&#39;artifactsLocationSasToken&#39;))]&amp;quot;,
                        &amp;quot;[concat(parameters(&#39;artifactsLocation&#39;), &#39;/scripts/azureProviderAndCreds.tf&#39;, parameters(&#39;artifactsLocationSasToken&#39;))]&amp;quot;
                    ]
                },
                &amp;quot;protectedSettings&amp;quot;: {
                    &amp;quot;commandToExecute&amp;quot;: &amp;quot;[concat(&#39;bash infra.sh &amp;amp;&amp;amp; bash install.sh &#39;, variables(&#39;installParm1&#39;), variables(&#39;installParm2&#39;), variables(&#39;installParm3&#39;), variables(&#39;installParm4&#39;), &#39; -k &#39;, listKeys(resourceId(&#39;Microsoft.Storage/storageAccounts&#39;, variables(&#39;stateStorageAccountName&#39;)), &#39;2017-10-01&#39;).keys[0].value, &#39; -l &#39;, reference(concat(resourceId(&#39;Microsoft.Compute/virtualMachines/&#39;, parameters(&#39;vmName&#39;)),&#39;/providers/Microsoft.ManagedIdentity/Identities/default&#39;),&#39;2015-08-31-PREVIEW&#39;).principalId)]&amp;quot;
                }
            },
            &amp;quot;dependsOn&amp;quot;: [
                &amp;quot;[resourceId(&#39;Microsoft.Authorization/roleAssignments&#39;, variables(&#39;resourceGuid&#39;))]&amp;quot;
            ]
        }
[snip]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;vmにログインし-環境を確認&#34;&gt;VMにログインし、環境を確認&lt;/h3&gt;

&lt;p&gt;では出来上がったVMにsshし、いろいろのぞいてみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh your-vm-public-ip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Terraformのバージョンは、現時点で最新の0.11.5が入っています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ terraform -v
Terraform v0.11.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;環境変数ARM_USE_MSIはtrueに設定されています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ echo $ARM_USE_MSI
true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;MSIも有効化されています(SystemAssigned)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az vm identity show -g tf-msi-poc-ejp-rg -n tfmsipocvm01
{
  &amp;quot;additionalProperties&amp;quot;: {},
  &amp;quot;identityIds&amp;quot;: null,
  &amp;quot;principalId&amp;quot;: &amp;quot;aaaa-aaaa-aaaa-aaaa-aaaa&amp;quot;,
  &amp;quot;tenantId&amp;quot;: &amp;quot;tttt-tttt-tttt-tttt&amp;quot;,
  &amp;quot;type&amp;quot;: &amp;quot;SystemAssigned&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;さて、このVMはMSIが使えるようになったわけですが、操作できるリソースのスコープは、このVMが属するリソースグループに限定されてます。新たなリソースグループを作成したい場合は、ロールを付与し、スコープを広げます。~/にtfEnv.shというスクリプトが用意されています。用意されたスクリプトを実行すると、サブスクリプションスコープのContributorがVMに割り当てられます。必要に応じて変更しましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls
tfEnv.sh  tfTemplate

$ cat tfEnv.sh
az login
az role assignment create  --assignee &amp;quot;aaaa-aaaa-aaaa-aaaa-aaaa&amp;quot; --role &#39;b24988ac-6180-42a0-ab88-20f7382dd24c&#39;  --scope /subscriptions/&amp;quot;cccc-cccc-cccc-cccc&amp;quot;

$ . ~/tfEnv.sh
To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code HOGEHOGE to authenticate.
[snip]
{
  &amp;quot;additionalProperties&amp;quot;: {},
  &amp;quot;canDelegate&amp;quot;: null,
  &amp;quot;id&amp;quot;: &amp;quot;/subscriptions/cccc-cccc-cccc-cccc/providers/Microsoft.Authorization/roleAssignments/ffff-ffff-ffff-ffff&amp;quot;,
  &amp;quot;name&amp;quot;: &amp;quot;ffff-ffff-ffff-ffff&amp;quot;,
  &amp;quot;principalId&amp;quot;: &amp;quot;aaaa-aaaa-aaaa-aaaa-aaaa&amp;quot;,
  &amp;quot;roleDefinitionId&amp;quot;: &amp;quot;/subscriptions/cccc-cccc-cccc-cccc/providers/Microsoft.Authorization/roleDefinitions/b24988ac-6180-42a0-ab88-20f7382dd24c&amp;quot;,
  &amp;quot;scope&amp;quot;: &amp;quot;/subscriptions/cccc-cccc-cccc-cccc&amp;quot;,
  &amp;quot;type&amp;quot;: &amp;quot;Microsoft.Authorization/roleAssignments&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ちなみに、role id &amp;ldquo;b24988ac-6180-42a0-ab88-20f7382dd24c&amp;rdquo;はConributorを指します。&lt;/p&gt;

&lt;p&gt;tfTemplateというディレクトリも用意されているようです。2つのファイルがあります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls tfTemplate/
azureProviderAndCreds.tf  remoteState.tf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;azureProviderAndCreds.tfは、tfファイルのテンプレートです。コメントアウトと説明のとおり、MSIを使う場合には、このテンプレートは必要ありません。subscription_idとtenant_idは、VMのプロビジョニング時に環境変数にセットされています。そしてclient_idとclient_secretは、MSIを通じて取得されます。明示的に変えたい時のみ指定しましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat tfTemplate/azureProviderAndCreds.tf
#
#
# Provider and credential snippet to add to configurations
# Assumes that there&#39;s a terraform.tfvars file with the var values
#
# Uncomment the creds variables if using service principal auth
# Leave them commented to use MSI auth
#
#variable subscription_id {}
#variable tenant_id {}
#variable client_id {}
#variable client_secret {}

provider &amp;quot;azurerm&amp;quot; {
#    subscription_id = &amp;quot;${var.subscription_id}&amp;quot;
#    tenant_id = &amp;quot;${var.tenant_id}&amp;quot;
#    client_id = &amp;quot;${var.client_id}&amp;quot;
#    client_secret = &amp;quot;${var.client_secret}&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;remoteState.tfは、TerraformのstateをAzureのBlob上に置く場合に使います。Blobの&lt;a href=&#34;https://azure.microsoft.com/en-us/blog/soft-delete-for-azure-storage-blobs-now-in-public-preview/&#34;&gt;soft delete&lt;/a&gt;が使えるようになったこともあり、事件や事故を考慮すると、できればstateはローカルではなくBlobで管理したいところです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat tfTemplate/remoteState.tf
terraform {
 backend &amp;quot;azurerm&amp;quot; {
  storage_account_name = &amp;quot;storestaterandomid&amp;quot;
  container_name       = &amp;quot;terraform-state&amp;quot;
  key                  = &amp;quot;prod.terraform.tfstate&amp;quot;
  access_key           = &amp;quot;KYkCz88z+7yoyoyoiyoyoyoiyoyoyoiyoiTDZRbrwAWIPWD+rU6g==&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;soft delete設定は、別途 &lt;a href=&#34;https://docs.microsoft.com/en-us/cli/azure/storage/blob/service-properties/delete-policy?view=azure-cli-latest#az-storage-blob-service-properties-delete-policy-update&#34;&gt;az storage blob service-properties delete-policy update&lt;/a&gt; コマンドで行ってください。&lt;/p&gt;

&lt;h3 id=&#34;プロビジョニングしてみる&#34;&gt;プロビジョニングしてみる&lt;/h3&gt;

&lt;p&gt;ではTerraformを動かしてみましょう。サブディレクトリsampleを作り、そこで作業します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir sample
$ cd sample/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;stateはBlobで管理しましょう。先ほどのremoteState.tfを実行ディレクトリにコピーします。アクセスキーが入っていますので、このディレクトリをコード管理システム配下に置くのであれば、.gitignoreなどで除外をお忘れなく。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cp ../tfTemplate/remoteState.tf ./
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここのキーが残ってしまうのが現時点での課題。ストレージのキー問題は&lt;a href=&#34;https://feedback.azure.com/forums/217298-storage/suggestions/14831712-allow-user-based-access-to-blob-containers-for-su&#34;&gt;対応がはじまったので&lt;/a&gt;、いずれ解決するはずです。&lt;/p&gt;

&lt;p&gt;ではTerraformで作るリソースを書きます。さくっとACI上にnginxコンテナーを作りましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vim main.tf
resource &amp;quot;azurerm_resource_group&amp;quot; &amp;quot;tf-msi-poc&amp;quot; {
    name     = &amp;quot;tf-msi-poc-aci-wus-rg&amp;quot;
    location = &amp;quot;West US&amp;quot;
}

resource &amp;quot;random_integer&amp;quot; &amp;quot;random_int&amp;quot; {
    min = 100
    max = 999
}

resource &amp;quot;azurerm_container_group&amp;quot; &amp;quot;aci-example&amp;quot; {
    name                = &amp;quot;aci-cg-${random_integer.random_int.result}&amp;quot;
    location            = &amp;quot;${azurerm_resource_group.tf-msi-poc.location}&amp;quot;
    resource_group_name = &amp;quot;${azurerm_resource_group.tf-msi-poc.name}&amp;quot;
    ip_address_type     = &amp;quot;public&amp;quot;
    dns_name_label      = &amp;quot;tomakabe-aci-cg-${random_integer.random_int.result}&amp;quot;
    os_type             = &amp;quot;linux&amp;quot;

    container {
        name    = &amp;quot;nginx&amp;quot;
        image   = &amp;quot;nginx&amp;quot;
        cpu     = &amp;quot;0.5&amp;quot;
        memory  = &amp;quot;1.0&amp;quot;
        port    = &amp;quot;80&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;init、plan、アプラーイ。アプライ王子。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ terraform init
$ terraform plan
$ terraform apply -auto-approve
[snip]
Apply complete! Resources: 3 added, 0 changed, 0 destroyed.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;できたか確認。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az container show -g tf-msi-poc-aci-wus-rg -n aci-cg-736 -o table
Name        ResourceGroup          ProvisioningState    Image    IP:ports         CPU/Memory       OsType    Location
----------  ---------------------  -------------------  -------  ---------------  ---------------  --------  ----------
aci-cg-736  tf-msi-poc-aci-wus-rg  Succeeded            nginx    13.91.90.117:80  0.5 core/1.0 gb  Linux     westus
$ curl 13.91.90.117
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
[snip]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;おまけ&#34;&gt;おまけ&lt;/h2&gt;

&lt;p&gt;サービスプリンシパルは、アプリに対して権限を付与するために必要な仕組みなのですが、使わなくなった際に消し忘れることが多いです。意識して消さないと、散らかり放題。&lt;/p&gt;

&lt;p&gt;MSIの場合、対象のVMを消すとそのプリンシパルも消えます。爽快感ほとばしる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az ad sp show --id aaaa-aaaa-aaaa-aaaa-aaaa
Resource &#39;aaaa-aaaa-aaaa-aaaa-aaaa&#39; does not exist or one of its queried reference-property objects are not present.
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Azure DNS Private Zonesの動きを確認する</title>
      <link>http://torumakabe.github.io/post/azure_private_dns_preview/</link>
      <pubDate>Tue, 27 Mar 2018 00:10:30 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/azure_private_dns_preview/</guid>
      <description>

&lt;h2 id=&#34;プライベートゾーンのパブリックプレビュー開始&#34;&gt;プライベートゾーンのパブリックプレビュー開始&lt;/h2&gt;

&lt;p&gt;Azure DNSのプライベートゾーン対応が、全リージョンでパブリックプレビューとなりました。ゾーンとプレビューのプライベートとパブリックが入り混じって、なにやら紛らわしいですが。&lt;/p&gt;

&lt;p&gt;さて、このプライベートゾーン対応ですが、名前のとおりAzure DNSをプライベートな仮想ネットワーク(VNet)で使えるようになります。加えて、しみじみと嬉しい便利機能がついています。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Split-Horizonに対応します。VNet内からの問い合わせにはプライベートゾーン、それ以外からはパブリックゾーンのレコードを返します。&lt;/li&gt;
&lt;li&gt;仮想マシンの作成時、プライベートゾーンへ自動でホスト名を追加します。&lt;/li&gt;
&lt;li&gt;プライベートゾーンとVNetをリンクして利用します。複数のVNetをリンクすることが可能です。&lt;/li&gt;
&lt;li&gt;リンクの種類として、仮想マシンホスト名の自動登録が行われるVNetをRegistration VNet、名前解決(正引き)のみ可能なResolution VNetがあります。&lt;/li&gt;
&lt;li&gt;プライベートゾーンあたり、Registration VNetの現時点の上限数は1、Resolution VNetは10です。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;公式ドキュメントは&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/dns/private-dns-overview&#34;&gt;こちら&lt;/a&gt;。現時点の&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/dns/private-dns-overview#limitations&#34;&gt;制約もまとまっている&lt;/a&gt;ので、目を通しておきましょう。&lt;/p&gt;

&lt;h2 id=&#34;動きを見てみよう&#34;&gt;動きを見てみよう&lt;/h2&gt;

&lt;p&gt;公式ドキュメントには&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/dns/private-dns-scenarios&#34;&gt;想定シナリオ&lt;/a&gt;があり、これを読めばできることがだいたい分かります。ですが、名前解決は呼吸のようなもの、体に叩き込みたいお気持ちです。手を動かして確認します。&lt;/p&gt;

&lt;h3 id=&#34;事前に準備する環境&#34;&gt;事前に準備する環境&lt;/h3&gt;

&lt;p&gt;下記リソースを先に作っておきます。手順は割愛。ドメイン名はexample.comとしましたが、適宜読み替えてください。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;VNet *2

&lt;ul&gt;
&lt;li&gt;vnet01&lt;/li&gt;
&lt;li&gt;subnet01

&lt;ul&gt;
&lt;li&gt;subnet01-nsg (allow ssh)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;vnet02&lt;/li&gt;
&lt;li&gt;subnet01

&lt;ul&gt;
&lt;li&gt;subnet01-nsg (allow ssh)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Azure DNS Public Zone

&lt;ul&gt;
&lt;li&gt;example.com&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;azure-cliへdns拡張を導入&#34;&gt;Azure CLIへDNS拡張を導入&lt;/h3&gt;

&lt;p&gt;プレビュー機能をCLIに導入します。いずれ要らなくなるかもしれませんので、要否は&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/dns/private-dns-getstarted-cli#to-installuse-azure-dns-private-zones-feature-public-preview&#34;&gt;公式ドキュメント&lt;/a&gt;で確認してください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az extension add --name dns
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;プライベートゾーンの作成&#34;&gt;プライベートゾーンの作成&lt;/h3&gt;

&lt;p&gt;既存のゾーンを確認します。パブリックゾーンがあります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az network dns zone list -o table
ZoneName      ResourceGroup             RecordSets    MaxRecordSets
------------  ----------------------  ------------  ---------------
example.com   common-global-rg                   2             5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;プライベートゾーンを作成します。Registration VNetとしてvnet01をリンクします。&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/dns/private-dns-overview#limitations&#34;&gt;現時点の制約&lt;/a&gt;で、リンク時にはVNet上にVMが無い状態にする必要があります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az network dns zone create -g private-dns-poc-ejp-rg -n example.com --zone-type Private --registration-vnets vnet01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同じ名前のゾーンが2つになりました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az network dns zone list -o table
ZoneName      ResourceGroup             RecordSets    MaxRecordSets
------------  ----------------------  ------------  ---------------
example.com   common-global-rg                   2             5000
example.com   private-dns-poc-ejp-rg             1             5000
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;registration-vnetへvmを作成&#34;&gt;Registration VNetへVMを作成&lt;/h3&gt;

&lt;p&gt;VMを2つ作ります。1つにはインターネット経由でsshするので、パブリックIPを割り当てます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ BASE_NAME=&amp;quot;private-dns-poc-ejp&amp;quot;
$ az network public-ip create -n vm01-pip -g ${BASE_NAME}-rg
$ az network nic create -g ${BASE_NAME}-rg -n vm01-nic --public-ip-address vm01-pip --vnet vnet01 --subnet subnet01
$ az vm create -g ${BASE_NAME}-rg -n vm01 --image Canonical:UbuntuServer:16.04.0-LTS:latest --size Standard_B1s --nics vm01-nic
$ az network nic create -g ${BASE_NAME}-rg -n vm02-nic --vnet vnet01 --subnet subnet01
$ az vm create -g ${BASE_NAME}-rg -n vm02 --image Canonical:UbuntuServer:16.04.0-LTS:latest --size Standard_B1s --nics vm02-nic
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;パブリックipをパブリックゾーンへ登録&#34;&gt;パブリックIPをパブリックゾーンへ登録&lt;/h3&gt;

&lt;p&gt;Split-Horizonの動きを確認したいので、パブリックIPをパブリックゾーンへ登録します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az network public-ip show -g private-dns-poc-ejp-rg -n vm01-pip --query ipAddress
&amp;quot;13.78.84.84&amp;quot;
$ az network dns record-set a add-record -g common-global-rg -z example.com -n vm01 -a 13.78.84.84
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;パブリックゾーンで名前解決できることを確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nslookup vm01.example.com
Server:         103.5.140.1
Address:        103.5.140.1#53

Non-authoritative answer:
Name:   vm01.example.com
Address: 13.78.84.84
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;registration-vnetの動きを確認&#34;&gt;Registration VNetの動きを確認&lt;/h3&gt;

&lt;p&gt;vnet01のvm01へ、パブリックIP経由でsshします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh vm01.example.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同じRegistration VNet上のvm02を正引きします。ドメイン名無し、ホスト名だけでnslookupすると、VNetの内部ドメイン名がSuffixになります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vm01:~$ nslookup vm02
Server:         168.63.129.16
Address:        168.63.129.16#53

Non-authoritative answer:
Name:   vm02.aioh0amlfdze5drhlpb1ktqwxd.lx.internal.cloudapp.net
Address: 10.0.0.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ドメイン名をつけてみましょう。Nameはvnet01にリンクしたプライベートゾーンのドメイン名になりました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vm01:~$ nslookup vm02.example.com
Server:         168.63.129.16
Address:        168.63.129.16#53

Non-authoritative answer:
Name:   vm02.example.com
Address: 10.0.0.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;逆引きもできます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vm01:~$ nslookup 10.0.0.5
Server:         168.63.129.16
Address:        168.63.129.16#53

Non-authoritative answer:
5.0.0.10.in-addr.arpa   name = vm02.example.com.

Authoritative answers can be found from:
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;split-horizonの動きを確認&#34;&gt;Split-Horizonの動きを確認&lt;/h3&gt;

&lt;p&gt;さて、いま作業をしているvm01には、インターネット経由でパブリックゾーンで名前解決してsshしたわけですが、プライベートなVNet内でnslookupするとどうなるでしょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vm01:~$ nslookup vm01.example.com
Server:         168.63.129.16
Address:        168.63.129.16#53

Non-authoritative answer:
Name:   vm01.example.com
Address: 10.0.0.4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;プライベートゾーンで解決されました。Split-Horizonが機能していることが分かります。&lt;/p&gt;

&lt;p&gt;あ、どうでもいいことですが、Split-Horizonって戦隊モノの必殺技みたいなネーミングですね。叫びながら地面に拳を叩きつけたい感じ。&lt;/p&gt;

&lt;h3 id=&#34;resolution-vnetの動きを確認&#34;&gt;Resolution VNetの動きを確認&lt;/h3&gt;

&lt;p&gt;vnet02を作成し、Resolution VNetとしてプライベートゾーンとリンクします。そして、vnet02にvm03を作ります。vm03へのsshまで一気に進めます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ BASE_NAME=&amp;quot;private-dns-poc-ejp&amp;quot;
$ az network vnet create -g ${BASE_NAME}-rg -n vnet02 --address-prefix 10.1.0.0/16 --subnet-name subnet01
$ az network vnet subnet update -g ${BASE_NAME}-rg -n subnet01 --vnet-name vnet02 --network-security-group subnet01-nsg
$ az network public-ip create -n vm03-pip -g ${BASE_NAME}-rg
$ az network dns zone update -g private-dns-poc-ejp-rg -n example.com --resolution-vnets vnet02
$ az network nic create -g ${BASE_NAME}-rg -n vm03-nic --public-ip-address vm03-pip --vnet vnet02 --subnet subnet01
$ az vm create -g ${BASE_NAME}-rg -n vm03 --image Canonical:UbuntuServer:16.04.0-LTS:latest --size Standard_B1s --nics vm03-nic
$ az network public-ip show -g private-dns-poc-ejp-rg -n vm03-pip --query ipAddress
&amp;quot;13.78.54.133&amp;quot;
$ ssh 13.78.54.133
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;名前解決の確認が目的なので、vnet01/02間はPeeringしません。&lt;/p&gt;

&lt;p&gt;では、vnet01上のvm01を正引きします。ドメイン名を指定しないと、解決できません。vnet02上にvm01がある、と指定されたと判断するからです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vm03:~$ nslookup vm01
Server:         168.63.129.16
Address:        168.63.129.16#53

** server can&#39;t find vm01: SERVFAIL
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ではプライベートゾーンのドメイン名をつけてみます。解決できました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vm03:~$ nslookup vm01.example.com
Server:         168.63.129.16
Address:        168.63.129.16#53

Non-authoritative answer:
Name:   vm01.example.com
Address: 10.0.0.4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Resolution VNetからは、逆引きできません。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vm03:~$ nslookup 10.0.0.4
Server:         168.63.129.16
Address:        168.63.129.16#53

** server can&#39;t find 4.0.0.10.in-addr.arpa: NXDOMAIN
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ところでRegistration VNetからResolution VNetのホスト名をnslookupするとどうなるでしょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh vm01.example.com
vm01:~$ nslookup vm03
Server:         168.63.129.16
Address:        168.63.129.16#53

** server can&#39;t find vm03: SERVFAIL

vm01:~$ nslookup vm03.example.com
Server:         168.63.129.16
Address:        168.63.129.16#53

** server can&#39;t find vm03.example.com: NXDOMAIN
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ドメイン名あり、なしに関わらず、名前解決できません。VNetが別なのでVNetの内部DNSで解決できない、また、Resolution VNetのVMはレコードがプライベートゾーンに自動登録されないことが分かります。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AzureのAvailability Zonesへ分散するVMSSをTerraformで作る</title>
      <link>http://torumakabe.github.io/post/az_vmss_terraform/</link>
      <pubDate>Mon, 26 Mar 2018 00:08:30 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/az_vmss_terraform/</guid>
      <description>

&lt;h2 id=&#34;動機&#34;&gt;動機&lt;/h2&gt;

&lt;p&gt;Terraform Azure Provider 1.3.0で、VMSSを作る際にAvailability Zonesを指定できるように&lt;a href=&#34;https://github.com/terraform-providers/terraform-provider-azurerm/pull/811&#34;&gt;なりました&lt;/a&gt;。Availability Zonesはインフラの根っこの仕組みなので、現在(&lt;sup&gt;2018&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;)限定されたリージョンで長めのプレビュー期間がとられています。ですが、GAやグローバル展開を見据え、素振りしておきましょう。&lt;/p&gt;

&lt;h2 id=&#34;前提条件&#34;&gt;前提条件&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Availability Zones対応リージョンを選びます。現在は&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/availability-zones/az-overview#regions-that-support-availability-zones&#34;&gt;5リージョン&lt;/a&gt;です。この記事ではEast US 2とします。&lt;/li&gt;
&lt;li&gt;Availability Zonesのプレビューに&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/availability-zones/az-overview&#34;&gt;サインアップ&lt;/a&gt;済みとします。&lt;/li&gt;
&lt;li&gt;bashでsshの公開鍵が~/.ssh/id_rsa.pubにあると想定します。&lt;/li&gt;
&lt;li&gt;動作確認した環境は以下です。

&lt;ul&gt;
&lt;li&gt;Terraform 0.11.2&lt;/li&gt;
&lt;li&gt;Terraform Azure Provider 1.3.0&lt;/li&gt;
&lt;li&gt;WSL (ubuntu 16.04)&lt;/li&gt;
&lt;li&gt;macos (High Sierra 10.13.3)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;コード&#34;&gt;コード&lt;/h2&gt;

&lt;p&gt;以下のファイルを同じディレクトリに作成します。&lt;/p&gt;

&lt;h3 id=&#34;terraform-メインコード&#34;&gt;Terraform メインコード&lt;/h3&gt;

&lt;p&gt;VMSSと周辺リソースを作ります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;最終行近くの &amp;ldquo;zones = [1, 2, 3]&amp;rdquo; がポイントです。これだけで、インスタンスを散らす先のゾーンを指定できます。&lt;/li&gt;
&lt;li&gt;クロスゾーン負荷分散、冗長化するため、Load BalancerとパブリックIPのSKUをStandardにします。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[main.tf]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;resource &amp;quot;azurerm_resource_group&amp;quot; &amp;quot;poc&amp;quot; {
  name     = &amp;quot;${var.resource_group_name}&amp;quot;
  location = &amp;quot;East US 2&amp;quot;
}

resource &amp;quot;azurerm_virtual_network&amp;quot; &amp;quot;poc&amp;quot; {
  name                = &amp;quot;vnet01&amp;quot;
  resource_group_name = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  location            = &amp;quot;${azurerm_resource_group.poc.location}&amp;quot;
  address_space       = [&amp;quot;10.0.0.0/16&amp;quot;]
}

resource &amp;quot;azurerm_subnet&amp;quot; &amp;quot;poc&amp;quot; {
  name                      = &amp;quot;subnet01&amp;quot;
  resource_group_name       = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  virtual_network_name      = &amp;quot;${azurerm_virtual_network.poc.name}&amp;quot;
  address_prefix            = &amp;quot;10.0.2.0/24&amp;quot;
  network_security_group_id = &amp;quot;${azurerm_network_security_group.poc.id}&amp;quot;
}

resource &amp;quot;azurerm_network_security_group&amp;quot; &amp;quot;poc&amp;quot; {
  name                = &amp;quot;nsg01&amp;quot;
  resource_group_name = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  location            = &amp;quot;${azurerm_resource_group.poc.location}&amp;quot;

  security_rule = [
    {
      name                       = &amp;quot;allow_http&amp;quot;
      priority                   = 100
      direction                  = &amp;quot;Inbound&amp;quot;
      access                     = &amp;quot;Allow&amp;quot;
      protocol                   = &amp;quot;Tcp&amp;quot;
      source_port_range          = &amp;quot;*&amp;quot;
      destination_port_range     = &amp;quot;80&amp;quot;
      source_address_prefix      = &amp;quot;*&amp;quot;
      destination_address_prefix = &amp;quot;*&amp;quot;
    },
    {
      name                       = &amp;quot;allow_ssh&amp;quot;
      priority                   = 101
      direction                  = &amp;quot;Inbound&amp;quot;
      access                     = &amp;quot;Allow&amp;quot;
      protocol                   = &amp;quot;Tcp&amp;quot;
      source_port_range          = &amp;quot;*&amp;quot;
      destination_port_range     = &amp;quot;22&amp;quot;
      source_address_prefix      = &amp;quot;*&amp;quot;
      destination_address_prefix = &amp;quot;*&amp;quot;
    },
  ]
}

resource &amp;quot;azurerm_public_ip&amp;quot; &amp;quot;poc&amp;quot; {
  name                         = &amp;quot;pip01&amp;quot;
  resource_group_name          = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  location                     = &amp;quot;${azurerm_resource_group.poc.location}&amp;quot;
  public_ip_address_allocation = &amp;quot;static&amp;quot;
  domain_name_label            = &amp;quot;${var.scaleset_name}&amp;quot;

  sku = &amp;quot;Standard&amp;quot;
}

resource &amp;quot;azurerm_lb&amp;quot; &amp;quot;poc&amp;quot; {
  name                = &amp;quot;lb01&amp;quot;
  resource_group_name = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  location            = &amp;quot;${azurerm_resource_group.poc.location}&amp;quot;

  frontend_ip_configuration {
    name                 = &amp;quot;fipConf01&amp;quot;
    public_ip_address_id = &amp;quot;${azurerm_public_ip.poc.id}&amp;quot;
  }

  sku = &amp;quot;Standard&amp;quot;
}

resource &amp;quot;azurerm_lb_backend_address_pool&amp;quot; &amp;quot;poc&amp;quot; {
  name                = &amp;quot;bePool01&amp;quot;
  resource_group_name = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  loadbalancer_id     = &amp;quot;${azurerm_lb.poc.id}&amp;quot;
}

resource &amp;quot;azurerm_lb_rule&amp;quot; &amp;quot;poc&amp;quot; {
  name                           = &amp;quot;lbRule&amp;quot;
  resource_group_name            = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  loadbalancer_id                = &amp;quot;${azurerm_lb.poc.id}&amp;quot;
  protocol                       = &amp;quot;Tcp&amp;quot;
  frontend_port                  = 80
  backend_port                   = 80
  frontend_ip_configuration_name = &amp;quot;fipConf01&amp;quot;
  backend_address_pool_id        = &amp;quot;${azurerm_lb_backend_address_pool.poc.id}&amp;quot;
  probe_id                       = &amp;quot;${azurerm_lb_probe.poc.id}&amp;quot;
}

resource &amp;quot;azurerm_lb_probe&amp;quot; &amp;quot;poc&amp;quot; {
  name                = &amp;quot;http-probe&amp;quot;
  resource_group_name = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  loadbalancer_id     = &amp;quot;${azurerm_lb.poc.id}&amp;quot;
  port                = 80
}

resource &amp;quot;azurerm_lb_nat_pool&amp;quot; &amp;quot;poc&amp;quot; {
  count                          = 3
  name                           = &amp;quot;ssh&amp;quot;
  resource_group_name            = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  loadbalancer_id                = &amp;quot;${azurerm_lb.poc.id}&amp;quot;
  protocol                       = &amp;quot;Tcp&amp;quot;
  frontend_port_start            = 50000
  frontend_port_end              = 50119
  backend_port                   = 22
  frontend_ip_configuration_name = &amp;quot;fipConf01&amp;quot;
}

data &amp;quot;template_cloudinit_config&amp;quot; &amp;quot;poc&amp;quot; {
  gzip          = true
  base64_encode = true

  part {
    content_type = &amp;quot;text/cloud-config&amp;quot;
    content      = &amp;quot;${file(&amp;quot;${path.module}/cloud-config.yaml&amp;quot;)}&amp;quot;
  }
}

resource &amp;quot;azurerm_virtual_machine_scale_set&amp;quot; &amp;quot;poc&amp;quot; {
  name                = &amp;quot;${var.scaleset_name}&amp;quot;
  resource_group_name = &amp;quot;${azurerm_resource_group.poc.name}&amp;quot;
  location            = &amp;quot;${azurerm_resource_group.poc.location}&amp;quot;
  upgrade_policy_mode = &amp;quot;Manual&amp;quot;

  sku {
    name     = &amp;quot;Standard_B1s&amp;quot;
    tier     = &amp;quot;Standard&amp;quot;
    capacity = 3
  }

  storage_profile_image_reference {
    publisher = &amp;quot;Canonical&amp;quot;
    offer     = &amp;quot;UbuntuServer&amp;quot;
    sku       = &amp;quot;16.04-LTS&amp;quot;
    version   = &amp;quot;latest&amp;quot;
  }

  storage_profile_os_disk {
    name              = &amp;quot;&amp;quot;
    caching           = &amp;quot;ReadWrite&amp;quot;
    create_option     = &amp;quot;FromImage&amp;quot;
    managed_disk_type = &amp;quot;Standard_LRS&amp;quot;
  }

  os_profile {
    computer_name_prefix = &amp;quot;pocvmss&amp;quot;
    admin_username       = &amp;quot;${var.admin_username}&amp;quot;
    admin_password       = &amp;quot;&amp;quot;
    custom_data          = &amp;quot;${data.template_cloudinit_config.poc.rendered}&amp;quot;
  }

  os_profile_linux_config {
    disable_password_authentication = true

    ssh_keys {
      path     = &amp;quot;/home/${var.admin_username}/.ssh/authorized_keys&amp;quot;
      key_data = &amp;quot;${file(&amp;quot;~/.ssh/id_rsa.pub&amp;quot;)}&amp;quot;
    }
  }

  network_profile {
    name    = &amp;quot;terraformnetworkprofile&amp;quot;
    primary = true

    ip_configuration {
      name                                   = &amp;quot;PoCIPConfiguration&amp;quot;
      subnet_id                              = &amp;quot;${azurerm_subnet.poc.id}&amp;quot;
      load_balancer_backend_address_pool_ids = [&amp;quot;${azurerm_lb_backend_address_pool.poc.id}&amp;quot;]
      load_balancer_inbound_nat_rules_ids    = [&amp;quot;${element(azurerm_lb_nat_pool.poc.*.id, count.index)}&amp;quot;]
    }
  }

  zones = [1, 2, 3]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;cloud-init-configファイル&#34;&gt;cloud-init configファイル&lt;/h3&gt;

&lt;p&gt;各インスタンスがどのゾーンで動いているか確認したいので、インスタンス作成時にcloud-initでWebサーバーを仕込みます。メタデータからインスタンス名と実行ゾーンを引っ張り、nginxのドキュメントルートに書きます。&lt;/p&gt;

&lt;p&gt;[cloud-config.yaml]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#cloud-config
package_upgrade: true
packages:
  - nginx
runcmd:
  - &#39;echo &amp;quot;[Instance Name]: `curl -H Metadata:true &amp;quot;http://169.254.169.254/metadata/instance/compute/name?api-version=2017-12-01&amp;amp;format=text&amp;quot;`    [Zone]: `curl -H Metadata:true &amp;quot;http://169.254.169.254/metadata/instance/compute/zone?api-version=2017-12-01&amp;amp;format=text&amp;quot;`&amp;quot; &amp;gt; /var/www/html/index.nginx-debian.html&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;インスタンス作成時、パッケージの導入やアップデートに時間をかけたくない場合は、Packerなどで前もってカスタムイメージを作っておくのも手です。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/virtual-machines/linux/build-image-with-packer&#34;&gt;Packer を使用して Azure に Linux 仮想マシンのイメージを作成する方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/terraform/terraform-create-vm-scaleset-network-disks-using-packer-hcl&#34;&gt;Terraform を使用して Packer カスタム イメージから Azure 仮想マシン スケール セットを作成する&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;terraform-変数ファイル&#34;&gt;Terraform 変数ファイル&lt;/h3&gt;

&lt;p&gt;変数は別ファイルへ。&lt;/p&gt;

&lt;p&gt;[variables.tf]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;variable &amp;quot;resource_group_name&amp;quot; {
  default = &amp;quot;your-rg&amp;quot;
}

variable &amp;quot;scaleset_name&amp;quot; {
  default = &amp;quot;yourvmss01&amp;quot;
}

variable &amp;quot;admin_username&amp;quot; {
  default = &amp;quot;yourname&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;実行&#34;&gt;実行&lt;/h2&gt;

&lt;p&gt;では実行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ terraform init
$ terraform plan
$ terraform apply
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5分くらいで完了しました。このサンプルでは、この後のcloud-initのパッケージ処理に時間がかかります。待てない場合は前述の通り、カスタムイメージを使いましょう。&lt;/p&gt;

&lt;p&gt;インスタンスへのsshを通すよう、Load BalancerにNATを設定していますので、cloud-initの進捗は確認できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh -p 50000 yourname@yourvmss01.eastus2.cloudapp.azure.com
$ tail -f /var/log/cloud-init-output.log
Cloud-init v. 17.1 finished at Sun, 25 Mar 2018 10:41:40 +0000. Datasource DataSourceAzure [seed=/dev/sr0].  Up 611.51 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ではWebサーバーにアクセスしてみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ while true; do curl yourvmss01.eastus2.cloudapp.azure.com; sleep 1; done;
[Instance Name]: yourvmss01_2    [Zone]: 3
[Instance Name]: yourvmss01_0    [Zone]: 1
[Instance Name]: yourvmss01_2    [Zone]: 3
[Instance Name]: yourvmss01_1    [Zone]: 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;VMSSのインスタンスがゾーンに分散されたことが分かります。&lt;/p&gt;

&lt;p&gt;では、このままスケールアウトしてみましょう。main.tfのazurerm_virtual_machine_scale_set.poc.sku.capacityを3から4にし、再度applyします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Instance Name]: yourvmss01_1    [Zone]: 2
[Instance Name]: yourvmss01_3    [Zone]: 1
[Instance Name]: yourvmss01_3    [Zone]: 1
[Instance Name]: yourvmss01_1    [Zone]: 2
[Instance Name]: yourvmss01_3    [Zone]: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ダウンタイムなしに、yourvmss01_3が追加されました。すこぶる簡単。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AKSのService作成時にホスト名を付ける</title>
      <link>http://torumakabe.github.io/post/aks_dns/</link>
      <pubDate>Mon, 12 Mar 2018 00:21:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/aks_dns/</guid>
      <description>

&lt;h2 id=&#34;2つのやり口&#34;&gt;2つのやり口&lt;/h2&gt;

&lt;p&gt;Azure Container Service(AKS)はServiceを公開する際、パブリックIPを割り当てられます。でもIPだけじゃなく、ホスト名も同時に差し出して欲しいケースがありますよね。&lt;/p&gt;

&lt;p&gt;わたしの知る限り、2つの方法があります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AKS(k8s) 1.9で対応した&lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/47849&#34;&gt;DNSラベル名付与機能&lt;/a&gt;を使う&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/external-dns&#34;&gt;Kubenetes ExternalDNS&lt;/a&gt;を使ってAzure DNSへAレコードを追加する&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以下、AKS 1.9.2での実現手順です。&lt;/p&gt;

&lt;h2 id=&#34;dnsラベル名付与機能&#34;&gt;DNSラベル名付与機能&lt;/h2&gt;

&lt;p&gt;簡単です。Serviceのannotationsに定義するだけ。試しにnginxをServiceとして公開し、確認してみましょう。&lt;/p&gt;

&lt;p&gt;[nginx-label.yaml]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx
spec:
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: hogeginx
  annotations:
    service.beta.kubernetes.io/azure-dns-label-name: hogeginx
spec:
  selector:
    app: nginx
  type: LoadBalancer
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デプロイ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f nginx-label.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;パブリックIP(EXTERNAL-IP)が割り当てられた後、ラベル名が使えます。ルールは [ラベル名].[リージョン].cloudapp.azure.com です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl hogeginx.eastus.cloudapp.azure.com
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
[snip]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ドメイン名は指定しなくていいから、Service毎にホスト名を固定したいんじゃ、という場合にはこれでOK。&lt;/p&gt;

&lt;h2 id=&#34;kubenetes-externaldns&#34;&gt;Kubenetes ExternalDNS&lt;/h2&gt;

&lt;p&gt;任意のドメイン名を使いたい場合は、Incubatorプロジェクトのひとつ、Kubenetes ExternalDNSを使ってAzure DNSへAレコードを追加する手があります。本家の説明は&lt;a href=&#34;https://github.com/kubernetes-incubator/external-dns/blob/master/docs/tutorials/azure.md&#34;&gt;こちら&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;Kubenetes ExternalDNSは、Azure DNSなどAPIを持つDNSサービスを操作するアプリです。k8sのDeploymentとして動かせます。Route 53などにも対応。&lt;/p&gt;

&lt;p&gt;さて動かしてみましょう。前提として、すでにAzure DNSにゾーンがあるものとします。&lt;/p&gt;

&lt;p&gt;ExternalDNSがDNSゾーンを操作できるよう、サービスプリンシパルを作成しましょう。スコープはDNSゾーンが置かれているリソースグループ、ロールはContributorとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az ad sp create-for-rbac --role=&amp;quot;Contributor&amp;quot; --scopes=&amp;quot;/subscriptions/your-subscription-id/resourceGroups/hoge-dns-rg&amp;quot; -n hogeExtDnsSp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;appId、password、tenantを控えておいてください。次でsecretに使います。&lt;/p&gt;

&lt;p&gt;ではExteralDNSに渡すsecretを作ります。まずJSONファイルに書きます。&lt;/p&gt;

&lt;p&gt;[azure.json]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &amp;quot;tenantId&amp;quot;: &amp;quot;your-tenant&amp;quot;,
    &amp;quot;subscriptionId&amp;quot;: &amp;quot;your-subscription-id&amp;quot;,
    &amp;quot;aadClientId&amp;quot;: &amp;quot;your-appId&amp;quot;,
    &amp;quot;aadClientSecret&amp;quot;: &amp;quot;your-password&amp;quot;,
    &amp;quot;resourceGroup&amp;quot;: &amp;quot;hoge-dns-rg&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;JSONファイルを元に、secretを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create secret generic azure-config-file --from-file=azure.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ExteralDNSのマニフェストを作ります。ドメイン名はexmaple.comとしていますが、使うDNSゾーンに合わせてください。以下はRBACを使っていない環境での書き方です。&lt;/p&gt;

&lt;p&gt;[extdns.yaml]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: external-dns
spec:
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: external-dns
    spec:
      containers:
      - name: external-dns
        image: registry.opensource.zalan.do/teapot/external-dns:v0.4.8
        args:
        - --source=service
        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.
        - --provider=azure
        - --azure-resource-group=hoge-dns-rg # (optional) use the DNS zones from the tutorial&#39;s resource group
        volumeMounts:
        - name: azure-config-file
          mountPath: /etc/kubernetes
          readOnly: true
      volumes:
      - name: azure-config-file
        secret:
          secretName: azure-config-file
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ExternalDNSをデプロイします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f extdns.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ではホスト名を付与するServiceのマニフェストを作りましょう。先ほどのDNSラベル名付与機能と同様、annotationsへ定義します。&lt;/p&gt;

&lt;p&gt;[nginx-extdns.yaml]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx-extdns
spec:
  template:
    metadata:
      labels:
        app: nginx-extdns
    spec:
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: hogeginx-extdns
  annotations:
    external-dns.alpha.kubernetes.io/hostname: hogeginx.example.com
spec:
  selector:
    app: nginx-extdns
  type: LoadBalancer
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デプローイ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f nginx-extdns.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;パブリックIP(EXTERNAL-IP)が割り当てられた後、Aレコードが登録されます。確認してみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az network dns record-set a list -g hoge-dns-rg -z example.com -o table
Name      ResourceGroup       Ttl  Type    Metadata
--------  ----------------  -----  ------  ----------
hogeginx  hoge-dns-rg         300  A
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ゲッツ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl hogeginx.example.com
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
[snip]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Incubatorプロジェクトなので今後大きく変化する可能性がありますが、ご参考になれば。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AKSのIngress TLS証明書を自動更新する</title>
      <link>http://torumakabe.github.io/post/aks_tls_autorenewal/</link>
      <pubDate>Sun, 11 Feb 2018 00:20:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/aks_tls_autorenewal/</guid>
      <description>

&lt;h2 id=&#34;カジュアルな証明書管理方式が欲しい&#34;&gt;カジュアルな証明書管理方式が欲しい&lt;/h2&gt;

&lt;p&gt;ChromeがHTTPサイトに対する警告を&lt;a href=&#34;https://japan.cnet.com/article/35100589/&#34;&gt;強化するそうです&lt;/a&gt;。非HTTPSサイトには、生きづらい世の中になりました。&lt;/p&gt;

&lt;p&gt;さてそうなると、TLS証明書の入手と更新、めんどくさいですね。ガチなサイトでは証明書の維持管理を計画的に行うべきですが、検証とかちょっとした用途で立てるサイトでは、とにかくめんどくさい。カジュアルな方式が望まれます。&lt;/p&gt;

&lt;p&gt;そこで、Azure Container Service(AKS)で使える気軽な方法をご紹介します。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;TLSはIngress(NGINX Ingress Controller)でまとめて終端&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://letsencrypt.org/&#34;&gt;Let&amp;rsquo;s Encypt&lt;/a&gt;から証明書を入手&lt;/li&gt;
&lt;li&gt;Kubenetesのアドオンである&lt;a href=&#34;https://github.com/jetstack/cert-manager/&#34;&gt;cert-manager&lt;/a&gt;で証明書の入手、更新とIngressへの適用を自動化

&lt;ul&gt;
&lt;li&gt;ACME(Automatic Certificate Management Environment)対応&lt;/li&gt;
&lt;li&gt;cert-managerはまだ歴史の浅いプロジェクトだが、&lt;a href=&#34;https://github.com/jetstack/cert-manager/&#34;&gt;kube-lego&lt;/a&gt;の後継として期待&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;なおKubernetes/AKSは開発ペースやエコシステムの変化が速いので要注意。この記事は2018/2/10に書いています。&lt;/p&gt;

&lt;h2 id=&#34;使い方&#34;&gt;使い方&lt;/h2&gt;

&lt;p&gt;AKSクラスターと、Azure DNS上に利用可能なゾーンがあることを前提にします。ない場合、それぞれ公式ドキュメントを参考にしてください。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/aks/kubernetes-walkthrough&#34;&gt;Azure Container Service (AKS) クラスターのデプロイ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/dns/dns-getstarted-cli&#34;&gt;Azure CLI 2.0 で Azure DNS の使用を開始する&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;まずAKSにNGINX Ingress Controllerを導入します。helmで入れるのが楽でしょう。&lt;a href=&#34;http://torumakabe.github.io/post/aks_ingress_quickdeploy/&#34;&gt;この記事&lt;/a&gt;も参考に。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm install stable/nginx-ingress --name my-nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サービスの状況を確認します。NGINX Ingress ControllerにEXTERNAL-IPが割り当てられるまで、待ちます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get svc
NAME                                     TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                     AGE
kubernetes                               ClusterIP      10.0.0.1       &amp;lt;none&amp;gt;           443/TCP                     79d
my-nginx-nginx-ingress-controller        LoadBalancer   10.0.2.105     52.234.148.138   80:30613/TCP,443:30186/TCP   6m
my-nginx-nginx-ingress-default-backend   ClusterIP      10.0.102.246   &amp;lt;none&amp;gt;           80/TCP                     6m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;EXTERNAL-IPが割り当てられたら、Azure DNSで名前解決できるようにします。Azure CLIを使います。Ingressのホスト名をwww.example.comとする例です。このホスト名で、後ほどLet&amp;rsquo;s Encryptから証明書を取得します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az network dns record-set a add-record -z example.com -g your-dnszone-rg -n www -a 52.234.148.138
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cert-managerのソースをGitHubから取得し、contribからhelm installします。いずれstableを使えるようになるでしょう。なお、このAKSクラスターはまだRBACを使っていないので、&amp;rdquo;&amp;ndash;set rbac.create=false&amp;rdquo;オプションを指定しています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/jetstack/cert-manager
$ cd cert-manager/
$ helm install --name cert-manager --namespace kube-system contrib/charts/cert-manager --set rbac.create=false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;では任意の作業ディレクトリに移動し、以下の内容でマニフェストを作ります。cm-issuer-le-staging-sample.yamlとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: certmanager.k8s.io/v1alpha1
kind: Issuer
metadata:
  name: letsencrypt-staging
  namespace: default
spec:
  acme:
    # The ACME server URL
    server: https://acme-staging.api.letsencrypt.org/directory
    # Email address used for ACME registration
    email: hoge@example.com
    # Name of a secret used to store the ACME account private key
    privateKeySecretRef:
      name: letsencrypt-staging
    # Enable the HTTP-01 challenge provider
    http01: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;証明書を発行してもらうLet&amp;rsquo;s EncryptをIssuerとして登録するわけですが、まずはステージングのAPIエンドポイントを指定しています。Let&amp;rsquo;s Encryptには&lt;a href=&#34;https://letsencrypt.org/docs/rate-limits/&#34;&gt;Rate Limit&lt;/a&gt;があり、失敗した時に痛いからです。Let&amp;rsquo;s EncryptのステージングAPIを使うとフェイクな証明書(Fake LE Intermediate X1)が発行されますが、流れの確認やマニフェストの検証は、できます。&lt;/p&gt;

&lt;p&gt;なお、Let&amp;rsquo;s Encryptとのチャレンジには今回、HTTPを使います。DNSチャレンジも&lt;a href=&#34;https://github.com/jetstack/cert-manager/pull/246&#34;&gt;いずれ対応する見込み&lt;/a&gt;です。&lt;/p&gt;

&lt;p&gt;では、Issuerを登録します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f cm-issuer-le-staging-sample.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次は証明書の設定です。マニフェストはcm-cert-le-staging-sample.yamlとします。acme節にACME構成を書きます。チャレンジはHTTP、ingressClassはnginxです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: example-com
  namespace: default
spec:
  secretName: example-com-tls
  issuerRef:
    name: letsencrypt-staging
  commonName: www.example.com
  dnsNames:
  - www.example.com
  acme:
    config:
    - http01:
        ingressClass: nginx
      domains:
      - www.example.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;証明書設定をデプロイします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f cm-cert-le-staging-sample.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;証明書の発行状況を確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe certificate example-com
Name:         example-com
Namespace:    default
[snip]
Events:
  Type     Reason                 Age              From                     Message
  ----     ------                 ----             ----                     -------
  Warning  ErrorCheckCertificate  8m               cert-manager-controller  Error checking existing TLS certificate: secret &amp;quot;example-com-tls&amp;quot; not found
  Normal   PrepareCertificate     8m               cert-manager-controller  Preparing certificate with issuer
  Normal   PresentChallenge       8m               cert-manager-controller  Presenting http-01 challenge for domain www.example.com
  Normal   SelfCheck              8m               cert-manager-controller  Performing self-check for domain www.example.com
  Normal   ObtainAuthorization    7m               cert-manager-controller  Obtained authorization for domain www.example.com
  Normal   IssueCertificate       7m               cert-manager-controller  Issuing certificate...
  Normal   CeritifcateIssued      7m               cert-manager-controller  Certificated issuedsuccessfully
  Normal   RenewalScheduled       7m (x2 over 7m)  cert-manager-controller  Certificate scheduled for renewal in 1438 hours
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;無事に証明書が発行され、更新もスケジュールされました。手順やマニフェストの書きっぷりは問題なさそうです。これをもってステージング完了としましょう。&lt;/p&gt;

&lt;p&gt;ではLet&amp;rsquo;s EncryptのAPIエンドポイントをProduction向けに変更し、新たにIssuer登録します。cm-issuer-le-prod-sample.yamlとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: certmanager.k8s.io/v1alpha1
kind: Issuer
metadata:
  name: letsencrypt-prod
  namespace: default
spec:
  acme:
    # The ACME server URL
    server: https://acme-v01.api.letsencrypt.org/directory
    # Email address used for ACME registration
    email: hoge@example.com
    # Name of a secret used to store the ACME account private key
    privateKeySecretRef:
      name: letsencrypt-prod
    # Enable the HTTP-01 challenge provider
    http01: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デプロイします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f cm-issuer-le-prod-sample.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同様に、Production向けの証明書設定をします。cm-cert-le-prod-sample.yamlとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: prod-example-com
  namespace: default
spec:
  secretName: prod-example-com-tls
  issuerRef:
    name: letsencrypt-prod
  commonName: www.example.com
  dnsNames:
  - www.example.com
  acme:
    config:
    - http01:
        ingressClass: nginx
      domains:
      - www.example.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デプロイします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f cm-cert-le-prod-sample.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;発行状況を確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe certificate prod-example-com
Name:         prod-example-com
Namespace:    default
[snip]
Events:
  Type     Reason                 Age              From                     Message
  ----     ------                 ----             ----                     -------
  Warning  ErrorCheckCertificate  27s              cert-manager-controller  Error checking existing TLS certificate: secret &amp;quot;prod-example-com-tls&amp;quot; not found
  Normal   PrepareCertificate     27s              cert-manager-controller  Preparing certificate with issuer
  Normal   PresentChallenge       26s              cert-manager-controller  Presenting http-01 challenge for domain www.example.com
  Normal   SelfCheck              26s              cert-manager-controller  Performing self-check for domain www.example.com
  Normal   IssueCertificate       7s               cert-manager-controller  Issuing certificate...
  Normal   ObtainAuthorization    7s               cert-manager-controller  Obtained authorization for domain www.example.com
  Normal   RenewalScheduled       6s (x3 over 5m)  cert-manager-controller  Certificate scheduled for renewal in 1438 hours
  Normal   CeritifcateIssued      6s               cert-manager-controller  Certificated issuedsuccessfully
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;証明書が発行され、1438時間(約60日)内の更新がスケジュールされました。&lt;/p&gt;

&lt;p&gt;ではバックエンドを設定して確認してみましょう。バックエンドにNGINXを立て、exposeします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run nginx --image nginx --port 80
$ kubectl expose deployment nginx --type NodePort
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ingressを設定します。ファイル名はingress-nginx-sample.yamlとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
  name: ingress-nginx-sample
spec:
  rules:
    - host: www.example.com
      http:
        paths:
          - path: /
            backend:
              serviceName: nginx
              servicePort: 80
  tls:
    - hosts:
      - www.example.com
      secretName: prod-example-com-tls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デプロイします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f ingress-nginx-sample.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;いざ確認。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl https://www.example.com/
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
[snip]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;便利ですね。Let&amp;rsquo;s Encryptをはじめ、関連プロジェクトに感謝です。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AKSのNGINX Ingress Controllerのデプロイで悩んだら</title>
      <link>http://torumakabe.github.io/post/aks_ingress_quickdeploy/</link>
      <pubDate>Sat, 10 Feb 2018 11:00:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/aks_ingress_quickdeploy/</guid>
      <description>

&lt;h2 id=&#34;楽したいならhelmで入れましょう&#34;&gt;楽したいならhelmで入れましょう&lt;/h2&gt;

&lt;p&gt;AKSに限った話ではありませんが、Kubernetesにぶら下げるアプリの数が多くなってくると、URLマッピングやTLS終端がしたくなります。方法は色々あるのですが、シンプルな選択肢はNGINX Ingress Controllerでしょう。&lt;/p&gt;

&lt;p&gt;さて、そのNGINX Ingress Controllerのデプロイは&lt;a href=&#34;https://github.com/kubernetes/ingress-nginx/blob/master/deploy/README.md&#34;&gt;GitHubのドキュメント&lt;/a&gt;通りに淡々とやればいいのですが、&lt;a href=&#34;https://github.com/kubernetes/helm&#34;&gt;helm&lt;/a&gt;を使えばコマンド一発です。そのようにドキュメントにも書いてあるのですが、最後の方で出てくるので「それ早く言ってよ」な感じです。&lt;/p&gt;

&lt;p&gt;せっかくなので、Azure(AKS)での使い方をまとめておきます。開発ペースやエコシステムの変化が速いので要注意。この記事は2018/2/10に書いています。&lt;/p&gt;

&lt;h2 id=&#34;使い方&#34;&gt;使い方&lt;/h2&gt;

&lt;p&gt;AKSクラスターと、Azure DNS上に利用可能なゾーンがあることを前提にします。ない場合、それぞれ公式ドキュメントを参考にしてください。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/aks/kubernetes-walkthrough&#34;&gt;Azure Container Service (AKS) クラスターのデプロイ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/dns/dns-getstarted-cli&#34;&gt;Azure CLI 2.0 で Azure DNS の使用を開始する&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ではhelmでNGINX Ingress Controllerを導入します。helmを使っていなければ、&lt;a href=&#34;https://github.com/kubernetes/helm#install&#34;&gt;入れておいてください&lt;/a&gt;。デプロイはこれだけ。Chartは&lt;a href=&#34;https://github.com/kubernetes/charts/tree/master/stable/nginx-ingress&#34;&gt;ここ&lt;/a&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm install stable/nginx-ingress --name my-nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;バックエンドへのつなぎが機能するか、Webアプリを作ってテストします。NGINXとApacheを選びました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run nginx --image nginx --port 80
$ kubectl run apache --image httpd --port 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サービスとしてexposeします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl expose deployment nginx --type NodePort
$ kubectl expose deployment apache --type NodePort
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;現時点のサービスたちを確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get svc
NAME                                     TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)                  AGE
apache                                   NodePort       10.0.244.167   &amp;lt;none&amp;gt;          80:30928/TCP                 14h
kubernetes                               ClusterIP      10.0.0.1       &amp;lt;none&amp;gt;          443/TCP                  79d
my-nginx-nginx-ingress-controller        LoadBalancer   10.0.91.78     13.72.108.187   80:32448/TCP,443:31991/TCP   14h
my-nginx-nginx-ingress-default-backend   ClusterIP      10.0.74.104    &amp;lt;none&amp;gt;          80/TCP                  14h
nginx                                    NodePort       10.0.191.16    &amp;lt;none&amp;gt;          80:30752/TCP                 14h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;AKSの場合はパブリックIPがNGINX Ingress Controllerに割り当てられます。EXTERNAL-IPがpendingの場合は割り当て中なので、しばし待ちます。&lt;/p&gt;

&lt;p&gt;割り当てられたら、EXTERNAL-IPをAzure DNSで名前解決できるようにしましょう。Azure CLIを使います。dev.example.comの例です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az network dns record-set a add-record -z example.com -g your-dnszone-rg -n dev -a 13.72.108.187
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;TLSが終端できるかも検証したいので、Secretを作ります。証明書とキーはLet&amp;rsquo;s Encryptで作っておきました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create secret tls example-tls --key privkey.pem --cert fullchain.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ではIngressを構成しましょう。以下をファイル名ingress-nginx-sample.yamlとして保存します。IngressでTLSを終端し、/へのアクセスは先ほどexposeしたNGINXのサービスへ、/apacheへのアクセスはApacheへ流します。rewrite-targetをannotaionsで指定するのを、忘れずに。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /
  name: ingress-nginx-sample
spec:
  rules:
    - host: dev.example.com
      http:
        paths:
          - path: /
            backend:
              serviceName: nginx
              servicePort: 80
          - path: /apache
            backend:
              serviceName: apache
              servicePort: 80
  tls:
    - hosts:
      - dev.example.com
      secretName: example-tls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;あとは反映するだけ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f ingress-nginx-sample.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;curlで確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl https://dev.example.com
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
[snip]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/apacheへのパスも確認します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl https://dev.example.com/apache
&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;It works!&amp;lt;/h1&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;簡単ですね。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Azureのリソースグループ限定 共同作成者をいい感じに作る</title>
      <link>http://torumakabe.github.io/post/azure_rg_contributor/</link>
      <pubDate>Mon, 22 Jan 2018 22:00:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/azure_rg_contributor/</guid>
      <description>

&lt;h2 id=&#34;共同作成者は-ちょっと強い&#34;&gt;共同作成者は、ちょっと強い&lt;/h2&gt;

&lt;p&gt;Azureのリソースグループは、リソースを任意のグループにまとめ、ライフサイクルや権限の管理を一括して行える便利なコンセプトです。&lt;/p&gt;

&lt;p&gt;ユースケースのひとつに、&amp;rdquo;本番とは分離した開発向けリソースグループを作って、アプリ/インフラ開発者に開放したい&amp;rdquo;、があります。新しい技術は試行錯誤で身につくので、こういった環境は重要です。&lt;/p&gt;

&lt;p&gt;なのですが、このようなケースで、権限付与の落とし穴があります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;サブスクリプション所有者が開発用リソースグループを作る&lt;/li&gt;
&lt;li&gt;スコープを開発用リソースグループに限定し、開発者に対し共同作成者ロールを割り当てる&lt;/li&gt;
&lt;li&gt;開発者はリソースグループ限定で、のびのび試行錯誤できて幸せ&lt;/li&gt;
&lt;li&gt;開発者がスッキリしたくなり、リソースグループごとバッサリ削除 (共同作成者なので可能)&lt;/li&gt;
&lt;li&gt;開発者にはサブスクリプションレベルの権限がないため、リソースグループを作成できない&lt;/li&gt;
&lt;li&gt;詰む&lt;/li&gt;
&lt;li&gt;サブスクリプション所有者が、リソースグループ作成と権限付与をやり直し&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;共同作成者ロールから、リソースグループの削除権限だけを除外できると、いいんですが。そこでカスタムロールの出番です。リソースグループ限定、グループ削除権限なしの共同作成者を作ってみましょう。&lt;/p&gt;

&lt;h2 id=&#34;いい感じのカスタムロールを作る&#34;&gt;いい感じのカスタムロールを作る&lt;/h2&gt;

&lt;p&gt;Azureのカスタムロールは、個別リソースレベルで粒度の細かい権限設定ができます。ですが、やり過ぎると破綻するため、シンプルなロールを最小限作る、がおすすめです。&lt;/p&gt;

&lt;p&gt;シンプルに行きましょう。まずはカスタムロールの定義を作ります。role.jsonとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &amp;quot;Name&amp;quot;: &amp;quot;Resource Group Contributor&amp;quot;,
    &amp;quot;IsCustom&amp;quot;: true,
    &amp;quot;Description&amp;quot;: &amp;quot;Lets you manage everything except access to resources, but can not delete Resouce Group&amp;quot;,
    &amp;quot;Actions&amp;quot;: [
        &amp;quot;*&amp;quot;
    ],
    &amp;quot;NotActions&amp;quot;: [
        &amp;quot;Microsoft.Authorization/*/Delete&amp;quot;,
        &amp;quot;Microsoft.Authorization/*/Write&amp;quot;,
        &amp;quot;Microsoft.Authorization/elevateAccess/Action&amp;quot;,
        &amp;quot;Microsoft.Resources/subscriptions/resourceGroups/Delete&amp;quot;
    ],
    &amp;quot;AssignableScopes&amp;quot;: [
        &amp;quot;/subscriptions/your-subscriotion-id&amp;quot;
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;組み込みロールの共同作成者をテンプレに、NotActionsでリソースグループの削除権限を除外しました。AssignableScopesでリソースグループを限定してもいいですが、リソースグループの数だけロールを作るのはつらいので、ここでは指定しません。後からロールを割り当てる時にスコープを指定します。&lt;/p&gt;

&lt;p&gt;では、カスタムロールを作成します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az role definition create --role-definition ./role.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;出力にカスタムロールのIDが入っていますので、控えておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;id&amp;quot;: &amp;quot;/subscriptions/your-subscriotion-id/providers/Microsoft.Authorization/roleDefinitions/your-customrole-id&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;カスタムロールをユーザー-グループ-サービスプリンシパルに割り当てる&#34;&gt;カスタムロールをユーザー、グループ、サービスプリンシパルに割り当てる&lt;/h2&gt;

&lt;p&gt;次に、ユーザー/グループに先ほど作ったカスタムロールを割り当てます。スコープはリソースグループに限定します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az role assignment create --assignee-object-id your-user-or-group-object-id --role your-customrole-id --scope &amp;quot;/subscriptions/your-subscriotion-id/resourceGroups/sample-dev-rg&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サービスプリンシパル作成時に割り当てる場合は、以下のように。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az ad sp create-for-rbac -n &amp;quot;rgcontributor&amp;quot; -p &amp;quot;your-password&amp;quot; --role your-customrole-id --scopes &amp;quot;/subscriptions/your-subscriotion-id/resourceGroups/sample-dev-rg&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;余談ですが、&amp;rdquo;az ad sp create-for-rbac&amp;rdquo;コマンドはAzure ADアプリケーションを同時に作るため、別途アプリを作ってサービスプリンシパルと紐づける、という作業が要りません。&lt;/p&gt;

&lt;h2 id=&#34;試してみる&#34;&gt;試してみる&lt;/h2&gt;

&lt;p&gt;ログインして試してみましょう。サービスプリンシパルの例です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az login --service-principal -u &amp;quot;http://rgcontributor&amp;quot; -p &amp;quot;your-password&amp;quot; -t &amp;quot;your-tenant-id&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;検証したサブスクリプションには多数のリソースグループがあるのですが、スコープで指定したものだけが見えます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az group list -o table
Name              Location    Status
----------------  ----------  ---------
sample-dev-rg  japaneast   Succeeded
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このリソースグループに、VMを作っておきました。リストはしませんが、ストレージやネットワークなど関連リソースもこのグループにあります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az vm list -o table
Name              ResourceGroup     Location
----------------  ----------------  ----------
sampledevvm01     sample-dev-rg  japaneast
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;試しにリソースグループを作ってみます。サブスクリプションスコープの権限がないため怒られます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az group create -n rgc-poc-rg -l japaneast
The client &#39;aaaaa-bbbbb-ccccc-ddddd-eeeee&#39; with object id &#39;aaaaa-bbbbb-ccccc-ddddd-eeeee&#39; does not have authorization to perform action &#39;Microsoft.Resources/subscriptions/resourcegroups/write&#39; over scope &#39;/subscriptions/your-subscriotion-id/resourcegroups/rgc-poc-rg&#39;.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;リソースグループを消してみます。消すかい？ -&amp;gt; y -&amp;gt; ダメ、という、持ち上げて落とす怒り方です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az group delete -n sample-dev-rg
Are you sure you want to perform this operation? (y/n): y
The client &#39;aaaaa-bbbbb-ccccc-ddddd-eeeee&#39; with object id &#39;aaaaa-bbbbb-ccccc-ddddd-eeeee&#39; does not have authorization to perform action &#39;Microsoft.Resources/subscriptions/resourcegroups/delete&#39; over scope &#39;/subscriptions/your-subscriotion-id/resourcegroups/sample-dev-rg&#39;.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;でもリソースグループのリソースを一括削除したい&#34;&gt;でもリソースグループのリソースを一括削除したい&lt;/h2&gt;

&lt;p&gt;でも、リソースグループは消せなくても、リソースをバッサリ消す手段は欲しいですよね。そんな時には空のリソースマネージャーテンプレートを、completeモードでデプロイすると、消せます。&lt;/p&gt;

&lt;p&gt;空テンプレートを、empty.jsonとしましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &amp;quot;$schema&amp;quot;: &amp;quot;http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#&amp;quot;,
    &amp;quot;contentVersion&amp;quot;: &amp;quot;1.0.0.0&amp;quot;,
    &amp;quot;parameters&amp;quot;: {},
    &amp;quot;variables&amp;quot;: {},
    &amp;quot;resources&amp;quot;: [],
    &amp;quot;outputs&amp;quot;: {}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;破壊的空砲を打ちます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az group deployment create --mode complete -g sample-dev-rg --template-file ./empty.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;リソースグループは残ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az group list -o table
Name              Location    Status
----------------  ----------  ---------
sample-dev-rg  japaneast   Succeeded
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;VMは消えました。リストしませんが、他の関連リソースもバッサリ消えています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az vm list -o table

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>TerraformでAzure サンプル 2018/1版</title>
      <link>http://torumakabe.github.io/post/terraform_azure_sample_201801/</link>
      <pubDate>Mon, 08 Jan 2018 16:30:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/terraform_azure_sample_201801/</guid>
      <description>

&lt;h2 id=&#34;サンプルのアップデート&#34;&gt;サンプルのアップデート&lt;/h2&gt;

&lt;p&gt;年末にリポジトリの大掃除をしていて、2年前に書いたTerraform &amp;amp; Azureの&lt;a href=&#34;http://torumakabe.github.io/post/azure_tf_fundamental_rules/&#34;&gt;記事&lt;/a&gt;に目が止まりました。原則はいいとして、&lt;a href=&#34;https://github.com/ToruMakabe/Terraform_Azure_Sample&#34;&gt;サンプル&lt;/a&gt;は2年物で腐りかけです。ということでアップデートします。&lt;/p&gt;

&lt;h2 id=&#34;インパクトの大きな変更点&#34;&gt;インパクトの大きな変更点&lt;/h2&gt;

&lt;p&gt;Terraformの、ここ2年の重要なアップデートは以下でしょうか。Azure視点で。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;BackendにAzure Blobを使えるようになった&lt;/li&gt;
&lt;li&gt;Workspaceで同一コード・複数環境管理ができるようになった&lt;/li&gt;
&lt;li&gt;対応リソースが増えた&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://registry.terraform.io/&#34;&gt;Terraform Module Registry&lt;/a&gt;が公開された&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;更新版サンプルの方針&#34;&gt;更新版サンプルの方針&lt;/h2&gt;

&lt;p&gt;重要アップデートをふまえ、以下の方針で新サンプルを作りました。&lt;/p&gt;

&lt;h3 id=&#34;チーム-複数端末での運用&#34;&gt;チーム、複数端末での運用&lt;/h3&gt;

&lt;p&gt;BackendにAzure Blobがサポートされたので、チーム、複数端末でstateの共有がしやすくなりました。ひとつのプロジェクトや環境を、チームメンバーがどこからでも、だけでなく、複数プロジェクトでのstate共有もできます。&lt;/p&gt;

&lt;h3 id=&#34;workspaceの導入&#34;&gt;Workspaceの導入&lt;/h3&gt;

&lt;p&gt;従来は /dev /stage /prodなど、環境別にコードを分けて管理していました。ゆえに環境間のコード同期が課題でしたが、TerraformのWorkspace機能で解決しやすくなりました。リソース定義で ${terraform.workspace} 変数を参照するように書けば、ひとつのコードで複数環境を扱えます。&lt;/p&gt;

&lt;p&gt;要件によっては、従来通り環境別にコードを分けた方がいいこともあるでしょう。環境間の差分が大きい、開発とデプロイのタイミングやライフサイクルが異なるなど、Workspaceが使いづらいケースもあるでしょう。その場合は無理せず従来のやり方で。今回のサンプルは「Workspaceを使ったら何ができるか？」を考えるネタにしてください。&lt;/p&gt;

&lt;h3 id=&#34;module-terraform-module-registryの活用&#34;&gt;Module、Terraform Module Registryの活用&lt;/h3&gt;

&lt;p&gt;TerraformのModuleはとても強力な機能なのですが、あーでもないこーでもないと、こだわり過ぎるとキリがありません。「うまいやり方」を見てから使いたいのが人情です。そこでTerraform Module Registryを活かします。お墨付きのVerifiedモジュールが公開されていますので、そのまま使うもよし、ライセンスを確認の上フォークするのもよし、です。&lt;/p&gt;

&lt;h3 id=&#34;リソースグループは環境ごとに準備し-管理をterraformから分離&#34;&gt;リソースグループは環境ごとに準備し、管理をTerraformから分離&lt;/h3&gt;

&lt;p&gt;AzureのリソースをプロビジョニングするTerraformコードの多くは、Azureのリソースグループを管理下に入れている印象です。すなわちdestroyするとリソースグループごとバッサリ消える。わかりやすいけど破壊的。&lt;/p&gt;

&lt;p&gt;TerraformはApp ServiceやACIなどPaaS、アプリ寄りのリソースも作成できるようになってきたので、アプリ開発者にTerraformを開放したいケースが増えてきています。dev環境をアプリ開発者とインフラ技術者がコラボして育て、そのコードをstageやprodにデプロイする、など。&lt;/p&gt;

&lt;p&gt;ところで。TerraformのWorkspaceは、こんな感じで簡単に切り替えられます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform workspace select prod
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;みなまで言わなくても分かりますね。悲劇はプラットフォーム側で回避しましょう。今回のサンプルではリソースグループをTerraform管理下に置かず、別途作成します。Terraformからはdata resourcesとしてRead Onlyで参照する実装です。環境別のリソースグループを作成し、dev環境のみアプリ開発者へ権限を付与します。&lt;/p&gt;

&lt;h2 id=&#34;サンプル解説&#34;&gt;サンプル解説&lt;/h2&gt;

&lt;p&gt;サンプルは&lt;a href=&#34;https://github.com/ToruMakabe/Terraform_Azure_Sample_201801&#34;&gt;GitHub&lt;/a&gt;に置きました。合わせてご確認ください。&lt;/p&gt;

&lt;p&gt;このコードをapplyすると、以下のリソースが出来上がります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;NGINX on Ubuntu Webサーバー VMスケールセット&lt;/li&gt;
&lt;li&gt;VMスケールセット向けロードバランサー&lt;/li&gt;
&lt;li&gt;踏み台サーバー&lt;/li&gt;
&lt;li&gt;上記を配置するネットワーク (仮想ネットワーク、サブネット、NSG)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;リポジトリ構造&#34;&gt;リポジトリ構造&lt;/h3&gt;

&lt;p&gt;サンプルのリポジトリ構造です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;├── modules
│   ├── computegroup
│   │   ├── main.tf
│   │   ├── os
│   │   │   ├── outputs.tf
│   │   │   └── variables.tf
│   │   ├── outputs.tf
│   │   └── variables.tf
│   ├── loadbalancer
│   │   ├── main.tf
│   │   ├── outputs.tf
│   │   └── variables.tf
│   └── network
│       ├── main.tf
│       ├── outputs.tf
│       └── variables.tf
└── projects
    ├── project_a
    │   ├── backend.tf
    │   ├── main.tf
    │   ├── outputs.tf
    │   └── variables.tf
    └── shared
        ├── backend.tf
        ├── main.tf
        ├── outputs.tf
        └── variables.tf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/modulesには&lt;a href=&#34;https://registry.terraform.io/browse?provider=azurerm&#34;&gt;Terraform Module Registry&lt;/a&gt;でVerifiedされているモジュールをフォークしたコードを入れました。フォークした理由は、リソースグループをdata resource化して参照のみにしたかったためです。&lt;/p&gt;

&lt;p&gt;そして、/projectsに2つのプロジェクトを作りました。プロジェクトでリソースとTerraformの実行単位、stateを分割します。sharedで土台となる仮想ネットワークと踏み台サーバー関連リソース、project_aでVMスケールセットとロードバランサーを管理します。&lt;/p&gt;

&lt;p&gt;このボリュームだとプロジェクトを分割する必然性は低いのですが、以下のケースにも対応できるように分けました。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;アプリ開発者がproject_a下でアプリ関連リソースに集中したい&lt;/li&gt;
&lt;li&gt;性能観点で分割したい (Terraformはリソース量につれて重くなりがち)&lt;/li&gt;
&lt;li&gt;有事を考慮し影響範囲を分割したい&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;プロジェクト間では、stateをremote_stateを使って共有します。サンプルではsharedで作成した仮想ネットワークのサブネットIDを&lt;a href=&#34;https://github.com/ToruMakabe/Terraform_Azure_Sample_201801/blob/master/projects/shared/outputs.tf#L1&#34;&gt;output&lt;/a&gt;し、project_aで参照できるよう&lt;a href=&#34;https://github.com/ToruMakabe/Terraform_Azure_Sample_201801/blob/master/projects/project_a/backend.tf.sample#L10&#34;&gt;定義&lt;/a&gt;しています。&lt;/p&gt;

&lt;h2 id=&#34;使い方&#34;&gt;使い方&lt;/h2&gt;

&lt;h3 id=&#34;前提&#34;&gt;前提&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Linux、WSL、macOSなどbash環境の実行例です&lt;/li&gt;
&lt;li&gt;SSHの公開鍵をTerraform実行環境の ~/.ssh/id_rsa.pub として準備してください&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;管理者向けのサービスプリンシパルを用意する&#34;&gt;管理者向けのサービスプリンシパルを用意する&lt;/h3&gt;

&lt;p&gt;インフラのプロビジョニングの主体者、管理者向けのサービスプリンシパルを用意します。リソースグループを作成できる権限が必要です。&lt;/p&gt;

&lt;p&gt;もしなければ作成します。組み込みロールでは、サブスクリプションに対するContributorが妥当でしょう。&lt;a href=&#34;https://www.terraform.io/docs/providers/azurerm/authenticating_via_service_principal.html&#34;&gt;Terraformのドキュメント&lt;/a&gt;も参考に。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az ad sp create-for-rbac --role=&amp;quot;Contributor&amp;quot; --scopes=&amp;quot;/subscriptions/SUBSCRIPTION_ID&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;出力されるappId、password、tenantを控えます。既存のサービスプリンシパルを使うのであれば、同情報を確認してください。&lt;/p&gt;

&lt;p&gt;なお参考までに。Azure Cloud ShellなどAzure CLIが導入されている環境では、特に認証情報の指定なしでterraform planやapply時にAzureのリソースにアクセスできます。TerraformがCLIの認証トークンを&lt;a href=&#34;https://github.com/terraform-providers/terraform-provider-azurerm/blob/master/azurerm/helpers/authentication/config.go&#34;&gt;使う&lt;/a&gt;からです。&lt;/p&gt;

&lt;p&gt;そしてBackendをAzure Blobとする場合、Blobにアクセスするためのキーが別途必要です。ですが、残念ながらBackendロジックでキーを得る際に、このトークンが&lt;a href=&#34;https://github.com/hashicorp/terraform/blob/master/backend/remote-state/azure/backend.go&#34;&gt;使われません&lt;/a&gt;。キーを明示することもできますが、Blobのアクセスキーは漏洩時のリカバリーが大変です。できれば直に扱いたくありません。&lt;/p&gt;

&lt;p&gt;サービスプリンシパル認証であれば、Azureリソースへのプロビジョニング、Backendアクセスどちらも&lt;a href=&#34;https://www.terraform.io/docs/backends/types/azurerm.html&#34;&gt;対応できます&lt;/a&gt;。これがこのサンプルでサービスプリンシパル認証を選んだ理由です。&lt;/p&gt;

&lt;h3 id=&#34;管理者の環境変数を設定する&#34;&gt;管理者の環境変数を設定する&lt;/h3&gt;

&lt;p&gt;Terraformが認証関連で必要な情報を環境変数で設定します。先ほど控えた情報を使います。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export ARM_SUBSCRIPTION_ID=&amp;quot;&amp;lt;your subscription id&amp;gt;&amp;quot;
export ARM_CLIENT_ID=&amp;quot;&amp;lt;your servicce principal appid&amp;gt;&amp;quot;
export ARM_CLIENT_SECRET=&amp;quot;&amp;lt;your service principal password&amp;gt;&amp;quot;
export ARM_TENANT_ID=&amp;quot;&amp;lt;your service principal tenant&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;workspaceを作る&#34;&gt;Workspaceを作る&lt;/h3&gt;

&lt;p&gt;開発(dev)/ステージング(stage)/本番(prod)、3つのWorkspaceを作る例です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform workspace new dev
terraform workspace new stage
terraform workspace new prod
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;リソースグループを作る&#34;&gt;リソースグループを作る&lt;/h3&gt;

&lt;p&gt;まずWorkspace別にリソースグループを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az group create -n tf-sample-dev-rg -l japaneast
az group create -n tf-sample-stage-rg -l japaneast
az group create -n tf-sample-prod-rg -l japaneast
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;リソースグループ名にはルールがあります。Workspace別にリソースグループを分離するため、Terraformのコードで ${terraform.workspace} 変数を使っているためです。この変数は実行時に評価されます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data &amp;quot;azurerm_resource_group&amp;quot; &amp;quot;resource_group&amp;quot; {
  name = &amp;quot;${var.resource_group_name}-${terraform.workspace}-rg&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;${var.resource_group_name} は接頭辞です。サンプルではvariables.tfで&amp;rdquo;tf-sample&amp;rdquo;と指定しています。&lt;/p&gt;

&lt;p&gt;次にBackend、state共有向けリソースグループを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az group create -n tf-sample-state-rg -l japaneast
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このリソースグループは、各projectのbackend.tfで指定しています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform {
  backend &amp;quot;azurerm&amp;quot; {
    resource_group_name  = &amp;quot;tf-sample-state-rg&amp;quot;
    storage_account_name = &amp;quot;&amp;lt;your storage account name&amp;gt;&amp;quot;
    container_name       = &amp;quot;tfstate-project-a&amp;quot;
    key                  = &amp;quot;terraform.tfstate&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最後にアプリ開発者がリソースグループtf-sample-dev-rg、tf-sample-state-rgへアクセスできるよう、アプリ開発者向けサービスプリンシパルを作成します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az ad sp create-for-rbac --role=&amp;quot;Contributor&amp;quot; --scopes &amp;quot;/subscriptions/&amp;lt;your subscription id&amp;gt;/resourceGroups/tf-sample-dev-rg&amp;quot; &amp;quot;/subscriptions/&amp;lt;your subscription id&amp;gt;/resourceGroups/tf-sample-state-rg&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;出力されるappId、password、tenantは、アプリ開発者向けに控えておきます。&lt;/p&gt;

&lt;h3 id=&#34;backendを準備する&#34;&gt;Backendを準備する&lt;/h3&gt;

&lt;p&gt;project別にストレージアカウントとコンテナーを作ります。tf-sample-state-rgに&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ストレージアカウント (名前は任意)&lt;/li&gt;
&lt;li&gt;コンテナー *2 (tfstate-project-a, tfstate-shared)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;を作ってください。GUIでもCLIでも、お好きなやり方で。&lt;/p&gt;

&lt;p&gt;その後、project_a/backend.tf.sample、shared/backend.tf.sampleをそれぞれbackend.tfにリネームし、先ほど作ったストレージアカウント名を指定します。以下はproject_a/backend.tf.sampleの例。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform {
  backend &amp;quot;azurerm&amp;quot; {
    resource_group_name  = &amp;quot;tf-sample-state-rg&amp;quot;
    storage_account_name = &amp;quot;&amp;lt;your storage account name&amp;gt;&amp;quot;
    container_name       = &amp;quot;tfstate-project-a&amp;quot;
    key                  = &amp;quot;terraform.tfstate&amp;quot;
  }
}

data &amp;quot;terraform_remote_state&amp;quot; &amp;quot;shared&amp;quot; {
  backend = &amp;quot;azurerm&amp;quot;

  config {
    resource_group_name  = &amp;quot;tf-sample-state-rg&amp;quot;
    storage_account_name = &amp;quot;&amp;lt;your storage account name&amp;gt;&amp;quot;
    container_name       = &amp;quot;tfstate-shared&amp;quot;
    key                  = &amp;quot;terraform.tfstateenv:${terraform.workspace}&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これで準備完了です。&lt;/p&gt;

&lt;h3 id=&#34;実行&#34;&gt;実行&lt;/h3&gt;

&lt;p&gt;Workspaceをdevに切り替えます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform workspace select dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;まずは土台となるリソースを作成するsharedから。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd shared
terraform init
terraform plan
terraform apply
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;土台となるリソースが作成されたら、次はproject_aを。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd ../project_a
terraform init
terraform plan
terraform apply
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここでは割愛しますが、dev向けサービスプリンシパルで認証しても、dev Workspaceではplan、apply可能です。&lt;/p&gt;

&lt;p&gt;dev Workspaceでコードが育ったら、stage/prod Workspaceに切り替えて実行します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;terraform workspace select stage
[以下devと同様の操作]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然、dev向けサービスプリンシパルで認証している場合は、stage/prodでのplan、apply、もちろんdestroyも失敗します。stage/prod リソースグループにアクセスする権限がないからです。&lt;/p&gt;

&lt;h2 id=&#34;参考情報&#34;&gt;参考情報&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/terraform/&#34;&gt;Terraform on Azure のドキュメント&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/terraform-providers/terraform-provider-azurerm/tree/master/examples&#34;&gt;サンプル集 on GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Windows上でLinux向けGoバイナリをDockerでビルドする</title>
      <link>http://torumakabe.github.io/post/golang_build_onwin_tolnx_docker/</link>
      <pubDate>Mon, 04 Dec 2017 22:00:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/golang_build_onwin_tolnx_docker/</guid>
      <description>

&lt;h2 id=&#34;小ネタです&#34;&gt;小ネタです&lt;/h2&gt;

&lt;p&gt;Goはクロスプラットフォーム開発しやすい言語なのですが、Windows上でLinux向けバイナリーをビルドするなら、gccが要ります。正直なところ入れたくありません。なのでDockerでやります。&lt;/p&gt;

&lt;h2 id=&#34;条件&#34;&gt;条件&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Docker for Windows

&lt;ul&gt;
&lt;li&gt;Linuxモード&lt;/li&gt;
&lt;li&gt;ドライブ共有&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;powershell窓で実行&#34;&gt;PowerShell窓で実行&lt;/h2&gt;

&lt;p&gt;ビルドしたいGoのソースがあるディレクトリで以下のコマンドを実行します。Linux向けバイナリーが同じディレクトリに出来ます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run --rm -it -e GOPATH=/go --mount type=bind,source=${env:GOPATH},target=/go --mount type=bind,source=${PWD},target=/work -w /work golang:1.9.2-alpine go build -a -tags netgo -installsuffix netgo -o yourapp_linux
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;golang:1.9.2-alpine DockerイメージはGOPATHに/goを&lt;a href=&#34;https://github.com/docker-library/golang/blob/0f5ee2149d00dcdbf48fca05acf582e45d8fa9a5/1.9/alpine3.6/Dockerfile&#34;&gt;設定して&lt;/a&gt;ビルドされていますが、念のため実行時にも設定&lt;/li&gt;
&lt;li&gt;-v オプションでのマウントは&lt;a href=&#34;https://docs.docker.com/engine/admin/volumes/bind-mounts/&#34;&gt;非推奨&lt;/a&gt;になったので &amp;ndash;mount で&lt;/li&gt;
&lt;li&gt;スタティックリンク&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Azure Blob アップローダーをGoで書いた、そしてその理由</title>
      <link>http://torumakabe.github.io/post/azblob_golang/</link>
      <pubDate>Tue, 28 Nov 2017 08:45:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/azblob_golang/</guid>
      <description>

&lt;h2 id=&#34;azure-blob-アップローダーをgoで書いた&#34;&gt;Azure Blob アップローダーをGoで書いた&lt;/h2&gt;

&lt;p&gt;ふたつほど理由があり、GolangでAzure Blobのファイルアップローダーを書きました。&lt;/p&gt;

&lt;h2 id=&#34;ひとつめの理由-sdkが新しくなったから&#34;&gt;ひとつめの理由: SDKが新しくなったから&lt;/h2&gt;

&lt;p&gt;最近公式ブログで&lt;a href=&#34;https://azure.microsoft.com/en-us/blog/preview-the-new-azure-storage-sdk-for-go-storage-sdks-roadmap/&#34;&gt;紹介された&lt;/a&gt;通り、Azure Storage SDK for Goが再設計され、プレビューが始まりました。GoはDockerやKubernetes、Terraformなど最近話題のプラットフォームやツールを書くのに使われており、ユーザーも増えています。再設計してもっと使いやすくしてちょ、という要望が多かったのも、うなずけます。&lt;/p&gt;

&lt;p&gt;ということで、新しいSDKで書いてみたかった、というのがひとつめの理由です。ローカルにあるファイルを読んでBlobにアップロードするコードは、こんな感じ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
	&amp;quot;context&amp;quot;
	&amp;quot;flag&amp;quot;
	&amp;quot;fmt&amp;quot;
	&amp;quot;log&amp;quot;
	&amp;quot;net/url&amp;quot;
	&amp;quot;os&amp;quot;

	&amp;quot;github.com/Azure/azure-storage-blob-go/2016-05-31/azblob&amp;quot;
)

var (
	accountName    string
	accountKey     string
	containerName  string
	fileName       string
	blockSize      int64
	blockSizeBytes int64
)

func init() {
	flag.StringVar(&amp;amp;accountName, &amp;quot;account-name&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;(Required) Storage Account Name&amp;quot;)
	flag.StringVar(&amp;amp;accountKey, &amp;quot;account-key&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;(Required) Storage Account Key&amp;quot;)
	flag.StringVar(&amp;amp;containerName, &amp;quot;c&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;(Required - short option) Blob Container Name&amp;quot;)
	flag.StringVar(&amp;amp;containerName, &amp;quot;container-name&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;(Required) Blob Container Name&amp;quot;)
	flag.StringVar(&amp;amp;fileName, &amp;quot;f&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;(Required - short option) Upload filename&amp;quot;)
	flag.StringVar(&amp;amp;fileName, &amp;quot;file&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;(Required) Upload filename&amp;quot;)
	flag.Int64Var(&amp;amp;blockSize, &amp;quot;b&amp;quot;, 4, &amp;quot;(Optional - short option) Blob Blocksize (MB) - From 1 to 100. Max filesize depends on this value. Max filesize = Blocksize * 50,000 blocks&amp;quot;)
	flag.Int64Var(&amp;amp;blockSize, &amp;quot;blocksize&amp;quot;, 4, &amp;quot;(Optional) Blob Blocksize (MB) - From 1 to 100. Max filesize depends on this value. Max filesize = Blocksize * 50,000 blocks&amp;quot;)
	flag.Parse()

	if (blockSize &amp;lt; 1) || (blockSize) &amp;gt; 100 {
		fmt.Println(&amp;quot;Blocksize must be from 1MB to 100MB&amp;quot;)
		os.Exit(1)
	}
	blockSizeBytes = blockSize * 1024 * 1024
}

func main() {
	file, err := os.Open(fileName)
	if err != nil {
		log.Fatal(err)
	}
	defer file.Close()
	fileSize, err := file.Stat()
	if err != nil {
		log.Fatal(err)
	}

	u, _ := url.Parse(fmt.Sprintf(&amp;quot;https://%s.blob.core.windows.net/%s/%s&amp;quot;, accountName, containerName, fileName))
	blockBlobURL := azblob.NewBlockBlobURL(*u, azblob.NewPipeline(azblob.NewSharedKeyCredential(accountName, accountKey), azblob.PipelineOptions{}))

	ctx := context.Background()

	fmt.Println(&amp;quot;Uploading block blob...&amp;quot;)
	putBlockList, err := azblob.UploadStreamToBlockBlob(ctx, file, fileSize.Size(), blockBlobURL,
		azblob.UploadStreamToBlockBlobOptions{
			BlockSize: blockSizeBytes,
			Progress: func(bytesTransferred int64) {
				fmt.Printf(&amp;quot;Uploaded %d of %d bytes.\n&amp;quot;, bytesTransferred, fileSize.Size())
			},
		})
	if err != nil {
		log.Fatal(err)
	}
	_ = putBlockList // Avoid compiler&#39;s &amp;quot;declared and not used&amp;quot; error

	fmt.Println(&amp;quot;Done&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以前のSDKと比較し、スッキリ書けるようになりました。進行状況もPipelineパッケージを使って、楽に取れるようになっています。ブロック分割のロジックを書く必要もなくなりました。ブロックサイズを指定すればOK。&lt;/p&gt;

&lt;p&gt;ちなみにファイルサイズがブロックサイズで割り切れると最終ブロックの転送がエラーになるバグを見つけたのですが、&lt;a href=&#34;https://github.com/Azure/azure-storage-blob-go/issues/8&#34;&gt;修正してもらった&lt;/a&gt;ので、次のリリースでは解決していると思います。&lt;/p&gt;

&lt;h2 id=&#34;ふたつめの理由-レガシー対応&#34;&gt;ふたつめの理由: レガシー対応&lt;/h2&gt;

&lt;p&gt;Blobのアップロードが目的であれば、Azure CLIをインストールすればOK。以上。なのですが、残念ながらそれができないケースがあります。&lt;/p&gt;

&lt;p&gt;たとえば。Azure CLI(2.0)はPythonで書かれています。なので、Pythonのバージョンや依存パッケージの兼ね合いで、「ちょっとそれウチのサーバーに入れるの？汚さないでくれる？ウチはPython2.6よ」と苦い顔をされることが、あるんですね。気持ちはわかります。立場の数だけ正義があります。Docker?その1歩半くらい前の話です。&lt;/p&gt;

&lt;p&gt;ですが、オンプレのシステムからクラウドにデータをアップロードして処理したい、なんていうニーズが急増している昨今、あきらめたくないわけであります。どうにか既存環境に影響なく入れられないものかと。そこでシングルバイナリーを作って、ポンと置いて、動かせるGoは尊いわけです。&lt;/p&gt;

&lt;p&gt;ファイルのアップロードだけでなく、Azureにちょっとした処理を任せたい、でもそれはいじりづらいシステムの上なのねん、って話は、結構多いんですよね。ということでシングルバイナリーを作って、ポンと置いて、動かせるGoは尊いわけです。大事なことなので2回書きました。&lt;/p&gt;

&lt;p&gt;C#やNode、Python SDKと比較してGoのそれはまだ物足りないところも多いわけですが、今後注目ということで地道に盛り上がっていこうと思います。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>自動化を愛するWindows使いへ Boxstarterのすすめ</title>
      <link>http://torumakabe.github.io/post/intro_boxstarter/</link>
      <pubDate>Fri, 13 Oct 2017 14:30:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/intro_boxstarter/</guid>
      <description>

&lt;h2 id=&#34;windowsのセットアップどうする問題&#34;&gt;Windowsのセットアップどうする問題&lt;/h2&gt;

&lt;p&gt;そろそろFall Creators Updateが来ますね。これを機にクリーンインストールしようか、という人も多いのではないでしょうか。端末って使っているうちに汚れていく宿命なので、わたしは定期的に「こうあるべき」という状態に戻します。年に2～3回はスッキリしたい派なので、アップデートはいいタイミングです。&lt;/p&gt;

&lt;p&gt;でもクリーンインストールすると、設定やアプリケーションの導入をGUIでやり直すのが、すこぶるめんどくせぇわけです。自動化したいですね。そこでBoxstarterをおすすめします。便利なのに、意外に知られていない。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://boxstarter.org/&#34;&gt;Boxstarter&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;わたしはマイクロソフトの仲間、Jessieの&lt;a href=&#34;https://blog.jessfraz.com/post/windows-for-linux-nerds/&#34;&gt;ポスト&lt;/a&gt;で知りました。サンクスJessie。&lt;/p&gt;

&lt;h2 id=&#34;boxstarterで出来ること&#34;&gt;Boxstarterで出来ること&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;シンプルなスクリプトで

&lt;ul&gt;
&lt;li&gt;Windowsの各種設定&lt;/li&gt;
&lt;li&gt;Chocolateyパッケージの導入&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;設定ファイルをネットワーク経由で読み込める

&lt;ul&gt;
&lt;li&gt;Gistから&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ベアメタルでも仮想マシンでもOK&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;実行手順&#34;&gt;実行手順&lt;/h2&gt;

&lt;p&gt;手順は&lt;a href=&#34;http://boxstarter.org/Learn/WebLauncher&#34;&gt;Boxstarterのサイト&lt;/a&gt;で紹介されています。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;スクリプトを作る&lt;/li&gt;
&lt;li&gt;Gistに上げる&lt;/li&gt;
&lt;li&gt;Boxstarterを導入する&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;PowerShell 3以降であれば&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;. { iwr -useb http://boxstarter.org/bootstrapper.ps1 } | iex; get-boxstarter -Force
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Gist上のスクリプトを指定して実行する&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;なお2017/10/13時点で、Boxstarterサイトのサンプルにはtypoがあるので注意 (-PackageNameオプション)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Install-BoxstarterPackage -PackageName &amp;quot;https://gist.githubusercontent.com/ToruMakabe/976ceab239ec930f8651cfd72087afac/raw/4fc77a1d08f078869962ae82233b2f8abc32d31f/boxstarter.txt&amp;quot; -DisableReboots
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上。&lt;/p&gt;

&lt;h2 id=&#34;サンプルスクリプト&#34;&gt;サンプルスクリプト&lt;/h2&gt;

&lt;p&gt;スクリプトは&lt;a href=&#34;https://gist.github.com/ToruMakabe/976ceab239ec930f8651cfd72087afac&#34;&gt;こんな感じ&lt;/a&gt;に書きます。&lt;/p&gt;

&lt;p&gt;ちなみに、わたしの環境です。こまごまとした設定やツールの導入はもちろん、Hyper-Vやコンテナ、Windows Subsystem for Linuxの導入も、一気にやっつけます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Learn more: http://boxstarter.org/Learn/WebLauncher

# Chocolateyパッケージがないもの、パッケージ更新が遅いものは別途入れます。メモです。
# Install manually (Ubuntu, VS, snip, Azure CLI/PS/Storage Explorer, Terraform, Go, 1Password 6, Driver Management Tool)

#---- TEMPORARY ---
Disable-UAC

#--- Fonts ---
choco install inconsolata
  
#--- Windows Settings ---
# 可能な設定はここで確認 --&amp;gt; [Boxstarter WinConfig Features](http://boxstarter.org/WinConfig)
Disable-GameBarTips

Set-WindowsExplorerOptions -EnableShowHiddenFilesFoldersDrives -EnableShowFileExtensions
Set-TaskbarOptions -Size Small -Dock Bottom -Combine Full -Lock

Set-ItemProperty -Path HKCU:\Software\Microsoft\Windows\CurrentVersion\Explorer\Advanced -Name NavPaneShowAllFolders -Value 1

#--- Windows Subsystems/Features ---
choco install Microsoft-Hyper-V-All -source windowsFeatures
choco install Microsoft-Windows-Subsystem-Linux -source windowsfeatures
choco install containers -source windowsfeatures

#--- Tools ---
choco install git.install
choco install yarn
choco install sysinternals
choco install 7zip

#--- Apps ---
choco install googlechrome
choco install docker-for-windows
choco install microsoft-teams
choco install slack
choco install putty
choco install visualstudiocode

#--- Restore Temporary Settings ---
Enable-UAC
Enable-MicrosoftUpdate
Install-WindowsUpdate -acceptEula
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;便利。&lt;/p&gt;

&lt;p&gt;ちなみにわたしはドキュメント類はOneDrive、コードはプライベートGit/GitHub、エディタの設定はVisual Studio Code &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=Shan.code-settings-sync&#34;&gt;Settings Sync拡張&lt;/a&gt;を使っているので、Boxstarterと合わせ、 環境の再現は2～3時間もあればできます。最近、バックアップからのリストアとか、してないです。&lt;/p&gt;

&lt;p&gt;新しい端末の追加もすぐできるので、物欲が捗るという副作用もあります。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Azure VPN Gateway Active/Active構成のスループット検証(リージョン内)</title>
      <link>http://torumakabe.github.io/post/azure_vpngw_act_act_perf/</link>
      <pubDate>Sun, 08 Oct 2017 10:30:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/azure_vpngw_act_act_perf/</guid>
      <description>

&lt;h2 id=&#34;動機&#34;&gt;動機&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kogelog.com/&#34;&gt;焦げlogさん&lt;/a&gt;で、とても興味深いエントリを拝見しました。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kogelog.com/2017/10/06/20171006-01/&#34;&gt;Azure VPN ゲートウェイをアクティブ/アクティブ構成した場合にスループットが向上するのか検証してみました&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;確かにActive/Active構成にはスループット向上を期待したくなります。その伸びが測定されており、胸が熱くなりました。ですが、ちょっと気になったのは&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;※それと、VpnGw3 よりも VpnGw2 のほうがスループットがよかったのが一番の謎ですが…&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ここです。VPN GatewayのSKU、VpnGw3とVpnGw2には小さくない価格差があり、その基準はスループットです。ここは現状を把握しておきたいところ。すごく。&lt;/p&gt;

&lt;p&gt;そこで、焦げlogさんの検証パターンの他に、追加で検証しました。それは同一リージョン内での測定です。リージョン内でVPNを張るケースはまれだと思いますが、リージョンが分かれることによる&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;遅延&lt;/li&gt;
&lt;li&gt;リージョン間通信に関するサムシング&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;を除き、VPN Gateway自身のスループットを測定したいからです。焦げlogさんの測定は東日本/西日本リージョン間で行われたので、その影響を確認する価値はあるかと考えました。&lt;/p&gt;

&lt;h2 id=&#34;検証方針&#34;&gt;検証方針&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;同一リージョン(東日本)に、2つのVNETを作る&lt;/li&gt;
&lt;li&gt;それぞれのVNETにVPN Gatewayを配置し、接続する&lt;/li&gt;
&lt;li&gt;比較しやすいよう、焦げlogさんの検証と条件を合わせる

&lt;ul&gt;
&lt;li&gt;同じ仮想マシンサイズ: DS3_V2&lt;/li&gt;
&lt;li&gt;同じストレージ: Premium Storage Managed Disk&lt;/li&gt;
&lt;li&gt;同じOS: Ubuntu 16.04&lt;/li&gt;
&lt;li&gt;同じツール: ntttcp&lt;/li&gt;
&lt;li&gt;同じパラメータ: ntttcp -r -m 16,*,&lt;IP&gt; -t 300&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;送信側 VNET1 -&amp;gt; 受信側 VNET2 のパターンに絞る&lt;/li&gt;
&lt;li&gt;スループットのポテンシャルを引き出す検証はしない&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;結果&#34;&gt;結果&lt;/h2&gt;

&lt;h3 id=&#34;vpngw1-650mbps&#34;&gt;VpnGW1(650Mbps)&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;パターン　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;送信側GW構成　　　　　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;受信側GW構成　　　　　　　　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;送信側スループット　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　受信側スループット&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　スループット平均&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　パターン1との比較&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン1　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;677.48Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;676.38Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;676.93Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン2　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;674.34Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;673.85Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;674.10Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;99%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン3　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;701.19Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;699.91Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;700.55Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;103%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;vpngw2-1gbps&#34;&gt;VpnGW2(1Gbps)&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;パターン　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;送信側GW構成　　　　　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;受信側GW構成　　　　　　　　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;送信側スループット　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　受信側スループット&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　スループット平均&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　パターン1との比較&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン1　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;813.09Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;805.60Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;809.35Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン2　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.18Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.18Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.18Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン3　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.02Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;256%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;vpngw3-1-25gbps&#34;&gt;VpnGW3(1.25Gbps)&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;パターン　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;送信側GW構成　　　　　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;受信側GW構成　　　　　　　　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;送信側スループット　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　受信側スループット&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　スループット平均&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　パターン1との比較&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン1　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;958.56Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;953.72Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;956.14Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン2　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Stb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.39Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.39Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.39Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;パターン3　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Act/Act&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.19Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.19Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.19Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;234%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;sku視点-パターン1-act-stb-to-act-stb&#34;&gt;SKU視点 パターン1(Act/Stb to Act/Stb)&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;SKU　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　スループット平均&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　VpnGw1との比較&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw1　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;676.93Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw2　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;809.35Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;119%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw3　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;956.14Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;141%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;sku視点-パターン2-act-stb-to-act-act&#34;&gt;SKU視点 パターン2(Act/Stb to Act/Act)&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;SKU　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　スループット平均&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　VpnGw1との比較&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw1　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;674.10Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw2　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.18Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;179%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw3　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.39Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;211%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;sku視点-パターン3-act-act-to-act-act&#34;&gt;SKU視点 パターン3(Act/Act to Act/Act)&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;SKU　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　スループット平均&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　VpnGw1との比較&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw1　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;700.55Mbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw2　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;297%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;VpnGw3　&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.19Gbps&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;320%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;考察と推奨&#34;&gt;考察と推奨&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;リージョン間の遅延やサムシングを除くと、SKUによるGatewayのスループット差は測定できる

&lt;ul&gt;
&lt;li&gt;Act/Actでないパターン1(Act/Stb to Act/Stb)で、その差がわかる&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;公式ドキュメントの通り、GatewayのAct/Act構成は可用性向上が目的であるため、スループットの向上はボーナスポイントと心得る

&lt;ul&gt;
&lt;li&gt;期待しちゃうのが人情ではありますが&lt;/li&gt;
&lt;li&gt;VpnGw2がコストパフォーマンス的に最適という人が多いかもしれませんね 知らんけど&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Azure Event GridでBlobイベントを拾う</title>
      <link>http://torumakabe.github.io/post/azure_blobevent/</link>
      <pubDate>Tue, 05 Sep 2017 12:00:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/azure_blobevent/</guid>
      <description>

&lt;h2 id=&#34;event-gridがblobに対応&#34;&gt;Event GridがBlobに対応&lt;/h2&gt;

&lt;p&gt;Event GridがBlobのイベントを拾えるように&lt;a href=&#34;https://azure.microsoft.com/en-us/blog/announcing-azure-blob-storage-events-preview/&#34;&gt;なりました&lt;/a&gt;。まだ申請が必要なプライベートプレビュー段階ですが、使い勝手の良いサービスに育つ予感がします。このたび検証する機会があったので、共有を。&lt;/p&gt;

&lt;p&gt;プレビュー中なので、今後仕様が変わるかもしれないこと、不具合やメンテナンス作業の可能性などは、ご承知おきください。&lt;/p&gt;

&lt;h2 id=&#34;event-gridがblobに対応して何がうれしいか&#34;&gt;Event GridがBlobに対応して何がうれしいか&lt;/h2&gt;

&lt;p&gt;Event Gridは、Azureで発生した様々なイベントを検知してWebhookで通知するサービスです。カスタムトピックも作成できます。&lt;/p&gt;

&lt;p&gt;イベントの発生元をPublisherと呼びますが、このたびPublisherとしてAzureのBlobがサポートされました。Blobの作成、削除イベントを検知し、Event GridがWebhookで通知します。通知先はHandlerと呼びます。Publisherとそこで拾うイベント、Handlerを紐づけるのがSubscriptionです。Subscriptionにはフィルタも定義できます。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/ff3644c9-58ab-4729-8939-66a83ab0605d.png&#34; alt=&#34;コンセプト&#34; title=&#34;Concept&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Event Gridに期待する理由はいくつかあります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;フィルタ

&lt;ul&gt;
&lt;li&gt;特定のBlobコンテナーにあるjpegファイルの作成イベントのみで発火させる、なんてことができます&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;信頼性

&lt;ul&gt;
&lt;li&gt;リトライ機能があるので、Handlerが一時的に黙ってしまっても対応できます&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;スケールと高スループット

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/ja-jp/azure/azure-functions/functions-bindings-storage-blob#blob-storage-triggers-and-bindings&#34;&gt;Azure Functions Blobトリガー&lt;/a&gt;のようにHandler側で定期的にスキャンする必要がありません。これまではファイル数が多いとつらかった&lt;/li&gt;
&lt;li&gt;具体的な数値はプレビュー後に期待しましょう&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ファンアウト

&lt;ul&gt;
&lt;li&gt;ひとつのイベントを複数のHandlerに紐づけられます&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Azureの外やサードパーティーとの連携

&lt;ul&gt;
&lt;li&gt;Webhookでシンプルにできます&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;前提条件&#34;&gt;前提条件&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Publisherに設定できるストレージアカウントはBlobストレージアカウントのみです。汎用ストレージアカウントは対応していません&lt;/li&gt;
&lt;li&gt;現時点ではWest Central USリージョンのみで提供しています&lt;/li&gt;
&lt;li&gt;プライベートプレビューは申請が必要です&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Azure CLIの下記コマンドでプレビューに申請できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az provider register --namespace  Microsoft.EventGrid
az feature register --name storageEventSubscriptions --namespace Microsoft.EventGrid
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のコマンドで確認し、statusが&amp;rdquo;Registered&amp;rdquo;であれば使えます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az feature show --name storageEventSubscriptions --namespace Microsoft.EventGrid
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;使い方&#34;&gt;使い方&lt;/h2&gt;

&lt;p&gt;ストレージアカウントの作成からSubscription作成までの流れを追ってみましょう。&lt;/p&gt;

&lt;p&gt;リソースグループを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az group create -n blobeventpoc-rg -l westcentralus
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Blobストレージアカウントを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az storage account create -n blobeventpoc01 -l westcentralus -g blobeventpoc-rg --sku Standard_LRS --kind BlobStorage --access-tier Hot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ではいよいよEvent GridのSubscriptionを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az eventgrid resource event-subscription create --endpoint https://requestb.in/y4jgj2x0 -n blobeventpocsub-jpg --prov
ider-namespace Microsoft.Storage --resource-type storageAccounts --included-event-types Microsoft.Storage.BlobCreated
-g blobeventpoc-rg --resource-name blobeventpoc01 --subject-ends-with jpg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下はパラメーターの補足です。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&amp;ndash;endpoint

&lt;ul&gt;
&lt;li&gt;Handlerのエンドポイントを指定します。ここではテストのために&lt;a href=&#34;https://requestb.in/&#34;&gt;RequestBin&lt;/a&gt;に作ったエンドポイントを指定します&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&amp;ndash;included-event-types

&lt;ul&gt;
&lt;li&gt;イベントの種類をフィルタします。Blobの削除イベントは不要で、作成のみ拾いたいため、Microsoft.Storage.BlobCreatedを指定します&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&amp;ndash;subject-ends-with

&lt;ul&gt;
&lt;li&gt;対象ファイルをフィルタします。Blob名の末尾文字列がjpgであるBlobのみイベントの対象にしました&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;では作成したストレージアカウントにBlobコンテナーを作成し、jpgファイルを置いてみましょう。テストには&lt;a href=&#34;https://azure.microsoft.com/ja-jp/features/storage-explorer/&#34;&gt;Azure Storage Explorer&lt;/a&gt;が便利です。&lt;/p&gt;

&lt;p&gt;RequestBinにWebhookが飛び、中身を見られます。スキーマの確認は&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-overview#event-schema&#34;&gt;こちら&lt;/a&gt;から。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[{
  &amp;quot;topic&amp;quot;: &amp;quot;/subscriptions/xxxxx-xxxxx-xxxxx-xxxxx/resourceGroups/blobeventpoc-rg/providers/Microsoft.Storage/storageAccounts/blobeventpoc01&amp;quot;,
  &amp;quot;subject&amp;quot;: &amp;quot;/blobServices/default/containers/images/blobs/handsomeyoungman.jpg&amp;quot;,
  &amp;quot;eventType&amp;quot;: &amp;quot;Microsoft.Storage.BlobCreated&amp;quot;,
  &amp;quot;eventTime&amp;quot;: &amp;quot;2017-09-02T02:25:15.2635962Z&amp;quot;,
  &amp;quot;id&amp;quot;: &amp;quot;f3ff6b96-001e-001d-6e92-23bdea0684d2&amp;quot;,
  &amp;quot;data&amp;quot;: {
    &amp;quot;api&amp;quot;: &amp;quot;PutBlob&amp;quot;,
    &amp;quot;clientRequestId&amp;quot;: &amp;quot;f3cab560-8f85-11e7-bad1-53b58c70ab53&amp;quot;,
    &amp;quot;requestId&amp;quot;: &amp;quot;f3ff6b96-001e-001d-6e92-23bdea000000&amp;quot;,
    &amp;quot;eTag&amp;quot;: &amp;quot;0x8D4F1A9D8A6703A&amp;quot;,
    &amp;quot;contentType&amp;quot;: &amp;quot;image/jpeg&amp;quot;,
    &amp;quot;contentLength&amp;quot;: 42497,
    &amp;quot;blobType&amp;quot;: &amp;quot;BlockBlob&amp;quot;,
    &amp;quot;url&amp;quot;: &amp;quot;https://blobeventpoc01.blob.core.windows.net/images/handsomeyoungman.jpg&amp;quot;,
    &amp;quot;sequencer&amp;quot;: &amp;quot;0000000000000BAB0000000000060986&amp;quot;,
    &amp;quot;storageDiagnostics&amp;quot;: {
      &amp;quot;batchId&amp;quot;: &amp;quot;f3a538cf-5b88-4bbf-908a-20a37c65e238&amp;quot;
    }
  }
}]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;.jpgだけじゃなくて.jpegも使われるかもしれませんね。ということで、エンドポイントが同じでフィルタ定義を変えたSubscriptionを追加します。&amp;ndash;subject-ends-withをjpegとします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az eventgrid resource event-subscription create --endpoint https://requestb.in/y4jgj2x0 -n blobeventpocsub-jpeg --pro
vider-namespace Microsoft.Storage --resource-type storageAccounts --included-event-types Microsoft.Storage.BlobCreated -
g blobeventpoc-rg --resource-name blobeventpoc01 --subject-ends-with jpeg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;すると、拡張子.jpegのファイルをアップロードしても発火しました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[{
  &amp;quot;topic&amp;quot;: &amp;quot;/subscriptions/xxxxx-xxxxx-xxxxx-xxxxx/resourceGroups/blobeventpoc-rg/providers/Microsoft.Storage/storageAccounts/blobeventpoc01&amp;quot;,
  &amp;quot;subject&amp;quot;: &amp;quot;/blobServices/default/containers/images/blobs/handsomeyoungman.jpeg&amp;quot;,
  &amp;quot;eventType&amp;quot;: &amp;quot;Microsoft.Storage.BlobCreated&amp;quot;,
  &amp;quot;eventTime&amp;quot;: &amp;quot;2017-09-02T02:36:33.827967Z&amp;quot;,
  &amp;quot;id&amp;quot;: &amp;quot;e8b036ee-001e-00e7-4994-23740d06225b&amp;quot;,
  &amp;quot;data&amp;quot;: {
    &amp;quot;api&amp;quot;: &amp;quot;PutBlob&amp;quot;,
    &amp;quot;clientRequestId&amp;quot;: &amp;quot;883ff7e0-8f87-11e7-bad1-53b58c70ab53&amp;quot;,
    &amp;quot;requestId&amp;quot;: &amp;quot;e8b036ee-001e-00e7-4994-23740d000000&amp;quot;,
    &amp;quot;eTag&amp;quot;: &amp;quot;0x8D4F1AB6D1B24F6&amp;quot;,
    &amp;quot;contentType&amp;quot;: &amp;quot;image/jpeg&amp;quot;,
    &amp;quot;contentLength&amp;quot;: 42497,
    &amp;quot;blobType&amp;quot;: &amp;quot;BlockBlob&amp;quot;,
    &amp;quot;url&amp;quot;: &amp;quot;https://blobeventpoc01.blob.core.windows.net/images/handsomeyoungman.jpeg&amp;quot;,
    &amp;quot;sequencer&amp;quot;: &amp;quot;0000000000000BAB0000000000060D42&amp;quot;,
    &amp;quot;storageDiagnostics&amp;quot;: {
      &amp;quot;batchId&amp;quot;: &amp;quot;9ec5c091-061d-4111-ad82-52d9803ce373&amp;quot;
    }
  }
}]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;azure-functionsに画像リサイズファンクションを作って連携してみる&#34;&gt;Azure Functionsに画像リサイズファンクションを作って連携してみる&lt;/h2&gt;

&lt;p&gt;Gvent Grid側の動きが確認できたので、サンプルアプリを作って検証してみましょう。Azure Functions上に画像ファイルのサイズを変えるHandlerアプリを作ってみます。&lt;/p&gt;

&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;

&lt;p&gt;当初想定したのは、ひとつのファンクションで、トリガーはEventGrid、入出力バインドにBlob、という作りでした。ですが、以下のように設計を変えました。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ToruMakabe/Images/master/blobevent-function-bindings.png&#34; alt=&#34;Bindings&#34; title=&#34;Bindings&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Using &lt;a href=&#34;https://functions-visualizer.azurewebsites.net/&#34;&gt;Azure Functions Bindings Visualizer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;その理由はEvent Grid Blobイベントのペイロードです。Blobファイル名がURLで渡されます。Azure FunctionsのBlob入出力バインド属性、&amp;rdquo;path&amp;rdquo;にURLは使えません。使えるのはコンテナー名+ファイル名です。&lt;/p&gt;

&lt;p&gt;入出力バインドを使わず、アプリのロジック内でStorage SDKを使って入出力してもいいのですが、Azure Functionsの魅力のひとつは宣言的にトリガーとバインドを定義し、アプリをシンプルに書けることなので、あまりやりたくないです。&lt;/p&gt;

&lt;p&gt;そこでイベントを受けてファイル名を取り出してQueueに入れるファンクションと、そのQueueをトリガーに画像をリサイズするファンクションに分けました。&lt;/p&gt;

&lt;p&gt;なお、この悩みはAzureの開発チームも認識しており、Functions側で対応する方針とのことです。&lt;/p&gt;

&lt;h3 id=&#34;handler&#34;&gt;Handler&lt;/h3&gt;

&lt;p&gt;C#(csx)で、Event GridからのWebhookを受けるHandlerを作ります。PublisherがBlobの場合、ペイロードにBlobのURLが入っていますので、そこからファイル名を抽出します。そして、そのファイル名をQueueに送ります。ファンクション名はBlobEventHandlerとしました。なおEventGridTriggerテンプレートは、現在は[試験段階]シナリオに入っています。&lt;/p&gt;

&lt;p&gt;[run.csx]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#r &amp;quot;Newtonsoft.json&amp;quot;
using Microsoft.Azure.WebJobs.Extensions.EventGrid;

public static void Run(EventGridEvent eventGridEvent, out string outputQueueItem, TraceWriter log)
{
    string imageUrl = eventGridEvent.Data[&amp;quot;url&amp;quot;].ToString();
    outputQueueItem = System.IO.Path.GetFileName(imageUrl);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Event GridのWebJobs拡張向けパッケージを指定します。&lt;/p&gt;

&lt;p&gt;[project.json]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
&amp;quot;frameworks&amp;quot;: {
  &amp;quot;net46&amp;quot;:{
    &amp;quot;dependencies&amp;quot;: {
      &amp;quot;Microsoft.Azure.WebJobs.Extensions.EventGrid&amp;quot;: &amp;quot;1.0.0-beta1-10006&amp;quot;
    }
  }
 }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;トリガーとバインドは以下の通りです。&lt;/p&gt;

&lt;p&gt;[function.json]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;bindings&amp;quot;: [
    {
      &amp;quot;type&amp;quot;: &amp;quot;eventGridTrigger&amp;quot;,
      &amp;quot;name&amp;quot;: &amp;quot;eventGridEvent&amp;quot;,
      &amp;quot;direction&amp;quot;: &amp;quot;in&amp;quot;
    },
    {
      &amp;quot;type&amp;quot;: &amp;quot;queue&amp;quot;,
      &amp;quot;name&amp;quot;: &amp;quot;outputQueueItem&amp;quot;,
      &amp;quot;queueName&amp;quot;: &amp;quot;imagefilename&amp;quot;,
      &amp;quot;connection&amp;quot;: &amp;quot;AzureWebJobsStorage&amp;quot;,
      &amp;quot;direction&amp;quot;: &amp;quot;out&amp;quot;
    }
  ],
  &amp;quot;disabled&amp;quot;: false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;resizer&#34;&gt;Resizer&lt;/h3&gt;

&lt;p&gt;Queueをトリガーに、Blobから画像ファイルを取り出し、縮小、出力するファンクションを作ります。ファンクション名はResizerとしました。&lt;/p&gt;

&lt;p&gt;[run.csx]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;using ImageResizer;

public static void Run(string myQueueItem, Stream inputBlob, Stream outputBlob, TraceWriter log)
{
  var imageBuilder = ImageResizer.ImageBuilder.Current;
  var size = imageDimensionsTable[ImageSize.Small];

  imageBuilder.Build(inputBlob, outputBlob,
    new ResizeSettings(size.Item1, size.Item2, FitMode.Max, null), false);

}

public enum ImageSize
{
  Small
}

private static Dictionary&amp;lt;ImageSize, Tuple&amp;lt;int, int&amp;gt;&amp;gt; imageDimensionsTable = new Dictionary&amp;lt;ImageSize, Tuple&amp;lt;int, int&amp;gt;&amp;gt;()
{
  { ImageSize.Small, Tuple.Create(100, 100) }
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ImageResizerのパッケージを指定します。&lt;/p&gt;

&lt;p&gt;[project.json]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
&amp;quot;frameworks&amp;quot;: {
  &amp;quot;net46&amp;quot;:{
    &amp;quot;dependencies&amp;quot;: {
      &amp;quot;ImageResizer&amp;quot;: &amp;quot;4.1.9&amp;quot;
    }
  }
 }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;トリガーとバインドは以下の通りです。{QueueTrigger}メタデータで、QueueのペイロードをBlobのpathに使います。ペイロードにはファイル名が入っています。&lt;/p&gt;

&lt;p&gt;また、画像を保存するBlobストレージアカウントの接続文字列は、環境変数BLOB_IMAGESへ事前に設定しています。なお、リサイズ後の画像を格納するBlobコンテナーは、&amp;rdquo;images-s&amp;rdquo;として別途作成しました。コンテナー&amp;rdquo;images&amp;rdquo;をイベントの発火対象コンテナーとして、Subscriptionにフィルタを定義したいからです。&lt;/p&gt;

&lt;p&gt;[function.json]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;bindings&amp;quot;: [
    {
      &amp;quot;name&amp;quot;: &amp;quot;myQueueItem&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;queueTrigger&amp;quot;,
      &amp;quot;direction&amp;quot;: &amp;quot;in&amp;quot;,
      &amp;quot;queueName&amp;quot;: &amp;quot;imagefilename&amp;quot;,
      &amp;quot;connection&amp;quot;: &amp;quot;AzureWebJobsStorage&amp;quot;
    },
    {
      &amp;quot;name&amp;quot;: &amp;quot;inputBlob&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;blob&amp;quot;,
      &amp;quot;path&amp;quot;: &amp;quot;images/{QueueTrigger}&amp;quot;,
      &amp;quot;connection&amp;quot;: &amp;quot;BLOB_IMAGES&amp;quot;,
      &amp;quot;direction&amp;quot;: &amp;quot;in&amp;quot;
    },
    {
      &amp;quot;name&amp;quot;: &amp;quot;outputBlob&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;blob&amp;quot;,
      &amp;quot;path&amp;quot;: &amp;quot;images-s/{QueueTrigger}&amp;quot;,
      &amp;quot;connection&amp;quot;: &amp;quot;BLOB_IMAGES&amp;quot;,
      &amp;quot;direction&amp;quot;: &amp;quot;out&amp;quot;
    }
  ],
  &amp;quot;disabled&amp;quot;: false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Handlerの準備が整いました。最後にEvent GridのSubscriptionを作成します。Azure FunctionsのBlobEventHandlerのトークン付きエンドポイントは、ポータルの[統合]で確認できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az eventgrid resource event-subscription create --endpoint &amp;quot;https://blobeventpoc.azurewebsites.net/admin/exte
nsions/EventGridExtensionConfig?functionName=BlobEventHandler&amp;amp;code=tokenTOKEN1234567890==&amp;quot; -n blobeventpocsub-jpg --provider-namespace Microsoft.Storage --resource-type storageAccounts --included-event-types &amp;quot;Microsoft.Storage.BlobCreated&amp;quot; -g blobeventpoc-rg --resource-name blobeventpoc01 --subject-begins-with &amp;quot;/blobServices/default/containers/images/&amp;quot;  --subject-ends-with jpg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これで、コンテナー&amp;rdquo;images&amp;rdquo;にjpgファイルがアップロードされると、コンテナー&amp;rdquo;images-s&amp;rdquo;に、リサイズされた同じファイル名の画像ファイルが出来上がります。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Azureでグローバルにデータをコピーするとどのくらい時間がかかるのか</title>
      <link>http://torumakabe.github.io/post/azureblobcopy_perf/</link>
      <pubDate>Tue, 13 Jun 2017 17:00:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/azureblobcopy_perf/</guid>
      <description>

&lt;h2 id=&#34;ファイルコピーの需要は根強い&#34;&gt;ファイルコピーの需要は根強い&lt;/h2&gt;

&lt;p&gt;グローバルでAzureを使うとき、データをどうやって同期、複製するかは悩みの種です。Cosmos DBなどリージョン間でデータ複製してくれるサービスを使うのが、楽ですし、おすすめです。&lt;/p&gt;

&lt;p&gt;でも、ファイルコピーを無くせないもろもろの事情もあります。となると、「地球の裏側へのファイルコピーに、どんだけ時間かかるのよ」は、課題です。&lt;/p&gt;

&lt;h2 id=&#34;調べてみた&#34;&gt;調べてみた&lt;/h2&gt;

&lt;p&gt;ということで、いくつかのパターンで調べたので参考までに。測定環境は以下の通り。&lt;/p&gt;

&lt;h3 id=&#34;ツールと実行環境&#34;&gt;ツールと実行環境&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;AzCopy 6.1.0&lt;/li&gt;
&lt;li&gt;Azure PowerShell 4.1.0&lt;/li&gt;
&lt;li&gt;Windows 10 1703&lt;/li&gt;
&lt;li&gt;ThinkPad X1 Carbon 2017, Core i7-7600U 2.8GHz, 16GB Memory&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;アクセス回線パターン&#34;&gt;アクセス回線パターン&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;一般的な回線&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;自宅(川崎)&lt;/li&gt;
&lt;li&gt;OCN光 100M マンションタイプ&lt;/li&gt;
&lt;li&gt;宅内は802.11ac(5GHz)&lt;/li&gt;
&lt;li&gt;川崎でアクセス回線に入り、横浜(保土ヶ谷)の局舎からインターネットへ&lt;/li&gt;
&lt;li&gt;ゲートウェイ名から推測&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;いい感じの回線&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;日本マイクロソフト 品川オフィス&lt;/li&gt;
&lt;li&gt;1Gbps 有線&lt;/li&gt;
&lt;li&gt;Azureデータセンターへ「ネットワーク的に近くて広帯域」&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;コピーするファイル&#34;&gt;コピーするファイル&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;総容量: 約60GB

&lt;ul&gt;
&lt;li&gt;6160ファイル&lt;/li&gt;
&lt;li&gt;1MB * 5000, 10MB * 1000, 100MB * 100, 500MB * 50, 1000MB * 10&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Linux fallocateコマンドで作成&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ファイル形式パターン&#34;&gt;ファイル形式パターン&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;ファイル、Blobそのまま送る (6160ファイル)&lt;/li&gt;
&lt;li&gt;ディスクイメージで送る (1ファイル)

&lt;ul&gt;
&lt;li&gt;Managed Diskとしてアタッチした100GBの領域にファイルシステムを作成し、6160ファイルを配置&lt;/li&gt;
&lt;li&gt;転送前にデタッチ、エクスポート(Blob SAS形式)&lt;/li&gt;
&lt;li&gt;AzCopyではなくAzure PowerShellでコピー指示 (AzCopyにBlob SAS指定オプションが見当たらなかった)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;対象のazureリージョン&#34;&gt;対象のAzureリージョン&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;東日本 (マスター、複製元と位置づける)&lt;/li&gt;
&lt;li&gt;米国中南部 (太平洋越え + 米国内を見たい)&lt;/li&gt;
&lt;li&gt;ブラジル南部&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;転送パターン&#34;&gt;転送パターン&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;ユーザー拠点の端末からAzureリージョン: AzCopy Upload&lt;/li&gt;
&lt;li&gt;Azureリージョン間 (Storage to Storage)

&lt;ul&gt;
&lt;li&gt;ファイル: AzCopy Copy&lt;/li&gt;
&lt;li&gt;イメージ: PowerShell Start-AzureStorageBlobCopy&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;結果&#34;&gt;結果&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;形式　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;コピー元　　　　　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;コピー先　　　　　　　　&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;コマンド　&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　並列数&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;　実行時間(時:分:秒)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;自宅&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 東日本&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Upload&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;07:55:22&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;自宅&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 米国中南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Upload&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10:22:30&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;自宅&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure ブラジル南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Upload&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12:46:37&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;オフィス&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 東日本&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Upload&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;00:20:47&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;オフィス&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 米国中南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Upload&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;00:45.11&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;オフィス&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure ブラジル南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Upload&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;02:07.58&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 東日本&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 米国中南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Copy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;N/A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;00:28:55&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;イメージ　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 東日本&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 米国中南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PowerShell&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;N/A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;00:11:11&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ファイル　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 東日本&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure ブラジル南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;AzCopy Copy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;N/A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;00.25:33&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;イメージ　&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure 東日本&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Azure ブラジル南部&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PowerShell&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;N/A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;00.09:20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;考察&#34;&gt;考察&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;アクセス回線の差が大きく影響&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;自宅パターンでプロバイダから帯域制限されていたかは不明 (自宅からAzure東日本まで16Mbpsくらいは出た)&lt;/li&gt;
&lt;li&gt;アクセス回線が細い場合はユーザー拠点から「まとめて」送らないほうがいい&lt;/li&gt;
&lt;li&gt;こまめに送る&lt;/li&gt;
&lt;li&gt;Azure内でデータを生成する&lt;/li&gt;
&lt;li&gt;もしくはExpressRouteを引く (自宅で、とは言っていない)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;アクセス回線が細い場合、AzCopy Uploadの並列数を下げる&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AzCopyのデフォルト並列数は実行環境のCPUコア数 *8だが、今回実施した端末での並列数(4コア * 8 = 32)ではかえって性能が劣化した&lt;/li&gt;
&lt;li&gt;アクセス回線に合わせて並列数は調整する&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Azureのリージョン間コピーは早い&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Azureバックボーンを通るから&lt;/li&gt;
&lt;li&gt;端末よりAzureストレージのほうがリソース的に強いし負荷分散しているから&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;地理的な距離感覚だけで考えてはダメ&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;地理的な近さではなく、ネットワーク的な近さと太さ&lt;/li&gt;
&lt;li&gt;Azureバックボーンを使うと日本とブラジルの間でもそれなりのスループット&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ファイル数が多いときはイメージで送るのも手&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ファイル数がコピー時間に影響する (1 vs 6160)&lt;/li&gt;
&lt;li&gt;そもそもアプリがBlobとして使うのか、ファイルシステムとして使うかにもよるが&amp;hellip;&lt;/li&gt;
&lt;li&gt;もしファイルシステムとして、であれば有効な手段&lt;/li&gt;
&lt;li&gt;エクスポートのひと手間は考慮&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Azureバックボーンを使うと、意外にブラジル近い&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;土管か(ない&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Azureバックボーンの帯域にはSLAがありませんが、意識して仕組みを作ると得をします。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Azureユーザー視点のLatency測定 2017/4版</title>
      <link>http://torumakabe.github.io/post/azure_latency/</link>
      <pubDate>Sun, 09 Apr 2017 15:15:00 +0900</pubDate>
      
      <guid>http://torumakabe.github.io/post/azure_latency/</guid>
      <description>

&lt;h2 id=&#34;関東の片隅で遅延を測る&#34;&gt;関東の片隅で遅延を測る&lt;/h2&gt;

&lt;p&gt;Twitterで「東阪の遅延って最近どのくらい？」と話題になっていたので。首都圏のAzureユーザー視線で測定しようと思います。&lt;/p&gt;

&lt;p&gt;せっかくなので、&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;太平洋のそれも測定しましょう&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/blog/how-microsoft-builds-its-fast-and-reliable-global-network/&#34;&gt;Azureバックボーンを通るリージョン間通信&lt;/a&gt;も測りましょう&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;計測パターン&#34;&gt;計測パターン&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;自宅(神奈川) -&amp;gt; OCN光 -&amp;gt; インターネット -&amp;gt; Azure東日本リージョン&lt;/li&gt;
&lt;li&gt;自宅(神奈川) -&amp;gt; OCN光 -&amp;gt; インターネット -&amp;gt; Azure西日本リージョン&lt;/li&gt;
&lt;li&gt;自宅(神奈川) -&amp;gt; OCN光 -&amp;gt; インターネット -&amp;gt; Azure米国西海岸リージョン&lt;/li&gt;
&lt;li&gt;Azure東日本リージョン -&amp;gt; Azureバックボーン -&amp;gt; Azure西日本リージョン&lt;/li&gt;
&lt;li&gt;Azure東日本リージョン -&amp;gt; Azureバックボーン -&amp;gt; Azure米国西海岸リージョン&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;もろもろの条件&#34;&gt;もろもろの条件&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;遅延測定ツール

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://technet.microsoft.com/en-us/sysinternals/psping.aspx&#34;&gt;PsPing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Azure各リージョンにD1_v2/Windows Server 2016仮想マシンを作成しPsPing&lt;/li&gt;
&lt;li&gt;NSGでデフォルト許可されているRDPポートへのPsPing&lt;/li&gt;
&lt;li&gt;VPN接続せず、パブリックIPへPsPing&lt;/li&gt;
&lt;li&gt;リージョン間PsPingは仮想マシンから仮想マシンへ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;自宅Wi-Fi環境

&lt;ul&gt;
&lt;li&gt;802.11ac(5GHz)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;自宅加入インターネット接続サービス

&lt;ul&gt;
&lt;li&gt;OCN 光 マンション 100M&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;OCNゲートウェイ

&lt;ul&gt;
&lt;li&gt;(ほげほげ)hodogaya.kanagawa.ocn.ne.jp&lt;/li&gt;
&lt;li&gt;神奈川県横浜市保土ケ谷区の局舎からインターネットに出ているようです&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;米国リージョン

&lt;ul&gt;
&lt;li&gt;US WEST (カリフォルニア)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;測定結果&#34;&gt;測定結果&lt;/h2&gt;

&lt;h3 id=&#34;1-自宅-神奈川-ocn光-インターネット-azure東日本リージョン&#34;&gt;1. 自宅(神奈川) -&amp;gt; OCN光 -&amp;gt; インターネット -&amp;gt; Azure東日本リージョン&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;TCP connect statistics for 104.41.187.55:3389:
  Sent = 4, Received = 4, Lost = 0 (0% loss),
  Minimum = 11.43ms, Maximum = 15.66ms, Average = 12.88ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-自宅-神奈川-ocn光-インターネット-azure西日本リージョン&#34;&gt;2. 自宅(神奈川) -&amp;gt; OCN光 -&amp;gt; インターネット -&amp;gt; Azure西日本リージョン&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;TCP connect statistics for 52.175.148.28:3389:
  Sent = 4, Received = 4, Lost = 0 (0% loss),
  Minimum = 17.96ms, Maximum = 19.64ms, Average = 18.92ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-自宅-神奈川-ocn光-インターネット-azure米国西海岸リージョン&#34;&gt;3. 自宅(神奈川) -&amp;gt; OCN光 -&amp;gt; インターネット -&amp;gt; Azure米国西海岸リージョン&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;TCP connect statistics for 40.83.220.19:3389:
  Sent = 4, Received = 4, Lost = 0 (0% loss),
  Minimum = 137.73ms, Maximum = 422.56ms, Average = 218.85ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-azure東日本リージョン-azureバックボーン-azure西日本リージョン&#34;&gt;4. Azure東日本リージョン -&amp;gt; Azureバックボーン -&amp;gt; Azure西日本リージョン&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;TCP connect statistics for 52.175.148.28:3389:
  Sent = 4, Received = 4, Lost = 0 (0% loss),
  Minimum = 8.61ms, Maximum = 9.38ms, Average = 9.00ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-azure東日本リージョン-azureバックボーン-azure米国西海岸リージョン&#34;&gt;5. Azure東日本リージョン -&amp;gt; Azureバックボーン -&amp;gt; Azure米国西海岸リージョン&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;TCP connect statistics for 40.83.220.19:3389:
  Sent = 4, Received = 4, Lost = 0 (0% loss),
  Minimum = 106.38ms, Maximum = 107.38ms, Average = 106.65ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Azureバックボーンを通すと首都圏からの遅延が半分になりました。Wi-Fiの有無など、ちょっと条件は違いますが。&lt;/p&gt;

&lt;h2 id=&#34;ひとこと&#34;&gt;ひとこと&lt;/h2&gt;

&lt;p&gt;インターネット、および接続サービスの遅延が性能の上がらない原因になっている場合は、Azureで完結させてみるのも手です。&lt;/p&gt;

&lt;p&gt;たとえば、&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;会社で契約しているインターネット接続サービスが、貧弱&lt;/li&gt;
&lt;li&gt;シリコンバレーの研究所からインターネット経由でデータを取得しているが、遅い&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;こんなケースではAzureを間に入れると、幸せになれるかもしれません。なったユーザーもいらっしゃいます。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>